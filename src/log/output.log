2024-02-19T16:14:37.439892938-06:00 - INFO - booting baby
2024-02-19T16:14:37.439969687-06:00 - INFO - booting up
2024-02-19T16:14:37.439994247-06:00 - INFO - booting up
2024-02-19T16:19:34.944912492-06:00 - INFO - booting baby
2024-02-19T16:19:34.944985442-06:00 - INFO - booting up
2024-02-19T16:19:34.945009552-06:00 - INFO - booting up
2024-02-19T16:20:27.563963015-06:00 - INFO - booting baby
2024-02-19T16:20:27.564034454-06:00 - INFO - booting up
2024-02-19T16:20:27.564059254-06:00 - INFO - booting up
2024-02-19T16:24:59.508894252-06:00 - INFO - booting baby
2024-02-19T16:24:59.508965292-06:00 - INFO - booting up
2024-02-19T16:24:59.508989361-06:00 - INFO - booting up
2024-02-19T16:25:04.622443136-06:00 - INFO - booting up
2024-02-19T16:25:04.650436521-06:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-02-19T16:25:04.650513220-06:00 - DEBUG - starting new connection: https://api.openai.com/
2024-02-19T16:25:04.650572410-06:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-02-19T16:25:04.650728459-06:00 - DEBUG - resolving host="api.openai.com"
2024-02-19T16:25:04.666319052-06:00 - DEBUG - connecting to 104.18.7.192:443
2024-02-19T16:25:04.672211955-06:00 - DEBUG - connected to 104.18.7.192:443
2024-02-19T16:25:04.683750831-06:00 - TRACE - client handshake Http1
2024-02-19T16:25:04.683791631-06:00 - TRACE - handshake complete, spawning background dispatcher task
2024-02-19T16:25:04.683847830-06:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-02-19T16:25:04.683875220-06:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-02-19T16:25:04.683932550-06:00 - TRACE - encode_headers;
2024-02-19T16:25:04.683944920-06:00 - TRACE - -> encode_headers;
2024-02-19T16:25:04.683951680-06:00 - TRACE - Client::encode method=POST, body=Some(Known(908))
2024-02-19T16:25:04.683974599-06:00 - TRACE - <- encode_headers;
2024-02-19T16:25:04.683981259-06:00 - TRACE - -- encode_headers;
2024-02-19T16:25:04.683989739-06:00 - TRACE - sized write, len = 908
2024-02-19T16:25:04.683996699-06:00 - TRACE - buffer.flatten self.len=202 buf.len=908
2024-02-19T16:25:04.684025569-06:00 - DEBUG - flushed 1110 bytes
2024-02-19T16:25:04.684036119-06:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-02-19T16:25:04.759298032-06:00 - TRACE - Conn::read_head
2024-02-19T16:25:04.759434430-06:00 - TRACE - received 1110 bytes
2024-02-19T16:25:04.759449960-06:00 - TRACE - parse_headers;
2024-02-19T16:25:04.759456640-06:00 - TRACE - -> parse_headers;
2024-02-19T16:25:04.759464810-06:00 - TRACE - Response.parse bytes=1110
2024-02-19T16:25:04.759488780-06:00 - TRACE - Response.parse Complete(808)
2024-02-19T16:25:04.759540320-06:00 - TRACE - <- parse_headers;
2024-02-19T16:25:04.759548569-06:00 - TRACE - -- parse_headers;
2024-02-19T16:25:04.759554939-06:00 - DEBUG - parsed 13 headers
2024-02-19T16:25:04.759561259-06:00 - DEBUG - incoming body is content-length (302 bytes)
2024-02-19T16:25:04.759580769-06:00 - TRACE - decode; state=Length(302)
2024-02-19T16:25:04.759590549-06:00 - DEBUG - incoming body completed
2024-02-19T16:25:04.759612169-06:00 - TRACE - maybe_notify; read_from_io blocked
2024-02-19T16:25:04.759637169-06:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-02-19T16:25:04.759650309-06:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-02-19T16:25:04.759690348-06:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-02-19T16:25:04.759707458-06:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-02-19T16:25:04.759780948-06:00 - INFO - your json object was not succesfully and sent and given a good request
2024-02-19T16:25:04.759861497-06:00 - TRACE - client tx closed
2024-02-19T16:25:04.759947906-06:00 - TRACE - pool closed, canceling idle interval
2024-02-19T16:25:04.759965916-06:00 - TRACE - State::close_read()
2024-02-19T16:25:04.759980396-06:00 - TRACE - State::close_write()
2024-02-19T16:25:04.760002356-06:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-02-19T16:25:04.760036316-06:00 - TRACE - shut down IO complete
2024-02-19T16:43:17.944954382-06:00 - INFO - booting baby
2024-02-19T16:43:17.945065041-06:00 - INFO - booting up
2024-02-19T16:43:17.945102141-06:00 - INFO - booting up
2024-02-19T16:43:26.198450844-06:00 - INFO - booting up
2024-02-19T16:43:26.217018588-06:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-02-19T16:43:26.217069998-06:00 - DEBUG - starting new connection: https://api.openai.com/
2024-02-19T16:43:26.217109657-06:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-02-19T16:43:26.217256796-06:00 - DEBUG - resolving host="api.openai.com"
2024-02-19T16:43:26.236806500-06:00 - DEBUG - connecting to 104.18.7.192:443
2024-02-19T16:43:26.244346744-06:00 - DEBUG - connected to 104.18.7.192:443
2024-02-19T16:43:26.255952477-06:00 - TRACE - client handshake Http1
2024-02-19T16:43:26.255989847-06:00 - TRACE - handshake complete, spawning background dispatcher task
2024-02-19T16:43:26.256067036-06:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-02-19T16:43:26.256112596-06:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-02-19T16:43:26.256184385-06:00 - TRACE - encode_headers;
2024-02-19T16:43:26.256200915-06:00 - TRACE - -> encode_headers;
2024-02-19T16:43:26.256211975-06:00 - TRACE - Client::encode method=POST, body=Some(Known(908))
2024-02-19T16:43:26.256236585-06:00 - TRACE - <- encode_headers;
2024-02-19T16:43:26.256249054-06:00 - TRACE - -- encode_headers;
2024-02-19T16:43:26.256261724-06:00 - TRACE - sized write, len = 908
2024-02-19T16:43:26.256272804-06:00 - TRACE - buffer.flatten self.len=202 buf.len=908
2024-02-19T16:43:26.256308754-06:00 - DEBUG - flushed 1110 bytes
2024-02-19T16:43:26.256323794-06:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-02-19T16:43:26.343986524-06:00 - TRACE - Conn::read_head
2024-02-19T16:43:26.344156842-06:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-02-19T16:43:26.344670167-06:00 - TRACE - Conn::read_head
2024-02-19T16:43:26.344698777-06:00 - TRACE - received 1110 bytes
2024-02-19T16:43:26.344710756-06:00 - TRACE - parse_headers;
2024-02-19T16:43:26.344720556-06:00 - TRACE - -> parse_headers;
2024-02-19T16:43:26.344733496-06:00 - TRACE - Response.parse bytes=1110
2024-02-19T16:43:26.344766086-06:00 - TRACE - Response.parse Complete(808)
2024-02-19T16:43:26.344825605-06:00 - TRACE - <- parse_headers;
2024-02-19T16:43:26.344837725-06:00 - TRACE - -- parse_headers;
2024-02-19T16:43:26.344845645-06:00 - DEBUG - parsed 13 headers
2024-02-19T16:43:26.344853005-06:00 - DEBUG - incoming body is content-length (302 bytes)
2024-02-19T16:43:26.344874145-06:00 - TRACE - decode; state=Length(302)
2024-02-19T16:43:26.344890035-06:00 - DEBUG - incoming body completed
2024-02-19T16:43:26.344913624-06:00 - TRACE - maybe_notify; read_from_io blocked
2024-02-19T16:43:26.344945594-06:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-02-19T16:43:26.344968234-06:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-02-19T16:43:26.345014143-06:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-02-19T16:43:26.345035623-06:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-02-19T16:43:26.345103803-06:00 - INFO - your json object was not succesfully and sent and given a good request in first section
2024-02-19T16:43:26.345173172-06:00 - TRACE - client tx closed
2024-02-19T16:43:26.345243271-06:00 - TRACE - pool closed, canceling idle interval
2024-02-19T16:43:26.345264341-06:00 - TRACE - State::close_read()
2024-02-19T16:43:26.345275641-06:00 - TRACE - State::close_write()
2024-02-19T16:43:26.345294321-06:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-02-19T16:43:26.345333360-06:00 - TRACE - shut down IO complete
2024-02-19T16:56:24.904898003-06:00 - INFO - booting baby
2024-02-19T16:56:24.905025731-06:00 - INFO - booting up
2024-02-19T16:56:24.905055681-06:00 - INFO - booting up
2024-02-19T16:56:29.264011382-06:00 - INFO - booting up
2024-02-19T16:56:29.283461561-06:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-02-19T16:56:29.283520300-06:00 - DEBUG - starting new connection: https://api.openai.com/
2024-02-19T16:56:29.283565070-06:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-02-19T16:56:29.283714159-06:00 - DEBUG - resolving host="api.openai.com"
2024-02-19T16:56:29.298584698-06:00 - DEBUG - connecting to 104.18.7.192:443
2024-02-19T16:56:29.309142216-06:00 - DEBUG - connected to 104.18.7.192:443
2024-02-19T16:56:29.322148711-06:00 - TRACE - client handshake Http1
2024-02-19T16:56:29.322188741-06:00 - TRACE - handshake complete, spawning background dispatcher task
2024-02-19T16:56:29.322253640-06:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-02-19T16:56:29.322284810-06:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-02-19T16:56:29.322345289-06:00 - TRACE - encode_headers;
2024-02-19T16:56:29.322359539-06:00 - TRACE - -> encode_headers;
2024-02-19T16:56:29.322367069-06:00 - TRACE - Client::encode method=POST, body=Some(Known(908))
2024-02-19T16:56:29.322387069-06:00 - TRACE - <- encode_headers;
2024-02-19T16:56:29.322394279-06:00 - TRACE - -- encode_headers;
2024-02-19T16:56:29.322404049-06:00 - TRACE - sized write, len = 908
2024-02-19T16:56:29.322411419-06:00 - TRACE - buffer.flatten self.len=202 buf.len=908
2024-02-19T16:56:29.322441209-06:00 - DEBUG - flushed 1110 bytes
2024-02-19T16:56:29.322453508-06:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-02-19T16:56:29.400385945-06:00 - TRACE - Conn::read_head
2024-02-19T16:56:29.400585213-06:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-02-19T16:56:29.400824651-06:00 - TRACE - Conn::read_head
2024-02-19T16:56:29.400863720-06:00 - TRACE - received 1110 bytes
2024-02-19T16:56:29.400887480-06:00 - TRACE - parse_headers;
2024-02-19T16:56:29.400908400-06:00 - TRACE - -> parse_headers;
2024-02-19T16:56:29.400929150-06:00 - TRACE - Response.parse bytes=1110
2024-02-19T16:56:29.400977549-06:00 - TRACE - Response.parse Complete(808)
2024-02-19T16:56:29.401069779-06:00 - TRACE - <- parse_headers;
2024-02-19T16:56:29.401088398-06:00 - TRACE - -- parse_headers;
2024-02-19T16:56:29.401099888-06:00 - DEBUG - parsed 13 headers
2024-02-19T16:56:29.401118278-06:00 - DEBUG - incoming body is content-length (302 bytes)
2024-02-19T16:56:29.401160088-06:00 - TRACE - decode; state=Length(302)
2024-02-19T16:56:29.401182478-06:00 - DEBUG - incoming body completed
2024-02-19T16:56:29.401220607-06:00 - TRACE - maybe_notify; read_from_io blocked
2024-02-19T16:56:29.401271157-06:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-02-19T16:56:29.401304366-06:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-02-19T16:56:29.401376126-06:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-02-19T16:56:29.401409556-06:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-02-19T16:56:29.401521665-06:00 - INFO - your json object was not succesfully. This is the json response: {"error":{"code":"invalid_api_key","message":"Incorrect API key provided: sk-7qvMo****************************************lGea. You can find your API key at https://platform.openai.com/account/api-keys.","param":null,"type":"invalid_request_error"}}
2024-02-19T16:56:29.401640573-06:00 - TRACE - client tx closed
2024-02-19T16:56:29.401743073-06:00 - TRACE - pool closed, canceling idle interval
2024-02-19T16:56:29.401794622-06:00 - TRACE - State::close_read()
2024-02-19T16:56:29.401817662-06:00 - TRACE - State::close_write()
2024-02-19T16:56:29.401833192-06:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-02-19T16:56:29.401881581-06:00 - TRACE - shut down IO complete
2024-02-19T17:09:49.176922069-06:00 - INFO - booting baby
2024-02-19T17:09:49.176998628-06:00 - INFO - booting up
2024-02-19T17:09:49.177032847-06:00 - INFO - booting up
2024-02-19T17:09:52.517061222-06:00 - INFO - booting up
2024-02-19T17:09:52.543810553-06:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-02-19T17:09:52.543893142-06:00 - DEBUG - starting new connection: https://api.openai.com/
2024-02-19T17:09:52.543936331-06:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-02-19T17:09:52.544075679-06:00 - DEBUG - resolving host="api.openai.com"
2024-02-19T17:09:52.561851843-06:00 - DEBUG - connecting to 104.18.7.192:443
2024-02-19T17:09:52.571834365-06:00 - DEBUG - connected to 104.18.7.192:443
2024-02-19T17:09:52.592617502-06:00 - TRACE - client handshake Http1
2024-02-19T17:09:52.592670681-06:00 - TRACE - handshake complete, spawning background dispatcher task
2024-02-19T17:09:52.592739800-06:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-02-19T17:09:52.592776740-06:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-02-19T17:09:52.592840489-06:00 - TRACE - encode_headers;
2024-02-19T17:09:52.592857199-06:00 - TRACE - -> encode_headers;
2024-02-19T17:09:52.592871058-06:00 - TRACE - Client::encode method=POST, body=Some(Known(908))
2024-02-19T17:09:52.592898938-06:00 - TRACE - <- encode_headers;
2024-02-19T17:09:52.592909918-06:00 - TRACE - -- encode_headers;
2024-02-19T17:09:52.592924438-06:00 - TRACE - sized write, len = 908
2024-02-19T17:09:52.592937387-06:00 - TRACE - buffer.flatten self.len=201 buf.len=908
2024-02-19T17:09:52.592975947-06:00 - DEBUG - flushed 1109 bytes
2024-02-19T17:09:52.592991957-06:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-02-19T17:09:52.676120738-06:00 - TRACE - Conn::read_head
2024-02-19T17:09:52.676261996-06:00 - TRACE - received 974 bytes
2024-02-19T17:09:52.676289696-06:00 - TRACE - parse_headers;
2024-02-19T17:09:52.676309416-06:00 - TRACE - -> parse_headers;
2024-02-19T17:09:52.676329196-06:00 - TRACE - Response.parse bytes=974
2024-02-19T17:09:52.676378985-06:00 - TRACE - Response.parse Complete(807)
2024-02-19T17:09:52.676455014-06:00 - TRACE - <- parse_headers;
2024-02-19T17:09:52.676475974-06:00 - TRACE - -- parse_headers;
2024-02-19T17:09:52.676495473-06:00 - DEBUG - parsed 13 headers
2024-02-19T17:09:52.676514393-06:00 - DEBUG - incoming body is content-length (167 bytes)
2024-02-19T17:09:52.676552083-06:00 - TRACE - decode; state=Length(167)
2024-02-19T17:09:52.676575012-06:00 - DEBUG - incoming body completed
2024-02-19T17:09:52.676615932-06:00 - TRACE - maybe_notify; read_from_io blocked
2024-02-19T17:09:52.676673621-06:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-02-19T17:09:52.676704081-06:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-02-19T17:09:52.676752530-06:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-02-19T17:09:52.676779240-06:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-02-19T17:09:52.676871469-06:00 - INFO - your json object was not succesfully. This is the json response: {"error":{"code":null,"message":"you must provide a model parameter","param":null,"type":"invalid_request_error"}}
2024-02-19T17:09:52.676973367-06:00 - TRACE - client tx closed
2024-02-19T17:09:52.677057176-06:00 - TRACE - State::close_read()
2024-02-19T17:09:52.677068616-06:00 - TRACE - State::close_write()
2024-02-19T17:09:52.677081406-06:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-02-19T17:09:52.677111886-06:00 - TRACE - pool closed, canceling idle interval
2024-02-19T17:09:52.677145415-06:00 - TRACE - shut down IO complete
2024-03-27T01:32:29.674160658-05:00 - INFO - booting up
2024-03-27T01:32:29.676167800-05:00 - INFO - alias was selected
2024-03-27T01:32:29.676227049-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-27T01:32:29.694471770-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-27T01:32:29.694530320-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-27T01:32:29.694565419-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-27T01:32:29.694670438-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-27T01:32:29.711852038-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-27T01:32:29.722281640-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-27T01:32:29.736803095-05:00 - TRACE - client handshake Http1
2024-03-27T01:32:29.736836395-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-27T01:32:29.736941004-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-27T01:32:29.737008393-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-27T01:32:29.737081262-05:00 - TRACE - encode_headers;
2024-03-27T01:32:29.737097182-05:00 - TRACE - -> encode_headers;
2024-03-27T01:32:29.737105892-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3851))
2024-03-27T01:32:29.737130562-05:00 - TRACE - <- encode_headers;
2024-03-27T01:32:29.737138382-05:00 - TRACE - -- encode_headers;
2024-03-27T01:32:29.737148392-05:00 - TRACE - sized write, len = 3851
2024-03-27T01:32:29.737156942-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3851
2024-03-27T01:32:29.737210561-05:00 - DEBUG - flushed 4053 bytes
2024-03-27T01:32:29.737224671-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-27T01:32:32.301113725-05:00 - TRACE - Conn::read_head
2024-03-27T01:32:32.301441712-05:00 - TRACE - received 1369 bytes
2024-03-27T01:32:32.301466562-05:00 - TRACE - parse_headers;
2024-03-27T01:32:32.301477742-05:00 - TRACE - -> parse_headers;
2024-03-27T01:32:32.301500562-05:00 - TRACE - Response.parse bytes=1369
2024-03-27T01:32:32.301557321-05:00 - TRACE - Response.parse Complete(1202)
2024-03-27T01:32:32.301647500-05:00 - TRACE - <- parse_headers;
2024-03-27T01:32:32.301657290-05:00 - TRACE - -- parse_headers;
2024-03-27T01:32:32.301664760-05:00 - DEBUG - parsed 24 headers
2024-03-27T01:32:32.301672250-05:00 - DEBUG - incoming body is content-length (620 bytes)
2024-03-27T01:32:32.301706370-05:00 - TRACE - decode; state=Length(620)
2024-03-27T01:32:32.301749109-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(453)), writing: KeepAlive, keep_alive: Busy }
2024-03-27T01:32:32.301812599-05:00 - TRACE - decode; state=Length(453)
2024-03-27T01:32:32.301905918-05:00 - TRACE - received 453 bytes
2024-03-27T01:32:32.301919568-05:00 - DEBUG - incoming body completed
2024-03-27T01:32:32.301949517-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-27T01:32:32.301982917-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-27T01:32:32.301999857-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-27T01:32:32.302018677-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-27T01:32:32.302036087-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-27T01:32:32.302118736-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "background = \"0x000000\"",
      "new": "background = \"0x111111\""
    }
  }
}
2024-03-27T01:32:32.302146875-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-27T01:32:32.302247165-05:00 - INFO - key change1
2024-03-27T01:32:32.302258514-05:00 - INFO - value "background = \"0x000000\""
2024-03-27T01:32:32.302286334-05:00 - INFO - old value is "background = \"0x000000\""
2024-03-27T01:32:32.302298334-05:00 - INFO - writing to file
2024-03-27T01:32:32.302401073-05:00 - INFO - New value: background = "0x111111"
2024-03-27T01:32:32.302431383-05:00 - INFO - 

2024-03-27T01:32:32.302439053-05:00 - INFO - is bool true true
2024-03-27T01:32:32.302464672-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"background = \\\"0x000000\\\"\",\n      \"new\": \"background = \\\"0x111111\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711521150,"id":"chatcmpl-97H7mUOKtCFFdcJW8Hz6KIUQfg8KO","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":44,"prompt_tokens":1057,"total_tokens":1101}}
2024-03-27T01:32:32.302523262-05:00 - TRACE - client tx closed
2024-03-27T01:32:32.302553292-05:00 - TRACE - State::close_read()
2024-03-27T01:32:32.302566782-05:00 - TRACE - State::close_write()
2024-03-27T01:32:32.302577821-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-27T01:32:32.302634421-05:00 - TRACE - shut down IO complete
2024-03-27T01:32:43.369454767-05:00 - INFO - booting up
2024-03-27T01:32:46.008006270-05:00 - INFO - booting up
2024-03-27T01:33:16.970328936-05:00 - INFO - booting up
2024-03-27T01:33:16.971446266-05:00 - INFO - user request this alias to be reverted alacritty.toml 
2024-03-27T01:33:16.971483175-05:00 - INFO - checking if alias exist, so it could be reverted
2024-03-27T01:33:16.971490995-05:00 - INFO - content alias is alacritty.toml 
2024-03-27T01:33:16.971587314-05:00 - INFO - sucessfully reverted the original file to its backup file
2024-03-27T01:33:21.531563759-05:00 - INFO - booting up
2024-03-27T01:33:21.532696208-05:00 - INFO - user request this alias to be reverted alacritty.toml 
2024-03-27T01:33:21.532800257-05:00 - INFO - checking if alias exist, so it could be reverted
2024-03-27T01:33:21.532809617-05:00 - INFO - content alias is alacritty.toml 
2024-03-27T01:33:21.532910336-05:00 - INFO - sucessfully reverted the original file to its backup file
2024-03-27T01:33:27.222197120-05:00 - INFO - booting up
2024-03-27T01:33:27.223404219-05:00 - INFO - alias was selected
2024-03-27T01:33:27.223465789-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-27T01:33:27.245542804-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-27T01:33:27.245602423-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-27T01:33:27.245635223-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-27T01:33:27.245747772-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-27T01:33:27.252232471-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-27T01:33:27.261945651-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-27T01:33:27.274182958-05:00 - TRACE - client handshake Http1
2024-03-27T01:33:27.274216377-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-27T01:33:27.274313176-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-27T01:33:27.274429365-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-27T01:33:27.274505425-05:00 - TRACE - encode_headers;
2024-03-27T01:33:27.274521754-05:00 - TRACE - -> encode_headers;
2024-03-27T01:33:27.274530284-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3851))
2024-03-27T01:33:27.274555084-05:00 - TRACE - <- encode_headers;
2024-03-27T01:33:27.274562964-05:00 - TRACE - -- encode_headers;
2024-03-27T01:33:27.274573324-05:00 - TRACE - sized write, len = 3851
2024-03-27T01:33:27.274590334-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3851
2024-03-27T01:33:27.274653693-05:00 - DEBUG - flushed 4053 bytes
2024-03-27T01:33:27.274666223-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-27T01:33:28.825954696-05:00 - TRACE - Conn::read_head
2024-03-27T01:33:28.826179294-05:00 - TRACE - received 1369 bytes
2024-03-27T01:33:28.826197364-05:00 - TRACE - parse_headers;
2024-03-27T01:33:28.826206294-05:00 - TRACE - -> parse_headers;
2024-03-27T01:33:28.826225563-05:00 - TRACE - Response.parse bytes=1369
2024-03-27T01:33:28.826271843-05:00 - TRACE - Response.parse Complete(1202)
2024-03-27T01:33:28.826350852-05:00 - TRACE - <- parse_headers;
2024-03-27T01:33:28.826361102-05:00 - TRACE - -- parse_headers;
2024-03-27T01:33:28.826369042-05:00 - DEBUG - parsed 24 headers
2024-03-27T01:33:28.826377062-05:00 - DEBUG - incoming body is content-length (620 bytes)
2024-03-27T01:33:28.826410652-05:00 - TRACE - decode; state=Length(620)
2024-03-27T01:33:28.826435241-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(453)), writing: KeepAlive, keep_alive: Busy }
2024-03-27T01:33:28.826537391-05:00 - TRACE - decode; state=Length(453)
2024-03-27T01:33:28.826670799-05:00 - TRACE - received 453 bytes
2024-03-27T01:33:28.826688129-05:00 - DEBUG - incoming body completed
2024-03-27T01:33:28.826718489-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-27T01:33:28.826753089-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-27T01:33:28.826771468-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-27T01:33:28.826789658-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-27T01:33:28.826806548-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-27T01:33:28.826861228-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-27T01:33:28.826874547-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "background = \"0x8b0000\"",
      "new": "background = \"0x1a1a1a\""
    }
  }
}
2024-03-27T01:33:28.826903987-05:00 - INFO - key change1
2024-03-27T01:33:28.826911197-05:00 - INFO - value "background = \"0x8b0000\""
2024-03-27T01:33:28.826934247-05:00 - INFO - old value is "background = \"0x8b0000\""
2024-03-27T01:33:28.826944427-05:00 - INFO - writing to file
2024-03-27T01:33:28.827030126-05:00 - INFO - New value: background = "0x1a1a1a"
2024-03-27T01:33:28.827053586-05:00 - INFO - 

2024-03-27T01:33:28.827059876-05:00 - INFO - is bool true true
2024-03-27T01:33:28.827080566-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"background = \\\"0x8b0000\\\"\",\n      \"new\": \"background = \\\"0x1a1a1a\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711521207,"id":"chatcmpl-97H8hDTPXSIATJKRymmX9g94EsT3h","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":50,"prompt_tokens":1059,"total_tokens":1109}}
2024-03-27T01:33:28.827123515-05:00 - TRACE - client tx closed
2024-03-27T01:33:28.827136345-05:00 - TRACE - State::close_read()
2024-03-27T01:33:28.827143355-05:00 - TRACE - pool closed, canceling idle interval
2024-03-27T01:33:28.827229034-05:00 - TRACE - State::close_write()
2024-03-27T01:33:28.827244904-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-27T01:33:28.827278784-05:00 - TRACE - shut down IO complete
2024-03-27T01:33:33.227465717-05:00 - INFO - booting up
2024-03-27T01:33:44.399716618-05:00 - INFO - booting up
2024-03-27T01:34:19.806498526-05:00 - INFO - booting up
2024-03-27T01:34:31.088797048-05:00 - INFO - booting up
2024-03-27T01:34:31.089885058-05:00 - INFO - user request this alias to be reverted alacritty.toml 
2024-03-27T01:34:31.089920558-05:00 - INFO - checking if alias exist, so it could be reverted
2024-03-27T01:34:31.089928768-05:00 - INFO - content alias is alacritty.toml 
2024-03-27T01:34:31.090029867-05:00 - INFO - sucessfully reverted the original file to its backup file
2024-03-27T01:34:52.320616359-05:00 - INFO - booting up
2024-03-27T01:34:52.321782888-05:00 - INFO - alias was selected
2024-03-27T01:34:52.321831477-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-27T01:34:52.340726751-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-27T01:34:52.340784541-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-27T01:34:52.340820520-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-27T01:34:52.340927499-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-27T01:34:52.356020019-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-27T01:34:52.364621719-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-27T01:34:52.376559587-05:00 - TRACE - client handshake Http1
2024-03-27T01:34:52.376600467-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-27T01:34:52.376710766-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-27T01:34:52.376783465-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-27T01:34:52.376859585-05:00 - TRACE - encode_headers;
2024-03-27T01:34:52.376875424-05:00 - TRACE - -> encode_headers;
2024-03-27T01:34:52.376884254-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3850))
2024-03-27T01:34:52.376909504-05:00 - TRACE - <- encode_headers;
2024-03-27T01:34:52.376917934-05:00 - TRACE - -- encode_headers;
2024-03-27T01:34:52.376927924-05:00 - TRACE - sized write, len = 3850
2024-03-27T01:34:52.376936354-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3850
2024-03-27T01:34:52.376988203-05:00 - DEBUG - flushed 4052 bytes
2024-03-27T01:34:52.377002193-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-27T01:34:54.226486753-05:00 - TRACE - Conn::read_head
2024-03-27T01:34:54.226756241-05:00 - TRACE - received 1369 bytes
2024-03-27T01:34:54.226777410-05:00 - TRACE - parse_headers;
2024-03-27T01:34:54.226788620-05:00 - TRACE - -> parse_headers;
2024-03-27T01:34:54.226813410-05:00 - TRACE - Response.parse bytes=1369
2024-03-27T01:34:54.226864520-05:00 - TRACE - Response.parse Complete(1202)
2024-03-27T01:34:54.226956299-05:00 - TRACE - <- parse_headers;
2024-03-27T01:34:54.226968569-05:00 - TRACE - -- parse_headers;
2024-03-27T01:34:54.226978218-05:00 - DEBUG - parsed 24 headers
2024-03-27T01:34:54.226988168-05:00 - DEBUG - incoming body is content-length (620 bytes)
2024-03-27T01:34:54.227024828-05:00 - TRACE - decode; state=Length(620)
2024-03-27T01:34:54.227053078-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(453)), writing: KeepAlive, keep_alive: Busy }
2024-03-27T01:34:54.227155737-05:00 - TRACE - decode; state=Length(453)
2024-03-27T01:34:54.227345025-05:00 - TRACE - received 453 bytes
2024-03-27T01:34:54.227364225-05:00 - DEBUG - incoming body completed
2024-03-27T01:34:54.227403384-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-27T01:34:54.227455114-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-27T01:34:54.227488794-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-27T01:34:54.227514083-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-27T01:34:54.227536053-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-27T01:34:54.227620682-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-27T01:34:54.227660162-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "background = \"0x1a1a1a\"",
      "new": "background = \"0x6a0dad\""
    }
  }
}
2024-03-27T01:34:54.227700112-05:00 - INFO - key change1
2024-03-27T01:34:54.227722751-05:00 - INFO - value "background = \"0x1a1a1a\""
2024-03-27T01:34:54.227748861-05:00 - INFO - old value is "background = \"0x1a1a1a\""
2024-03-27T01:34:54.227758941-05:00 - INFO - writing to file
2024-03-27T01:34:54.227846750-05:00 - INFO - New value: background = "0x6a0dad"
2024-03-27T01:34:54.227870350-05:00 - INFO - 

2024-03-27T01:34:54.227876740-05:00 - INFO - is bool true true
2024-03-27T01:34:54.227897890-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"background = \\\"0x1a1a1a\\\"\",\n      \"new\": \"background = \\\"0x6a0dad\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711521293,"id":"chatcmpl-97HA512mipItja6LC6a45FAXzK8Di","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":50,"prompt_tokens":1062,"total_tokens":1112}}
2024-03-27T01:34:54.227951389-05:00 - TRACE - pool closed, canceling idle interval
2024-03-27T01:34:54.227990709-05:00 - TRACE - client tx closed
2024-03-27T01:34:54.228019859-05:00 - TRACE - State::close_read()
2024-03-27T01:34:54.228034608-05:00 - TRACE - State::close_write()
2024-03-27T01:34:54.228049898-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-27T01:34:54.228096468-05:00 - TRACE - shut down IO complete
2024-03-27T01:34:58.852640853-05:00 - INFO - booting up
2024-03-27T01:34:58.853805843-05:00 - INFO - user request this alias to be reverted alacritty.toml 
2024-03-27T01:34:58.853843082-05:00 - INFO - checking if alias exist, so it could be reverted
2024-03-27T01:34:58.853851122-05:00 - INFO - content alias is alacritty.toml 
2024-03-27T01:34:58.853943561-05:00 - INFO - sucessfully reverted the original file to its backup file
2024-03-27T01:35:33.096965615-05:00 - INFO - booting up
2024-03-27T01:35:33.098082505-05:00 - INFO - alias was selected
2024-03-27T01:35:33.098127454-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-27T01:35:33.117379354-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-27T01:35:33.117440574-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-27T01:35:33.117473774-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-27T01:35:33.117584733-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-27T01:35:33.121786023-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-27T01:35:33.130400372-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-27T01:35:33.145624561-05:00 - TRACE - client handshake Http1
2024-03-27T01:35:33.145704110-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-27T01:35:33.145811149-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-27T01:35:33.145881618-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-27T01:35:33.145951738-05:00 - TRACE - encode_headers;
2024-03-27T01:35:33.145967747-05:00 - TRACE - -> encode_headers;
2024-03-27T01:35:33.145976817-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3850))
2024-03-27T01:35:33.146002017-05:00 - TRACE - <- encode_headers;
2024-03-27T01:35:33.146010307-05:00 - TRACE - -- encode_headers;
2024-03-27T01:35:33.146020167-05:00 - TRACE - sized write, len = 3850
2024-03-27T01:35:33.146028457-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3850
2024-03-27T01:35:33.146078356-05:00 - DEBUG - flushed 4052 bytes
2024-03-27T01:35:33.146092526-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-27T01:35:35.903741779-05:00 - TRACE - Conn::read_head
2024-03-27T01:35:35.903996146-05:00 - TRACE - received 1369 bytes
2024-03-27T01:35:35.904018466-05:00 - TRACE - parse_headers;
2024-03-27T01:35:35.904029236-05:00 - TRACE - -> parse_headers;
2024-03-27T01:35:35.904054206-05:00 - TRACE - Response.parse bytes=1369
2024-03-27T01:35:35.904126245-05:00 - TRACE - Response.parse Complete(1202)
2024-03-27T01:35:35.904218114-05:00 - TRACE - <- parse_headers;
2024-03-27T01:35:35.904230694-05:00 - TRACE - -- parse_headers;
2024-03-27T01:35:35.904240254-05:00 - DEBUG - parsed 24 headers
2024-03-27T01:35:35.904250034-05:00 - DEBUG - incoming body is content-length (620 bytes)
2024-03-27T01:35:35.904287934-05:00 - TRACE - decode; state=Length(620)
2024-03-27T01:35:35.904316203-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(453)), writing: KeepAlive, keep_alive: Busy }
2024-03-27T01:35:35.904382953-05:00 - TRACE - decode; state=Length(453)
2024-03-27T01:35:35.904426662-05:00 - TRACE - received 453 bytes
2024-03-27T01:35:35.904441652-05:00 - DEBUG - incoming body completed
2024-03-27T01:35:35.904470502-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-27T01:35:35.904509991-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-27T01:35:35.904531631-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-27T01:35:35.904552991-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-27T01:35:35.904574311-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-27T01:35:35.904687550-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-27T01:35:35.904777289-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "background = \"0x1a1a1a\"",
      "new": "background = \"0xcb4b16\""
    }
  }
}
2024-03-27T01:35:35.904815909-05:00 - INFO - key change1
2024-03-27T01:35:35.904825638-05:00 - INFO - value "background = \"0x1a1a1a\""
2024-03-27T01:35:35.904859188-05:00 - INFO - old value is "background = \"0x1a1a1a\""
2024-03-27T01:35:35.904873158-05:00 - INFO - writing to file
2024-03-27T01:35:35.904980107-05:00 - INFO - New value: background = "0xcb4b16"
2024-03-27T01:35:35.905011247-05:00 - INFO - 

2024-03-27T01:35:35.905019547-05:00 - INFO - is bool true true
2024-03-27T01:35:35.905046446-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"background = \\\"0x1a1a1a\\\"\",\n      \"new\": \"background = \\\"0xcb4b16\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711521333,"id":"chatcmpl-97HAjm5y6LN109JQBQT64AaNNP5NO","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":49,"prompt_tokens":1061,"total_tokens":1110}}
2024-03-27T01:35:35.905117226-05:00 - TRACE - client tx closed
2024-03-27T01:35:35.905207125-05:00 - TRACE - pool closed, canceling idle interval
2024-03-27T01:35:35.905295314-05:00 - TRACE - State::close_read()
2024-03-27T01:35:35.905338024-05:00 - TRACE - State::close_write()
2024-03-27T01:35:35.905357393-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-27T01:35:35.905408723-05:00 - TRACE - shut down IO complete
2024-03-27T01:35:40.155996078-05:00 - INFO - booting up
2024-03-27T01:35:40.157217567-05:00 - INFO - user request this alias to be reverted alacritty.toml 
2024-03-27T01:35:40.157259956-05:00 - INFO - checking if alias exist, so it could be reverted
2024-03-27T01:35:40.157269926-05:00 - INFO - content alias is alacritty.toml 
2024-03-27T01:35:40.157381055-05:00 - INFO - sucessfully reverted the original file to its backup file
2024-03-28T00:25:19.244919906-05:00 - INFO - booting up
2024-03-28T00:25:55.735135523-05:00 - INFO - booting up
2024-03-28T01:43:22.515719564-05:00 - INFO - booting up
2024-03-28T01:43:22.516921723-05:00 - INFO - file content [{"alias":"alacritty.toml","realpath":"/home/osamanoo/.config/alacritty/alacritty.toml","iteration":1,"backup_location":"backup_config/alacritty.toml","ts":"2024-03-25 04:06:09.037046463 UTC"}] 
2024-03-28T01:43:22.516954373-05:00 - INFO - cfl content: [ConfigFile { alias: "alacritty.toml", realpath: "/home/osamanoo/.config/alacritty/alacritty.toml", iteration: 1, backup_location: "backup_config/alacritty.toml", ts: "2024-03-25 04:06:09.037046463 UTC" }, ConfigFile { alias: "fish_config", realpath: "/home/osamanoo/.config/fish/config.fish", iteration: 1, backup_location: "backup_config/config.fish", ts: "2024-03-28 06:43:22.516949493 UTC" }]
2024-03-28T01:43:22.517045102-05:00 - INFO - could not find backup_config directory so a new one was created
2024-03-28T01:43:22.517132982-05:00 - INFO - writing the follow file metadata to config_file.json ConfigFile { alias: "fish_config", realpath: "/home/osamanoo/.config/fish/config.fish", iteration: 1, backup_location: "backup_config/config.fish", ts: "2024-03-28 06:43:22.516949493 UTC" }
2024-03-28T01:56:46.981287359-05:00 - INFO - booting up
2024-03-28T01:56:46.982977504-05:00 - INFO - file content [{"alias":"alacritty.toml","realpath":"/home/osamanoo/.config/alacritty/alacritty.toml","iteration":1,"backup_location":"backup_config/alacritty.toml","ts":"2024-03-25 04:06:09.037046463 UTC"},{"alias":"fish_config","realpath":"/home/osamanoo/.config/fish/config.fish","iteration":1,"backup_location":"backup_config/config.fish","ts":"2024-03-28 06:43:22.516949493 UTC"}] 
2024-03-28T01:56:46.983015474-05:00 - INFO - cfl content: [ConfigFile { alias: "alacritty.toml", realpath: "/home/osamanoo/.config/alacritty/alacritty.toml", iteration: 1, backup_location: "backup_config/alacritty.toml", ts: "2024-03-25 04:06:09.037046463 UTC" }, ConfigFile { alias: "fish_config", realpath: "/home/osamanoo/.config/fish/config.fish", iteration: 1, backup_location: "backup_config/config.fish", ts: "2024-03-28 06:43:22.516949493 UTC" }, ConfigFile { alias: "fisher_fish", realpath: "/home/osamanoo/.config/fish/completions/fisher.fish", iteration: 1, backup_location: "backup_config/fisher.fish", ts: "2024-03-28 06:56:46.983009804 UTC" }]
2024-03-28T01:56:46.983454590-05:00 - INFO - could not find backup_config directory so a new one was created
2024-03-28T01:56:46.983546229-05:00 - INFO - writing the follow file metadata to config_file.json ConfigFile { alias: "fisher_fish", realpath: "/home/osamanoo/.config/fish/completions/fisher.fish", iteration: 1, backup_location: "backup_config/fisher.fish", ts: "2024-03-28 06:56:46.983009804 UTC" }
2024-03-28T01:57:22.756276536-05:00 - INFO - booting up
2024-03-28T01:57:22.757472575-05:00 - INFO - list of all aliases [ConfigFile { alias: "alacritty.toml", realpath: "/home/osamanoo/.config/alacritty/alacritty.toml", iteration: 1, backup_location: "backup_config/alacritty.toml", ts: "2024-03-25 04:06:09.037046463 UTC" }, ConfigFile { alias: "fish_config", realpath: "/home/osamanoo/.config/fish/config.fish", iteration: 1, backup_location: "backup_config/config.fish", ts: "2024-03-28 06:43:22.516949493 UTC" }, ConfigFile { alias: "fisher_fish", realpath: "/home/osamanoo/.config/fish/completions/fisher.fish", iteration: 1, backup_location: "backup_config/fisher.fish", ts: "2024-03-28 06:56:46.983009804 UTC" }]
2024-03-28T01:59:33.944419419-05:00 - INFO - booting up
2024-03-28T01:59:33.945547538-05:00 - INFO - list of all aliases [ConfigFile { alias: "alacritty.toml", realpath: "/home/osamanoo/.config/alacritty/alacritty.toml", iteration: 1, backup_location: "backup_config/alacritty.toml", ts: "2024-03-25 04:06:09.037046463 UTC" }, ConfigFile { alias: "fish_config", realpath: "/home/osamanoo/.config/fish/config.fish", iteration: 1, backup_location: "backup_config/config.fish", ts: "2024-03-28 06:43:22.516949493 UTC" }, ConfigFile { alias: "fisher_fish", realpath: "/home/osamanoo/.config/fish/completions/fisher.fish", iteration: 1, backup_location: "backup_config/fisher.fish", ts: "2024-03-28 06:56:46.983009804 UTC" }]
2024-03-28T02:08:05.248208810-05:00 - INFO - booting up
2024-03-28T02:08:05.249368403-05:00 - INFO - list of all aliases [ConfigFile { alias: "alacritty.toml", realpath: "/home/osamanoo/.config/alacritty/alacritty.toml", iteration: 1, backup_location: "backup_config/alacritty.toml", ts: "2024-03-25 04:06:09.037046463 UTC" }, ConfigFile { alias: "fish_config", realpath: "/home/osamanoo/.config/fish/config.fish", iteration: 1, backup_location: "backup_config/config.fish", ts: "2024-03-28 06:43:22.516949493 UTC" }, ConfigFile { alias: "fisher_fish", realpath: "/home/osamanoo/.config/fish/completions/fisher.fish", iteration: 1, backup_location: "backup_config/fisher.fish", ts: "2024-03-28 06:56:46.983009804 UTC" }]
2024-03-28T02:10:07.255945437-05:00 - INFO - booting up
2024-03-28T02:10:07.257092470-05:00 - INFO - list of all aliases [ConfigFile { alias: "alacritty.toml", realpath: "/home/osamanoo/.config/alacritty/alacritty.toml", iteration: 1, backup_location: "backup_config/alacritty.toml", ts: "2024-03-25 04:06:09.037046463 UTC" }, ConfigFile { alias: "fish_config", realpath: "/home/osamanoo/.config/fish/config.fish", iteration: 1, backup_location: "backup_config/config.fish", ts: "2024-03-28 06:43:22.516949493 UTC" }, ConfigFile { alias: "fisher_fish", realpath: "/home/osamanoo/.config/fish/completions/fisher.fish", iteration: 1, backup_location: "backup_config/fisher.fish", ts: "2024-03-28 06:56:46.983009804 UTC" }]
2024-03-28T02:12:38.804020616-05:00 - INFO - booting up
2024-03-28T02:12:38.805665554-05:00 - INFO - list of all aliases [ConfigFile { alias: "alacritty.toml", realpath: "/home/osamanoo/.config/alacritty/alacritty.toml", iteration: 1, backup_location: "backup_config/alacritty.toml", ts: "2024-03-25 04:06:09.037046463 UTC" }, ConfigFile { alias: "fish_config", realpath: "/home/osamanoo/.config/fish/config.fish", iteration: 1, backup_location: "backup_config/config.fish", ts: "2024-03-28 06:43:22.516949493 UTC" }, ConfigFile { alias: "fisher_fish", realpath: "/home/osamanoo/.config/fish/completions/fisher.fish", iteration: 1, backup_location: "backup_config/fisher.fish", ts: "2024-03-28 06:56:46.983009804 UTC" }]
2024-03-28T02:13:45.120343844-05:00 - INFO - booting up
2024-03-28T02:13:45.121606045-05:00 - INFO - list of all aliases [ConfigFile { alias: "alacritty.toml", realpath: "/home/osamanoo/.config/alacritty/alacritty.toml", iteration: 1, backup_location: "backup_config/alacritty.toml", ts: "2024-03-25 04:06:09.037046463 UTC" }, ConfigFile { alias: "fish_config", realpath: "/home/osamanoo/.config/fish/config.fish", iteration: 1, backup_location: "backup_config/config.fish", ts: "2024-03-28 06:43:22.516949493 UTC" }, ConfigFile { alias: "fisher_fish", realpath: "/home/osamanoo/.config/fish/completions/fisher.fish", iteration: 1, backup_location: "backup_config/fisher.fish", ts: "2024-03-28 06:56:46.983009804 UTC" }]
2024-03-28T02:14:14.118130386-05:00 - INFO - booting up
2024-03-28T02:14:14.119310417-05:00 - INFO - list of all aliases [ConfigFile { alias: "alacritty.toml", realpath: "/home/osamanoo/.config/alacritty/alacritty.toml", iteration: 1, backup_location: "backup_config/alacritty.toml", ts: "2024-03-25 04:06:09.037046463 UTC" }, ConfigFile { alias: "fish_config", realpath: "/home/osamanoo/.config/fish/config.fish", iteration: 1, backup_location: "backup_config/config.fish", ts: "2024-03-28 06:43:22.516949493 UTC" }, ConfigFile { alias: "fisher_fish", realpath: "/home/osamanoo/.config/fish/completions/fisher.fish", iteration: 1, backup_location: "backup_config/fisher.fish", ts: "2024-03-28 06:56:46.983009804 UTC" }]
2024-03-28T02:14:21.246366615-05:00 - INFO - booting up
2024-03-28T02:14:21.247442857-05:00 - INFO - list of all aliases [ConfigFile { alias: "alacritty.toml", realpath: "/home/osamanoo/.config/alacritty/alacritty.toml", iteration: 1, backup_location: "backup_config/alacritty.toml", ts: "2024-03-25 04:06:09.037046463 UTC" }, ConfigFile { alias: "fish_config", realpath: "/home/osamanoo/.config/fish/config.fish", iteration: 1, backup_location: "backup_config/config.fish", ts: "2024-03-28 06:43:22.516949493 UTC" }, ConfigFile { alias: "fisher_fish", realpath: "/home/osamanoo/.config/fish/completions/fisher.fish", iteration: 1, backup_location: "backup_config/fisher.fish", ts: "2024-03-28 06:56:46.983009804 UTC" }]
2024-03-28T02:14:57.832466913-05:00 - INFO - booting up
2024-03-28T02:14:57.833580804-05:00 - INFO - alias was selected
2024-03-28T02:14:57.833640814-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-28T02:14:57.852869181-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-28T02:14:57.852922340-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-28T02:14:57.852954180-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-28T02:14:57.853052139-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-28T02:14:57.899031007-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-28T02:14:57.905111162-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-28T02:14:57.917360981-05:00 - TRACE - client handshake Http1
2024-03-28T02:14:57.917400741-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-28T02:14:57.917501250-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-28T02:14:57.917632909-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-28T02:14:57.917704768-05:00 - TRACE - encode_headers;
2024-03-28T02:14:57.917720588-05:00 - TRACE - -> encode_headers;
2024-03-28T02:14:57.917729308-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3850))
2024-03-28T02:14:57.917754008-05:00 - TRACE - <- encode_headers;
2024-03-28T02:14:57.917761958-05:00 - TRACE - -- encode_headers;
2024-03-28T02:14:57.917772338-05:00 - TRACE - sized write, len = 3850
2024-03-28T02:14:57.917780438-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3850
2024-03-28T02:14:57.917831177-05:00 - DEBUG - flushed 4052 bytes
2024-03-28T02:14:57.917845017-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:15:00.717649066-05:00 - TRACE - Conn::read_head
2024-03-28T02:15:00.717855864-05:00 - TRACE - received 1369 bytes
2024-03-28T02:15:00.717876454-05:00 - TRACE - parse_headers;
2024-03-28T02:15:00.717892304-05:00 - TRACE - -> parse_headers;
2024-03-28T02:15:00.717917504-05:00 - TRACE - Response.parse bytes=1369
2024-03-28T02:15:00.717970254-05:00 - TRACE - Response.parse Complete(1202)
2024-03-28T02:15:00.718043473-05:00 - TRACE - <- parse_headers;
2024-03-28T02:15:00.718054023-05:00 - TRACE - -- parse_headers;
2024-03-28T02:15:00.718062053-05:00 - DEBUG - parsed 24 headers
2024-03-28T02:15:00.718070113-05:00 - DEBUG - incoming body is content-length (620 bytes)
2024-03-28T02:15:00.718100633-05:00 - TRACE - decode; state=Length(620)
2024-03-28T02:15:00.718130772-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(453)), writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:15:00.718221012-05:00 - TRACE - decode; state=Length(453)
2024-03-28T02:15:00.718331261-05:00 - TRACE - received 453 bytes
2024-03-28T02:15:00.718346161-05:00 - DEBUG - incoming body completed
2024-03-28T02:15:00.718377721-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-28T02:15:00.718413070-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:15:00.718431130-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:15:00.718451200-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-28T02:15:00.718471210-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-28T02:15:00.718528830-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:15:00.718547139-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "background = \"0x1a1a1a\"",
      "new": "background = \"0x800080\""
    }
  }
}
2024-03-28T02:15:00.718586639-05:00 - INFO - key change1
2024-03-28T02:15:00.718595789-05:00 - INFO - value "background = \"0x1a1a1a\""
2024-03-28T02:15:00.718621709-05:00 - INFO - old value is "background = \"0x1a1a1a\""
2024-03-28T02:15:00.718633609-05:00 - INFO - writing to file
2024-03-28T02:15:00.718732378-05:00 - INFO - New value: background = "0x800080"
2024-03-28T02:15:00.718767158-05:00 - INFO - 

2024-03-28T02:15:00.718781098-05:00 - INFO - is bool true true
2024-03-28T02:15:00.718806628-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"background = \\\"0x1a1a1a\\\"\",\n      \"new\": \"background = \\\"0x800080\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711610098,"id":"chatcmpl-97eGQxoL9LNQASy3Rm1581fH1Fc3e","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":48,"prompt_tokens":1062,"total_tokens":1110}}
2024-03-28T02:15:00.718851507-05:00 - TRACE - client tx closed
2024-03-28T02:15:00.718866337-05:00 - TRACE - State::close_read()
2024-03-28T02:15:00.718876917-05:00 - TRACE - State::close_write()
2024-03-28T02:15:00.718887817-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-28T02:15:00.718929587-05:00 - TRACE - shut down IO complete
2024-03-28T02:15:07.982779917-05:00 - INFO - booting up
2024-03-28T02:15:07.983872759-05:00 - INFO - alias was selected
2024-03-28T02:15:07.983920108-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-28T02:15:08.001976964-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-28T02:15:08.002036353-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-28T02:15:08.002064483-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-28T02:15:08.002172812-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-28T02:15:08.008430204-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-28T02:15:08.016256476-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-28T02:15:08.029474187-05:00 - TRACE - client handshake Http1
2024-03-28T02:15:08.029505457-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-28T02:15:08.029588926-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-28T02:15:08.029654356-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-28T02:15:08.029723025-05:00 - TRACE - encode_headers;
2024-03-28T02:15:08.029738465-05:00 - TRACE - -> encode_headers;
2024-03-28T02:15:08.029747075-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3852))
2024-03-28T02:15:08.029771395-05:00 - TRACE - <- encode_headers;
2024-03-28T02:15:08.029779515-05:00 - TRACE - -- encode_headers;
2024-03-28T02:15:08.029789645-05:00 - TRACE - sized write, len = 3852
2024-03-28T02:15:08.029797745-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3852
2024-03-28T02:15:08.029848444-05:00 - DEBUG - flushed 4054 bytes
2024-03-28T02:15:08.029862344-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:15:09.400922846-05:00 - TRACE - Conn::read_head
2024-03-28T02:15:09.401192544-05:00 - TRACE - received 1369 bytes
2024-03-28T02:15:09.401215543-05:00 - TRACE - parse_headers;
2024-03-28T02:15:09.401226543-05:00 - TRACE - -> parse_headers;
2024-03-28T02:15:09.401248763-05:00 - TRACE - Response.parse bytes=1369
2024-03-28T02:15:09.401322013-05:00 - TRACE - Response.parse Complete(1201)
2024-03-28T02:15:09.401413582-05:00 - TRACE - <- parse_headers;
2024-03-28T02:15:09.401425702-05:00 - TRACE - -- parse_headers;
2024-03-28T02:15:09.401435382-05:00 - DEBUG - parsed 24 headers
2024-03-28T02:15:09.401444872-05:00 - DEBUG - incoming body is content-length (620 bytes)
2024-03-28T02:15:09.401480531-05:00 - TRACE - decode; state=Length(620)
2024-03-28T02:15:09.401507881-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(452)), writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:15:09.401620680-05:00 - TRACE - decode; state=Length(452)
2024-03-28T02:15:09.401745209-05:00 - TRACE - received 452 bytes
2024-03-28T02:15:09.401763059-05:00 - DEBUG - incoming body completed
2024-03-28T02:15:09.401808929-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-28T02:15:09.401850939-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:15:09.401872718-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:15:09.401895728-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-28T02:15:09.401917508-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-28T02:15:09.401986598-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:15:09.402011167-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "background = \"0x800080\"",
      "new": "background = \"0x1a1a1a\""
    }
  }
}
2024-03-28T02:15:09.402054837-05:00 - INFO - key change1
2024-03-28T02:15:09.402065907-05:00 - INFO - value "background = \"0x800080\""
2024-03-28T02:15:09.402104057-05:00 - INFO - old value is "background = \"0x800080\""
2024-03-28T02:15:09.402120337-05:00 - INFO - writing to file
2024-03-28T02:15:09.402248306-05:00 - INFO - New value: background = "0x1a1a1a"
2024-03-28T02:15:09.402290845-05:00 - INFO - 

2024-03-28T02:15:09.402300665-05:00 - INFO - is bool true true
2024-03-28T02:15:09.402333415-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"background = \\\"0x800080\\\"\",\n      \"new\": \"background = \\\"0x1a1a1a\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711610108,"id":"chatcmpl-97eGaGvMpmi8yAPxy7Km1eo5pH4qQ","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_b28b39ffa8","usage":{"completion_tokens":48,"prompt_tokens":1058,"total_tokens":1106}}
2024-03-28T02:15:28.703816593-05:00 - INFO - booting up
2024-03-28T02:15:28.704945064-05:00 - INFO - alias was selected
2024-03-28T02:15:28.704994564-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-28T02:15:28.723132958-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-28T02:15:28.723190768-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-28T02:15:28.723221658-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-28T02:15:28.723326277-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-28T02:15:28.732157031-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-28T02:15:28.739687293-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-28T02:15:28.755736542-05:00 - TRACE - client handshake Http1
2024-03-28T02:15:28.755769452-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-28T02:15:28.755861021-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-28T02:15:28.756001950-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-28T02:15:28.756071510-05:00 - TRACE - encode_headers;
2024-03-28T02:15:28.756116879-05:00 - TRACE - -> encode_headers;
2024-03-28T02:15:28.756127059-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3851))
2024-03-28T02:15:28.756168129-05:00 - TRACE - <- encode_headers;
2024-03-28T02:15:28.756178479-05:00 - TRACE - -- encode_headers;
2024-03-28T02:15:28.756188909-05:00 - TRACE - sized write, len = 3851
2024-03-28T02:15:28.756207729-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3851
2024-03-28T02:15:28.756258758-05:00 - DEBUG - flushed 4053 bytes
2024-03-28T02:15:28.756272508-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:15:29.900032008-05:00 - TRACE - Conn::read_head
2024-03-28T02:15:29.900251176-05:00 - TRACE - received 1369 bytes
2024-03-28T02:15:29.900277406-05:00 - TRACE - parse_headers;
2024-03-28T02:15:29.900289346-05:00 - TRACE - -> parse_headers;
2024-03-28T02:15:29.900311066-05:00 - TRACE - Response.parse bytes=1369
2024-03-28T02:15:29.900391175-05:00 - TRACE - Response.parse Complete(1201)
2024-03-28T02:15:29.900467245-05:00 - TRACE - <- parse_headers;
2024-03-28T02:15:29.900479775-05:00 - TRACE - -- parse_headers;
2024-03-28T02:15:29.900489235-05:00 - DEBUG - parsed 24 headers
2024-03-28T02:15:29.900498475-05:00 - DEBUG - incoming body is content-length (620 bytes)
2024-03-28T02:15:29.900525984-05:00 - TRACE - decode; state=Length(620)
2024-03-28T02:15:29.900547534-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(452)), writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:15:29.900642893-05:00 - TRACE - decode; state=Length(452)
2024-03-28T02:15:29.900770172-05:00 - TRACE - received 452 bytes
2024-03-28T02:15:29.900788482-05:00 - DEBUG - incoming body completed
2024-03-28T02:15:29.900823402-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-28T02:15:29.900862462-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:15:29.900884602-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:15:29.900905821-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-28T02:15:29.900924171-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-28T02:15:29.900990541-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:15:29.901010381-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "background = \"0x1a1a1a\"",
      "new": "background = \"0x6a0dad\""
    }
  }
}
2024-03-28T02:15:29.901058170-05:00 - INFO - key change1
2024-03-28T02:15:29.901069120-05:00 - INFO - value "background = \"0x1a1a1a\""
2024-03-28T02:15:29.901101220-05:00 - INFO - old value is "background = \"0x1a1a1a\""
2024-03-28T02:15:29.901116060-05:00 - INFO - writing to file
2024-03-28T02:15:29.901235569-05:00 - INFO - New value: background = "0x6a0dad"
2024-03-28T02:15:29.901279429-05:00 - INFO - 

2024-03-28T02:15:29.901299938-05:00 - INFO - is bool true true
2024-03-28T02:15:29.901331978-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"background = \\\"0x1a1a1a\\\"\",\n      \"new\": \"background = \\\"0x6a0dad\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711610129,"id":"chatcmpl-97eGvfHgHgELJmoKloRpynP9LRPmN","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":50,"prompt_tokens":1063,"total_tokens":1113}}
2024-03-28T02:15:40.863405914-05:00 - INFO - booting up
2024-03-28T02:15:54.567669063-05:00 - INFO - booting up
2024-03-28T02:15:54.568954353-05:00 - INFO - user request this alias to be reverted alacritty.toml 
2024-03-28T02:15:54.569082832-05:00 - INFO - checking if alias exist, so it could be reverted
2024-03-28T02:15:54.569093512-05:00 - INFO - content alias is alacritty.toml 
2024-03-28T02:15:54.569204481-05:00 - INFO - sucessfully reverted the original file to its backup file
2024-03-28T02:21:42.589199664-05:00 - INFO - booting up
2024-03-28T02:21:42.590298495-05:00 - INFO - list of all aliases [ConfigFile { alias: "alacritty.toml", realpath: "/home/osamanoo/.config/alacritty/alacritty.toml", iteration: 1, backup_location: "backup_config/alacritty.toml", ts: "2024-03-25 04:06:09.037046463 UTC" }, ConfigFile { alias: "fish_config", realpath: "/home/osamanoo/.config/fish/config.fish", iteration: 1, backup_location: "backup_config/config.fish", ts: "2024-03-28 06:43:22.516949493 UTC" }, ConfigFile { alias: "fisher_fish", realpath: "/home/osamanoo/.config/fish/completions/fisher.fish", iteration: 1, backup_location: "backup_config/fisher.fish", ts: "2024-03-28 06:56:46.983009804 UTC" }]
2024-03-28T02:25:14.968468936-05:00 - INFO - booting up
2024-03-28T02:25:14.969552727-05:00 - INFO - alias was selected
2024-03-28T02:25:14.969606136-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-28T02:25:14.988068064-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-28T02:25:14.988123143-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-28T02:25:14.988155533-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-28T02:25:14.988260182-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-28T02:25:15.006886588-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-28T02:25:15.013654792-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-28T02:25:15.028683438-05:00 - TRACE - client handshake Http1
2024-03-28T02:25:15.028718828-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-28T02:25:15.028814307-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-28T02:25:15.028882746-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-28T02:25:15.028958866-05:00 - TRACE - encode_headers;
2024-03-28T02:25:15.028974696-05:00 - TRACE - -> encode_headers;
2024-03-28T02:25:15.028983395-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3843))
2024-03-28T02:25:15.029007695-05:00 - TRACE - <- encode_headers;
2024-03-28T02:25:15.029015865-05:00 - TRACE - -- encode_headers;
2024-03-28T02:25:15.029026285-05:00 - TRACE - sized write, len = 3843
2024-03-28T02:25:15.029035025-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3843
2024-03-28T02:25:15.029123974-05:00 - DEBUG - flushed 4045 bytes
2024-03-28T02:25:15.029141204-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:25:15.941203351-05:00 - TRACE - Conn::read_head
2024-03-28T02:25:15.941380300-05:00 - TRACE - received 1369 bytes
2024-03-28T02:25:15.941397480-05:00 - TRACE - parse_headers;
2024-03-28T02:25:15.941406089-05:00 - TRACE - -> parse_headers;
2024-03-28T02:25:15.941423779-05:00 - TRACE - Response.parse bytes=1369
2024-03-28T02:25:15.941465399-05:00 - TRACE - Response.parse Complete(1202)
2024-03-28T02:25:15.941523388-05:00 - TRACE - <- parse_headers;
2024-03-28T02:25:15.941533258-05:00 - TRACE - -- parse_headers;
2024-03-28T02:25:15.941541088-05:00 - DEBUG - parsed 24 headers
2024-03-28T02:25:15.941549168-05:00 - DEBUG - incoming body is content-length (608 bytes)
2024-03-28T02:25:15.941579838-05:00 - TRACE - decode; state=Length(608)
2024-03-28T02:25:15.941598208-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(441)), writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:25:15.941653397-05:00 - TRACE - decode; state=Length(441)
2024-03-28T02:25:15.941689127-05:00 - TRACE - received 441 bytes
2024-03-28T02:25:15.941702197-05:00 - DEBUG - incoming body completed
2024-03-28T02:25:15.941731917-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-28T02:25:15.941757466-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:25:15.941771816-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:25:15.941785406-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-28T02:25:15.941797696-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-28T02:25:15.941841426-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:25:15.941859566-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "blue = \"0x268bd2\"",
      "new": "blue = \"0x268bbd\""
    }
  }
}
2024-03-28T02:25:15.941885925-05:00 - INFO - key change1
2024-03-28T02:25:15.941894385-05:00 - INFO - value "blue = \"0x268bd2\""
2024-03-28T02:25:15.941926745-05:00 - INFO - old value is "blue = \"0x268bd2\""
2024-03-28T02:25:15.941938075-05:00 - INFO - writing to file
2024-03-28T02:25:15.942033714-05:00 - INFO - New value: blue = "0x268bbd"
2024-03-28T02:25:15.942061404-05:00 - INFO - 

2024-03-28T02:25:15.942069284-05:00 - INFO - is bool true true
2024-03-28T02:25:15.942098174-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"blue = \\\"0x268bd2\\\"\",\n      \"new\": \"blue = \\\"0x268bbd\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711610715,"id":"chatcmpl-97eQNXWYby4VRfQsUx5B0CvduHguj","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":46,"prompt_tokens":1061,"total_tokens":1107}}
2024-03-28T02:25:15.942165173-05:00 - TRACE - client tx closed
2024-03-28T02:25:15.942192593-05:00 - TRACE - State::close_read()
2024-03-28T02:25:15.942206253-05:00 - TRACE - State::close_write()
2024-03-28T02:25:15.942216123-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-28T02:25:15.942265292-05:00 - TRACE - shut down IO complete
2024-03-28T02:25:51.828328930-05:00 - INFO - booting up
2024-03-28T02:25:51.829514260-05:00 - INFO - alias was selected
2024-03-28T02:25:51.829578519-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-28T02:25:51.849206427-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-28T02:25:51.849263696-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-28T02:25:51.849296616-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-28T02:25:51.849418045-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-28T02:25:51.854291794-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-28T02:25:51.860413493-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-28T02:25:51.873558025-05:00 - TRACE - client handshake Http1
2024-03-28T02:25:51.873591475-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-28T02:25:51.874033111-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-28T02:25:51.874107030-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-28T02:25:51.874181300-05:00 - TRACE - encode_headers;
2024-03-28T02:25:51.874196580-05:00 - TRACE - -> encode_headers;
2024-03-28T02:25:51.874205190-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3842))
2024-03-28T02:25:51.874229729-05:00 - TRACE - <- encode_headers;
2024-03-28T02:25:51.874243019-05:00 - TRACE - -- encode_headers;
2024-03-28T02:25:51.874253129-05:00 - TRACE - sized write, len = 3842
2024-03-28T02:25:51.874261629-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3842
2024-03-28T02:25:51.874313739-05:00 - DEBUG - flushed 4044 bytes
2024-03-28T02:25:51.874327229-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:25:53.626800440-05:00 - TRACE - Conn::read_head
2024-03-28T02:25:53.627008398-05:00 - TRACE - received 1369 bytes
2024-03-28T02:25:53.627027358-05:00 - TRACE - parse_headers;
2024-03-28T02:25:53.627037928-05:00 - TRACE - -> parse_headers;
2024-03-28T02:25:53.627061998-05:00 - TRACE - Response.parse bytes=1369
2024-03-28T02:25:53.627113297-05:00 - TRACE - Response.parse Complete(1203)
2024-03-28T02:25:53.627197597-05:00 - TRACE - <- parse_headers;
2024-03-28T02:25:53.627210097-05:00 - TRACE - -- parse_headers;
2024-03-28T02:25:53.627219617-05:00 - DEBUG - parsed 24 headers
2024-03-28T02:25:53.627229276-05:00 - DEBUG - incoming body is content-length (606 bytes)
2024-03-28T02:25:53.627262296-05:00 - TRACE - decode; state=Length(606)
2024-03-28T02:25:53.627296786-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(440)), writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:25:53.627342586-05:00 - TRACE - decode; state=Length(440)
2024-03-28T02:25:53.627387275-05:00 - TRACE - received 440 bytes
2024-03-28T02:25:53.627410415-05:00 - DEBUG - incoming body completed
2024-03-28T02:25:53.627437265-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-28T02:25:53.627482504-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:25:53.627530834-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:25:53.627554624-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-28T02:25:53.627574044-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-28T02:25:53.627655533-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:25:53.627691233-05:00 - TRACE - idle interval checking for expired
2024-03-28T02:25:53.627790302-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "red = \"0xdc322f\"",
      "new": "red = \"0xff0000\""
    }
  }
}
2024-03-28T02:25:53.627821882-05:00 - INFO - key change1
2024-03-28T02:25:53.627829242-05:00 - INFO - value "red = \"0xdc322f\""
2024-03-28T02:25:53.627854101-05:00 - INFO - old value is "red = \"0xdc322f\""
2024-03-28T02:25:53.627864021-05:00 - INFO - writing to file
2024-03-28T02:25:53.627955451-05:00 - INFO - New value: red = "0xff0000"
2024-03-28T02:25:53.627979710-05:00 - INFO - 

2024-03-28T02:25:53.627985900-05:00 - INFO - is bool true true
2024-03-28T02:25:53.628010560-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"red = \\\"0xdc322f\\\"\",\n      \"new\": \"red = \\\"0xff0000\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711610752,"id":"chatcmpl-97eQyAeDJB7usjUjuqmbx62kduRXQ","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":44,"prompt_tokens":1060,"total_tokens":1104}}
2024-03-28T02:26:04.836869819-05:00 - INFO - booting up
2024-03-28T02:26:04.837970250-05:00 - INFO - alias was selected
2024-03-28T02:26:04.838017469-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-28T02:26:04.856220478-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-28T02:26:04.856279787-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-28T02:26:04.856308057-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-28T02:26:04.856421186-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-28T02:26:04.859758599-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-28T02:26:04.868498316-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-28T02:26:04.881579138-05:00 - TRACE - client handshake Http1
2024-03-28T02:26:04.881610728-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-28T02:26:04.881713447-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-28T02:26:04.881785816-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-28T02:26:04.881858546-05:00 - TRACE - encode_headers;
2024-03-28T02:26:04.881874446-05:00 - TRACE - -> encode_headers;
2024-03-28T02:26:04.881883326-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3825))
2024-03-28T02:26:04.881912545-05:00 - TRACE - <- encode_headers;
2024-03-28T02:26:04.881921485-05:00 - TRACE - -- encode_headers;
2024-03-28T02:26:04.881931675-05:00 - TRACE - sized write, len = 3825
2024-03-28T02:26:04.881940085-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3825
2024-03-28T02:26:04.881991485-05:00 - DEBUG - flushed 4027 bytes
2024-03-28T02:26:04.882005425-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:26:06.322454070-05:00 - TRACE - Conn::read_head
2024-03-28T02:26:06.322646648-05:00 - TRACE - received 1369 bytes
2024-03-28T02:26:06.322665818-05:00 - TRACE - parse_headers;
2024-03-28T02:26:06.322676318-05:00 - TRACE - -> parse_headers;
2024-03-28T02:26:06.322697148-05:00 - TRACE - Response.parse bytes=1369
2024-03-28T02:26:06.322747778-05:00 - TRACE - Response.parse Complete(1203)
2024-03-28T02:26:06.322828287-05:00 - TRACE - <- parse_headers;
2024-03-28T02:26:06.322854457-05:00 - TRACE - -- parse_headers;
2024-03-28T02:26:06.322865197-05:00 - DEBUG - parsed 24 headers
2024-03-28T02:26:06.322874697-05:00 - DEBUG - incoming body is content-length (606 bytes)
2024-03-28T02:26:06.322902026-05:00 - TRACE - decode; state=Length(606)
2024-03-28T02:26:06.322923026-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(440)), writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:26:06.323011085-05:00 - TRACE - decode; state=Length(440)
2024-03-28T02:26:06.323139884-05:00 - TRACE - received 440 bytes
2024-03-28T02:26:06.323157644-05:00 - DEBUG - incoming body completed
2024-03-28T02:26:06.323189514-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-28T02:26:06.323227804-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:26:06.323249854-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:26:06.323275823-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-28T02:26:06.323294523-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-28T02:26:06.323366643-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:26:06.323386842-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "red = \"0xff0000\"",
      "new": "red = \"0xff0000\""
    }
  }
}
2024-03-28T02:26:06.323432022-05:00 - INFO - key change1
2024-03-28T02:26:06.323443292-05:00 - INFO - value "red = \"0xff0000\""
2024-03-28T02:26:06.323478562-05:00 - INFO - old value is "red = \"0xff0000\""
2024-03-28T02:26:06.323499282-05:00 - INFO - writing to file
2024-03-28T02:26:06.323638190-05:00 - INFO - New value: red = "0xff0000"
2024-03-28T02:26:06.323680870-05:00 - INFO - 

2024-03-28T02:26:06.323690960-05:00 - INFO - is bool true true
2024-03-28T02:26:06.323723120-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"red = \\\"0xff0000\\\"\",\n      \"new\": \"red = \\\"0xff0000\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711610765,"id":"chatcmpl-97eRByNmvLZtKlRD5pRZbXginfIvu","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":44,"prompt_tokens":1057,"total_tokens":1101}}
2024-03-28T02:26:10.111018319-05:00 - INFO - booting up
2024-03-28T02:26:10.112183349-05:00 - INFO - alias was selected
2024-03-28T02:26:10.112248969-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-28T02:26:10.130676335-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-28T02:26:10.130731695-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-28T02:26:10.130766234-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-28T02:26:10.130869603-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-28T02:26:10.135168608-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-28T02:26:10.140920561-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-28T02:26:10.156327303-05:00 - TRACE - client handshake Http1
2024-03-28T02:26:10.156359643-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-28T02:26:10.156462532-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-28T02:26:10.156534331-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-28T02:26:10.156615371-05:00 - TRACE - encode_headers;
2024-03-28T02:26:10.156632091-05:00 - TRACE - -> encode_headers;
2024-03-28T02:26:10.156641291-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3829))
2024-03-28T02:26:10.156667940-05:00 - TRACE - <- encode_headers;
2024-03-28T02:26:10.156676320-05:00 - TRACE - -- encode_headers;
2024-03-28T02:26:10.156687130-05:00 - TRACE - sized write, len = 3829
2024-03-28T02:26:10.156704680-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3829
2024-03-28T02:26:10.156759279-05:00 - DEBUG - flushed 4031 bytes
2024-03-28T02:26:10.156774429-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:26:13.388101621-05:00 - TRACE - Conn::read_head
2024-03-28T02:26:13.388347079-05:00 - TRACE - received 1369 bytes
2024-03-28T02:26:13.388367029-05:00 - TRACE - parse_headers;
2024-03-28T02:26:13.388378319-05:00 - TRACE - -> parse_headers;
2024-03-28T02:26:13.388403289-05:00 - TRACE - Response.parse bytes=1369
2024-03-28T02:26:13.388459238-05:00 - TRACE - Response.parse Complete(1206)
2024-03-28T02:26:13.388544218-05:00 - TRACE - <- parse_headers;
2024-03-28T02:26:13.388560858-05:00 - TRACE - -- parse_headers;
2024-03-28T02:26:13.388570607-05:00 - DEBUG - parsed 24 headers
2024-03-28T02:26:13.388580337-05:00 - DEBUG - incoming body is content-length (1195 bytes)
2024-03-28T02:26:13.388613357-05:00 - TRACE - decode; state=Length(1195)
2024-03-28T02:26:13.388639037-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(1032)), writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:26:13.388740236-05:00 - TRACE - decode; state=Length(1032)
2024-03-28T02:26:13.388856755-05:00 - TRACE - received 1032 bytes
2024-03-28T02:26:13.388875635-05:00 - DEBUG - incoming body completed
2024-03-28T02:26:13.388910925-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-28T02:26:13.388947124-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:26:13.388965084-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:26:13.388983794-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-28T02:26:13.389001054-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-28T02:26:13.389060553-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:26:13.389098273-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "{\n  \"results\": {\n    \"change1\": {\"old\": \"<text before change>\", \"new\": \"<text after change>\"},\n    \"change2\": {\"old\": \"<text before change>\", \"new\": \"<text after change>\"}\n  }\n}",
      "new": "{\n  \"results\": {\n    \"change1\": {\"old\": \"live_config_reload = true\", \"new\": \"live_config_reload = true\"},\n    \"change2\": {\"old\": \"[[colors.indexed_colors]]\\ncolor = \\\"0xcb4b16\\\"\\nindex = 16\", \"new\": \"[[colors.indexed_colors]]\\ncolor = \\\"0xcb4b16\\\"\\nindex = 16\"}\n  }\n}"
    }
  }
}
2024-03-28T02:26:13.389151673-05:00 - INFO - key change1
2024-03-28T02:26:13.389163043-05:00 - INFO - value "{\n  \"results\": {\n    \"change1\": {\"old\": \"<text before change>\", \"new\": \"<text after change>\"},\n    \"change2\": {\"old\": \"<text before change>\", \"new\": \"<text after change>\"}\n  }\n}"
2024-03-28T02:26:13.389213422-05:00 - INFO - old value is "{\n  \"results\": {\n    \"change1\": {\"old\": \"<text before change>\", \"new\": \"<text after change>\"},\n    \"change2\": {\"old\": \"<text before change>\", \"new\": \"<text after change>\"}\n  }\n}"
2024-03-28T02:26:13.389236222-05:00 - INFO - New value: {
  "results": {
    "change1": {"old": "live_config_reload = true", "new": "live_config_reload = true"},
    "change2": {"old": "[[colors.indexed_colors]]\ncolor = \"0xcb4b16\"\nindex = 16", "new": "[[colors.indexed_colors]]\ncolor = \"0xcb4b16\"\nindex = 16"}
  }
}
2024-03-28T02:26:13.389258912-05:00 - INFO - 

2024-03-28T02:26:13.389267832-05:00 - INFO - is bool true false
2024-03-28T02:26:13.389316831-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"{\\n  \\\"results\\\": {\\n    \\\"change1\\\": {\\\"old\\\": \\\"<text before change>\\\", \\\"new\\\": \\\"<text after change>\\\"},\\n    \\\"change2\\\": {\\\"old\\\": \\\"<text before change>\\\", \\\"new\\\": \\\"<text after change>\\\"}\\n  }\\n}\",\n      \"new\": \"{\\n  \\\"results\\\": {\\n    \\\"change1\\\": {\\\"old\\\": \\\"live_config_reload = true\\\", \\\"new\\\": \\\"live_config_reload = true\\\"},\\n    \\\"change2\\\": {\\\"old\\\": \\\"[[colors.indexed_colors]]\\\\ncolor = \\\\\\\"0xcb4b16\\\\\\\"\\\\nindex = 16\\\", \\\"new\\\": \\\"[[colors.indexed_colors]]\\\\ncolor = \\\\\\\"0xcb4b16\\\\\\\"\\\\nindex = 16\\\"}\\n  }\\n}\"\n    }\n  }\n}","role":"assistant"}}],"created":1711610770,"id":"chatcmpl-97eRGKlO9rU1QiCuX1ARSZJETOnTM","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":205,"prompt_tokens":1058,"total_tokens":1263}}
2024-03-28T02:26:13.389414401-05:00 - TRACE - client tx closed
2024-03-28T02:26:13.389441540-05:00 - TRACE - State::close_read()
2024-03-28T02:26:13.389456870-05:00 - TRACE - State::close_write()
2024-03-28T02:26:13.389472020-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-28T02:26:13.389512050-05:00 - TRACE - shut down IO complete
2024-03-28T02:26:24.844830097-05:00 - INFO - booting up
2024-03-28T02:26:24.846052137-05:00 - INFO - alias was selected
2024-03-28T02:26:24.846112047-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-28T02:26:24.864893801-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-28T02:26:24.864951671-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-28T02:26:24.864990371-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-28T02:26:24.865100570-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-28T02:26:24.878824446-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-28T02:26:24.885737058-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-28T02:26:24.898847819-05:00 - TRACE - client handshake Http1
2024-03-28T02:26:24.898878239-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-28T02:26:24.898974018-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-28T02:26:24.899046318-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-28T02:26:24.899119877-05:00 - TRACE - encode_headers;
2024-03-28T02:26:24.899135477-05:00 - TRACE - -> encode_headers;
2024-03-28T02:26:24.899144017-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3833))
2024-03-28T02:26:24.899168947-05:00 - TRACE - <- encode_headers;
2024-03-28T02:26:24.899181577-05:00 - TRACE - -- encode_headers;
2024-03-28T02:26:24.899191767-05:00 - TRACE - sized write, len = 3833
2024-03-28T02:26:24.899200277-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3833
2024-03-28T02:26:24.899250156-05:00 - DEBUG - flushed 4035 bytes
2024-03-28T02:26:24.899264026-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:26:26.290560712-05:00 - TRACE - Conn::read_head
2024-03-28T02:26:26.290739331-05:00 - TRACE - received 1369 bytes
2024-03-28T02:26:26.290756611-05:00 - TRACE - parse_headers;
2024-03-28T02:26:26.290765580-05:00 - TRACE - -> parse_headers;
2024-03-28T02:26:26.290783930-05:00 - TRACE - Response.parse bytes=1369
2024-03-28T02:26:26.290828520-05:00 - TRACE - Response.parse Complete(1203)
2024-03-28T02:26:26.290897369-05:00 - TRACE - <- parse_headers;
2024-03-28T02:26:26.290907519-05:00 - TRACE - -- parse_headers;
2024-03-28T02:26:26.290915489-05:00 - DEBUG - parsed 24 headers
2024-03-28T02:26:26.290923369-05:00 - DEBUG - incoming body is content-length (620 bytes)
2024-03-28T02:26:26.290947819-05:00 - TRACE - decode; state=Length(620)
2024-03-28T02:26:26.290966009-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(454)), writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:26:26.291092028-05:00 - TRACE - decode; state=Length(454)
2024-03-28T02:26:26.291220247-05:00 - TRACE - received 454 bytes
2024-03-28T02:26:26.291248606-05:00 - DEBUG - incoming body completed
2024-03-28T02:26:26.291282116-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-28T02:26:26.291320296-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:26:26.291341636-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:26:26.291362935-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-28T02:26:26.291381065-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-28T02:26:26.291447645-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:26:26.291468924-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "background = \"0x1a1a1a\"",
      "new": "background = \"0xff0000\""
    }
  }
}
2024-03-28T02:26:26.291510024-05:00 - INFO - key change1
2024-03-28T02:26:26.291521474-05:00 - INFO - value "background = \"0x1a1a1a\""
2024-03-28T02:26:26.291558444-05:00 - INFO - old value is "background = \"0x1a1a1a\""
2024-03-28T02:26:26.291578574-05:00 - INFO - writing to file
2024-03-28T02:26:26.291694763-05:00 - INFO - New value: background = "0xff0000"
2024-03-28T02:26:26.291731072-05:00 - INFO - 

2024-03-28T02:26:26.291744382-05:00 - INFO - is bool true true
2024-03-28T02:26:26.291778182-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"background = \\\"0x1a1a1a\\\"\",\n      \"new\": \"background = \\\"0xff0000\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711610785,"id":"chatcmpl-97eRVVGS9QAcgNISz4xkg82RW7K4t","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_b28b39ffa8","usage":{"completion_tokens":48,"prompt_tokens":1058,"total_tokens":1106}}
2024-03-28T02:26:32.006563494-05:00 - INFO - booting up
2024-03-28T02:26:32.007780964-05:00 - INFO - alias was selected
2024-03-28T02:26:32.007833193-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-28T02:26:32.026514718-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-28T02:26:32.026572557-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-28T02:26:32.026605597-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-28T02:26:32.026714316-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-28T02:26:32.030987261-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-28T02:26:32.036896471-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-28T02:26:32.051074505-05:00 - TRACE - client handshake Http1
2024-03-28T02:26:32.051103034-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-28T02:26:32.051198664-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-28T02:26:32.051283543-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-28T02:26:32.051354612-05:00 - TRACE - encode_headers;
2024-03-28T02:26:32.051370552-05:00 - TRACE - -> encode_headers;
2024-03-28T02:26:32.051379302-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3835))
2024-03-28T02:26:32.051404122-05:00 - TRACE - <- encode_headers;
2024-03-28T02:26:32.051416702-05:00 - TRACE - -- encode_headers;
2024-03-28T02:26:32.051426872-05:00 - TRACE - sized write, len = 3835
2024-03-28T02:26:32.051435262-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3835
2024-03-28T02:26:32.051485381-05:00 - DEBUG - flushed 4037 bytes
2024-03-28T02:26:32.051499421-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:26:33.356524827-05:00 - TRACE - Conn::read_head
2024-03-28T02:26:33.356752155-05:00 - TRACE - received 1369 bytes
2024-03-28T02:26:33.356771795-05:00 - TRACE - parse_headers;
2024-03-28T02:26:33.356782125-05:00 - TRACE - -> parse_headers;
2024-03-28T02:26:33.356817325-05:00 - TRACE - Response.parse bytes=1369
2024-03-28T02:26:33.356874134-05:00 - TRACE - Response.parse Complete(1205)
2024-03-28T02:26:33.356959494-05:00 - TRACE - <- parse_headers;
2024-03-28T02:26:33.356972004-05:00 - TRACE - -- parse_headers;
2024-03-28T02:26:33.356981774-05:00 - DEBUG - parsed 24 headers
2024-03-28T02:26:33.356991303-05:00 - DEBUG - incoming body is content-length (620 bytes)
2024-03-28T02:26:33.357024413-05:00 - TRACE - decode; state=Length(620)
2024-03-28T02:26:33.357050233-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(456)), writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:26:33.357152012-05:00 - TRACE - decode; state=Length(456)
2024-03-28T02:26:33.357278261-05:00 - TRACE - received 456 bytes
2024-03-28T02:26:33.357307421-05:00 - DEBUG - incoming body completed
2024-03-28T02:26:33.357344561-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-28T02:26:33.357384940-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:26:33.357406960-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:26:33.357428990-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-28T02:26:33.357449190-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-28T02:26:33.357520989-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:26:33.357544579-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "background = \"0xff0000\"",
      "new": "background = \"0x000000\""
    }
  }
}
2024-03-28T02:26:33.357593649-05:00 - INFO - key change1
2024-03-28T02:26:33.357604879-05:00 - INFO - value "background = \"0xff0000\""
2024-03-28T02:26:33.357638088-05:00 - INFO - old value is "background = \"0xff0000\""
2024-03-28T02:26:33.357655278-05:00 - INFO - writing to file
2024-03-28T02:26:33.357759197-05:00 - INFO - New value: background = "0x000000"
2024-03-28T02:26:33.357789627-05:00 - INFO - 

2024-03-28T02:26:33.357812667-05:00 - INFO - is bool true true
2024-03-28T02:26:33.357839067-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"background = \\\"0xff0000\\\"\",\n      \"new\": \"background = \\\"0x000000\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711610792,"id":"chatcmpl-97eRcUuggW1ojGqB7sJk0U26BdfiY","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":44,"prompt_tokens":1054,"total_tokens":1098}}
2024-03-28T02:27:23.168263443-05:00 - INFO - booting up
2024-03-28T02:27:23.169372453-05:00 - INFO - alias was selected
2024-03-28T02:27:23.169421563-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-28T02:27:23.188431814-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-28T02:27:23.188487234-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-28T02:27:23.188520324-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-28T02:27:23.188620543-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-28T02:27:23.192849107-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-28T02:27:23.201554135-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-28T02:27:23.215056592-05:00 - TRACE - client handshake Http1
2024-03-28T02:27:23.215089892-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-28T02:27:23.215187511-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-28T02:27:23.215255191-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-28T02:27:23.215330400-05:00 - TRACE - encode_headers;
2024-03-28T02:27:23.215349080-05:00 - TRACE - -> encode_headers;
2024-03-28T02:27:23.215358040-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3828))
2024-03-28T02:27:23.215391670-05:00 - TRACE - <- encode_headers;
2024-03-28T02:27:23.215400399-05:00 - TRACE - -- encode_headers;
2024-03-28T02:27:23.215410749-05:00 - TRACE - sized write, len = 3828
2024-03-28T02:27:23.215419119-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3828
2024-03-28T02:27:23.215471959-05:00 - DEBUG - flushed 4030 bytes
2024-03-28T02:27:23.215485709-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:27:27.833857363-05:00 - TRACE - Conn::read_head
2024-03-28T02:27:27.834105311-05:00 - TRACE - received 1369 bytes
2024-03-28T02:27:27.834127281-05:00 - TRACE - parse_headers;
2024-03-28T02:27:27.834138441-05:00 - TRACE - -> parse_headers;
2024-03-28T02:27:27.834160851-05:00 - TRACE - Response.parse bytes=1369
2024-03-28T02:27:27.834211151-05:00 - TRACE - Response.parse Complete(1204)
2024-03-28T02:27:27.834300540-05:00 - TRACE - <- parse_headers;
2024-03-28T02:27:27.834313120-05:00 - TRACE - -- parse_headers;
2024-03-28T02:27:27.834323030-05:00 - DEBUG - parsed 24 headers
2024-03-28T02:27:27.834332650-05:00 - DEBUG - incoming body is content-length (1539 bytes)
2024-03-28T02:27:27.834368059-05:00 - TRACE - decode; state=Length(1539)
2024-03-28T02:27:27.834395049-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(1374)), writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:27:27.834486278-05:00 - TRACE - decode; state=Length(1374)
2024-03-28T02:27:27.834584218-05:00 - TRACE - received 1369 bytes
2024-03-28T02:27:27.834610887-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(5)), writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:27:27.834635217-05:00 - TRACE - decode; state=Length(5)
2024-03-28T02:27:27.834655967-05:00 - TRACE - received 5 bytes
2024-03-28T02:27:27.834666287-05:00 - DEBUG - incoming body completed
2024-03-28T02:27:27.834694997-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-28T02:27:27.834727716-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:27:27.834745766-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:27:27.834784726-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-28T02:27:27.834802146-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-28T02:27:27.834892485-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:27:27.834924935-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "#[font]\nsize = \"28\"\n",
      "new": "[font]\nfamily = \"New Font Family\"\nsize = \"28\"\n"
    },
    "change2": {
      "old": "#[font.bold]\n#family = \"Mononoki Nerd Font\"\n#style = \"Bold\"\n",
      "new": "[font.bold]\nfamily = \"New Font Family\"\nstyle = \"Bold\"\n"
    },
    "change3": {
      "old": "#[font.bold_italic]\n#family = \"Mononoki Nerd Font\"\n#style = \"Bold Italic\"\n",
      "new": "[font.bold_italic]\nfamily = \"New Font Family\"\nstyle = \"Bold Italic\"\n"
    },
    "change4": {
      "old": "#[font.italic]\n#family = \"Mononoki Nerd Font\"\n#style = \"Italic\"\n",
      "new": "[font.italic]\nfamily = \"New Font Family\"\nstyle = \"Italic\"\n"
    },
    "change5": {
      "old": "#[font.normal]\n#family = \"Mononoki Nerd Font\"\n#style = \"Regular\"\n",
      "new": "[font.normal]\nfamily = \"New Font Family
2024-03-28T02:27:27.835119163-05:00 - TRACE - client tx closed
2024-03-28T02:27:27.835130873-05:00 - TRACE - State::close_read()
2024-03-28T02:27:27.835137033-05:00 - TRACE - State::close_write()
2024-03-28T02:27:27.835147183-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-28T02:27:27.835188083-05:00 - TRACE - shut down IO complete
2024-03-28T02:27:37.580267158-05:00 - INFO - booting up
2024-03-28T02:27:37.581418328-05:00 - INFO - alias was selected
2024-03-28T02:27:37.581482438-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-28T02:27:37.599831384-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-28T02:27:37.599889614-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-28T02:27:37.599925013-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-28T02:27:37.600030942-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-28T02:27:37.614182315-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-28T02:27:37.624637898-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-28T02:27:37.640945982-05:00 - TRACE - client handshake Http1
2024-03-28T02:27:37.640977992-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-28T02:27:37.641069531-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-28T02:27:37.641139680-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-28T02:27:37.641213480-05:00 - TRACE - encode_headers;
2024-03-28T02:27:37.641228960-05:00 - TRACE - -> encode_headers;
2024-03-28T02:27:37.641237669-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3828))
2024-03-28T02:27:37.641261979-05:00 - TRACE - <- encode_headers;
2024-03-28T02:27:37.641270219-05:00 - TRACE - -- encode_headers;
2024-03-28T02:27:37.641280559-05:00 - TRACE - sized write, len = 3828
2024-03-28T02:27:37.641288979-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3828
2024-03-28T02:27:37.641336899-05:00 - DEBUG - flushed 4030 bytes
2024-03-28T02:27:37.641350169-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:27:38.995613198-05:00 - TRACE - Conn::read_head
2024-03-28T02:27:38.995805857-05:00 - TRACE - received 1369 bytes
2024-03-28T02:27:38.995824927-05:00 - TRACE - parse_headers;
2024-03-28T02:27:38.995835537-05:00 - TRACE - -> parse_headers;
2024-03-28T02:27:38.995860356-05:00 - TRACE - Response.parse bytes=1369
2024-03-28T02:27:38.995909336-05:00 - TRACE - Response.parse Complete(1203)
2024-03-28T02:27:38.995988245-05:00 - TRACE - <- parse_headers;
2024-03-28T02:27:38.996000255-05:00 - TRACE - -- parse_headers;
2024-03-28T02:27:38.996009945-05:00 - DEBUG - parsed 24 headers
2024-03-28T02:27:38.996019455-05:00 - DEBUG - incoming body is content-length (627 bytes)
2024-03-28T02:27:38.996048245-05:00 - TRACE - decode; state=Length(627)
2024-03-28T02:27:38.996070055-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(461)), writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:27:38.996149194-05:00 - TRACE - decode; state=Length(461)
2024-03-28T02:27:38.996261683-05:00 - TRACE - received 461 bytes
2024-03-28T02:27:38.996276643-05:00 - DEBUG - incoming body completed
2024-03-28T02:27:38.996305983-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-28T02:27:38.996342222-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:27:38.996360262-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:27:38.996378542-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-28T02:27:38.996394502-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-28T02:27:38.996451121-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:27:38.996466781-05:00 - INFO - text full json {
  "results": {
    "change1": {"old": "#[font]\nsize = \"28\"", "new": "[font]\nfamily = \"New Font Name\"\nsize = \"28\""}
  }
}
2024-03-28T02:27:38.996501481-05:00 - INFO - key change1
2024-03-28T02:27:38.996510381-05:00 - INFO - value "#[font]\nsize = \"28\""
2024-03-28T02:27:38.996537361-05:00 - INFO - old value is "#[font]\nsize = \"28\""
2024-03-28T02:27:38.996549901-05:00 - INFO - writing to file
2024-03-28T02:27:38.996653210-05:00 - INFO - New value: [font]
family = "New Font Name"
size = "28"
2024-03-28T02:27:38.996689069-05:00 - INFO - 

2024-03-28T02:27:38.996697829-05:00 - INFO - is bool true true
2024-03-28T02:27:38.996724659-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\"old\": \"#[font]\\nsize = \\\"28\\\"\", \"new\": \"[font]\\nfamily = \\\"New Font Name\\\"\\nsize = \\\"28\\\"\"}\n  }\n}","role":"assistant"}}],"created":1711610858,"id":"chatcmpl-97eSgH8XW4vcxD8KgFZ5D7riKaxV1","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":48,"prompt_tokens":1053,"total_tokens":1101}}
2024-03-28T02:27:38.996775649-05:00 - TRACE - client tx closed
2024-03-28T02:27:38.996795679-05:00 - TRACE - State::close_read()
2024-03-28T02:27:38.996809018-05:00 - TRACE - State::close_write()
2024-03-28T02:27:38.996824268-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-28T02:27:38.996878008-05:00 - TRACE - shut down IO complete
2024-03-28T02:27:46.780993379-05:00 - INFO - booting up
2024-03-28T02:27:46.782083290-05:00 - ERROR - Error: 'alias was not found in storage. Now Exiting!
2024-03-28T02:27:54.713630319-05:00 - INFO - booting up
2024-03-28T02:27:54.714824329-05:00 - INFO - user request this alias to be reverted alacritty.toml 
2024-03-28T02:27:54.714875949-05:00 - INFO - checking if alias exist, so it could be reverted
2024-03-28T02:27:54.714886639-05:00 - INFO - content alias is alacritty.toml 
2024-03-28T02:27:54.715002748-05:00 - INFO - sucessfully reverted the original file to its backup file
2024-03-28T02:29:09.195911990-05:00 - INFO - booting up
2024-03-28T02:29:09.197236639-05:00 - INFO - alias was selected
2024-03-28T02:29:09.197284929-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-28T02:29:09.216538146-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-28T02:29:09.216593136-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-28T02:29:09.216624926-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-28T02:29:09.216723845-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-28T02:29:14.234987714-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-28T02:29:14.241198963-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-28T02:29:14.256467124-05:00 - TRACE - client handshake Http1
2024-03-28T02:29:14.256502564-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-28T02:29:14.256630083-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-28T02:29:14.256706432-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-28T02:29:14.256831091-05:00 - TRACE - encode_headers;
2024-03-28T02:29:14.256851141-05:00 - TRACE - -> encode_headers;
2024-03-28T02:29:14.256860911-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3858))
2024-03-28T02:29:14.256889251-05:00 - TRACE - <- encode_headers;
2024-03-28T02:29:14.256897551-05:00 - TRACE - -- encode_headers;
2024-03-28T02:29:14.256907941-05:00 - TRACE - sized write, len = 3858
2024-03-28T02:29:14.256916410-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3858
2024-03-28T02:29:14.256968430-05:00 - DEBUG - flushed 4060 bytes
2024-03-28T02:29:14.256982220-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:29:15.459088206-05:00 - TRACE - Conn::read_head
2024-03-28T02:29:15.459287724-05:00 - TRACE - received 1369 bytes
2024-03-28T02:29:15.459307524-05:00 - TRACE - parse_headers;
2024-03-28T02:29:15.459322194-05:00 - TRACE - -> parse_headers;
2024-03-28T02:29:15.459344294-05:00 - TRACE - Response.parse bytes=1369
2024-03-28T02:29:15.459393503-05:00 - TRACE - Response.parse Complete(1202)
2024-03-28T02:29:15.459474002-05:00 - TRACE - <- parse_headers;
2024-03-28T02:29:15.459486102-05:00 - TRACE - -- parse_headers;
2024-03-28T02:29:15.459506752-05:00 - DEBUG - parsed 24 headers
2024-03-28T02:29:15.459516882-05:00 - DEBUG - incoming body is content-length (667 bytes)
2024-03-28T02:29:15.459546242-05:00 - TRACE - decode; state=Length(667)
2024-03-28T02:29:15.459568312-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(500)), writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:29:15.459664701-05:00 - TRACE - decode; state=Length(500)
2024-03-28T02:29:15.459800230-05:00 - TRACE - received 500 bytes
2024-03-28T02:29:15.459826479-05:00 - DEBUG - incoming body completed
2024-03-28T02:29:15.459859109-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-28T02:29:15.459897299-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:29:15.459918849-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:29:15.459940309-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-28T02:29:15.459959778-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-28T02:29:15.460028428-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:29:15.460053058-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "",
      "new": "[[keyboard.bindings]]\naction = \"IncreaseFontSize\"\nkey = \"ZoomIn\"\nmods = \"Control\""
    }
  }
}
2024-03-28T02:29:15.460102197-05:00 - INFO - key change1
2024-03-28T02:29:15.460113427-05:00 - INFO - value ""
2024-03-28T02:29:15.460138447-05:00 - INFO - old value is ""
2024-03-28T02:29:15.460145507-05:00 - INFO - writing to file
2024-03-28T02:29:15.460875951-05:00 - INFO - New value: [[keyboard.bindings]]
action = "IncreaseFontSize"
key = "ZoomIn"
mods = "Control"
2024-03-28T02:29:15.460962400-05:00 - INFO - 

2024-03-28T02:29:15.460970300-05:00 - INFO - is bool true true
2024-03-28T02:29:15.460992730-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"\",\n      \"new\": \"[[keyboard.bindings]]\\naction = \\\"IncreaseFontSize\\\"\\nkey = \\\"ZoomIn\\\"\\nmods = \\\"Control\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711610954,"id":"chatcmpl-97eUExJ2HQAU1lU40tEoSzxEdF212","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":54,"prompt_tokens":1064,"total_tokens":1118}}
2024-03-28T02:29:33.485768589-05:00 - INFO - booting up
2024-03-28T02:29:33.486985870-05:00 - INFO - alias was selected
2024-03-28T02:29:33.487200788-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-28T02:29:33.509324712-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-28T02:29:33.509381021-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-28T02:29:33.509410271-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-28T02:29:33.509510250-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-28T02:29:33.514507299-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-28T02:29:33.522429182-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-28T02:29:33.533630138-05:00 - TRACE - client handshake Http1
2024-03-28T02:29:33.533659668-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-28T02:29:33.533755777-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-28T02:29:33.533822406-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-28T02:29:33.533897716-05:00 - TRACE - encode_headers;
2024-03-28T02:29:33.533919006-05:00 - TRACE - -> encode_headers;
2024-03-28T02:29:33.533933385-05:00 - TRACE - Client::encode method=POST, body=Some(Known(242538))
2024-03-28T02:29:33.533962355-05:00 - TRACE - <- encode_headers;
2024-03-28T02:29:33.533971075-05:00 - TRACE - -- encode_headers;
2024-03-28T02:29:33.533990195-05:00 - TRACE - sized write, len = 242538
2024-03-28T02:29:33.533999105-05:00 - TRACE - buffer.flatten self.len=204 buf.len=242538
2024-03-28T02:29:33.534175483-05:00 - DEBUG - flushed 16384 bytes
2024-03-28T02:29:33.534198203-05:00 - DEBUG - flushed 16384 bytes
2024-03-28T02:29:33.542711721-05:00 - TRACE - Conn::read_head
2024-03-28T02:29:33.542849480-05:00 - DEBUG - flushed 32768 bytes
2024-03-28T02:29:33.542878890-05:00 - DEBUG - flushed 16384 bytes
2024-03-28T02:29:33.562867432-05:00 - TRACE - Conn::read_head
2024-03-28T02:29:33.562956372-05:00 - DEBUG - flushed 32768 bytes
2024-03-28T02:29:33.562987121-05:00 - DEBUG - flushed 16384 bytes
2024-03-28T02:29:33.563013231-05:00 - DEBUG - flushed 16384 bytes
2024-03-28T02:29:33.563034941-05:00 - DEBUG - flushed 16384 bytes
2024-03-28T02:29:33.569117330-05:00 - TRACE - Conn::read_head
2024-03-28T02:29:33.569186129-05:00 - DEBUG - flushed 32768 bytes
2024-03-28T02:29:33.569216779-05:00 - DEBUG - flushed 16384 bytes
2024-03-28T02:29:33.569239589-05:00 - DEBUG - flushed 16384 bytes
2024-03-28T02:29:33.569262888-05:00 - DEBUG - flushed 13366 bytes
2024-03-28T02:29:33.569274778-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:29:34.401270972-05:00 - TRACE - Conn::read_head
2024-03-28T02:29:34.401398861-05:00 - TRACE - received 1369 bytes
2024-03-28T02:29:34.401417471-05:00 - TRACE - parse_headers;
2024-03-28T02:29:34.401427461-05:00 - TRACE - -> parse_headers;
2024-03-28T02:29:34.401451670-05:00 - TRACE - Response.parse bytes=1369
2024-03-28T02:29:34.401498350-05:00 - TRACE - Response.parse Complete(1135)
2024-03-28T02:29:34.401571900-05:00 - TRACE - <- parse_headers;
2024-03-28T02:29:34.401591099-05:00 - TRACE - -- parse_headers;
2024-03-28T02:29:34.401600589-05:00 - DEBUG - parsed 22 headers
2024-03-28T02:29:34.401609699-05:00 - DEBUG - incoming body is content-length (283 bytes)
2024-03-28T02:29:34.401636969-05:00 - TRACE - decode; state=Length(283)
2024-03-28T02:29:34.401658449-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(49)), writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:29:34.401780358-05:00 - TRACE - decode; state=Length(49)
2024-03-28T02:29:34.401905847-05:00 - TRACE - received 49 bytes
2024-03-28T02:29:34.401923467-05:00 - DEBUG - incoming body completed
2024-03-28T02:29:34.401955586-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-28T02:29:34.401993106-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:29:34.402014016-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:29:34.402034476-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-28T02:29:34.402052776-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-28T02:29:34.402111795-05:00 - ERROR - Something went wrong with the request
2024-03-28T02:29:34.402137615-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:29:34.402161985-05:00 - ERROR - json {"error":{"code":"context_length_exceeded","message":"This model's maximum context length is 16385 tokens. However, your messages resulted in 59761 tokens. Please reduce the length of the messages.","param":"messages","type":"invalid_request_error"}}
2024-03-28T02:29:34.402187235-05:00 - TRACE - idle interval checking for expired
2024-03-28T02:29:34.402228704-05:00 - TRACE - pool closed, canceling idle interval
2024-03-28T02:29:34.402276524-05:00 - TRACE - client tx closed
2024-03-28T02:29:34.402291234-05:00 - TRACE - State::close_read()
2024-03-28T02:29:34.402304704-05:00 - TRACE - State::close_write()
2024-03-28T02:29:34.402322453-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-28T02:29:34.402386003-05:00 - TRACE - shut down IO complete
2024-03-28T02:29:40.634350396-05:00 - INFO - booting up
2024-03-28T02:29:40.635453697-05:00 - INFO - alias was selected
2024-03-28T02:29:40.635655215-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-28T02:29:40.658288554-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-28T02:29:40.658345314-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-28T02:29:40.658380653-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-28T02:29:40.658490752-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-28T02:29:40.662381540-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-28T02:29:40.669748418-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-28T02:29:40.683006407-05:00 - TRACE - client handshake Http1
2024-03-28T02:29:40.683035276-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-28T02:29:40.683153015-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-28T02:29:40.683221335-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-28T02:29:40.683289654-05:00 - TRACE - encode_headers;
2024-03-28T02:29:40.683305464-05:00 - TRACE - -> encode_headers;
2024-03-28T02:29:40.683313904-05:00 - TRACE - Client::encode method=POST, body=Some(Known(242538))
2024-03-28T02:29:40.683338634-05:00 - TRACE - <- encode_headers;
2024-03-28T02:29:40.683346624-05:00 - TRACE - -- encode_headers;
2024-03-28T02:29:40.683356794-05:00 - TRACE - sized write, len = 242538
2024-03-28T02:29:40.683365334-05:00 - TRACE - buffer.flatten self.len=204 buf.len=242538
2024-03-28T02:29:40.683574612-05:00 - DEBUG - flushed 16384 bytes
2024-03-28T02:29:40.683604292-05:00 - DEBUG - flushed 16384 bytes
2024-03-28T02:29:40.738757279-05:00 - TRACE - Conn::read_head
2024-03-28T02:29:40.738892448-05:00 - DEBUG - flushed 32768 bytes
2024-03-28T02:29:40.738984427-05:00 - DEBUG - flushed 16384 bytes
2024-03-28T02:29:40.739014187-05:00 - DEBUG - flushed 16384 bytes
2024-03-28T02:29:40.739038507-05:00 - DEBUG - flushed 16384 bytes
2024-03-28T02:29:40.739061077-05:00 - DEBUG - flushed 16384 bytes
2024-03-28T02:29:40.739084576-05:00 - DEBUG - flushed 16384 bytes
2024-03-28T02:29:40.766354977-05:00 - TRACE - Conn::read_head
2024-03-28T02:29:40.766420906-05:00 - DEBUG - flushed 32768 bytes
2024-03-28T02:29:40.766459346-05:00 - DEBUG - flushed 16384 bytes
2024-03-28T02:29:40.766489546-05:00 - DEBUG - flushed 16384 bytes
2024-03-28T02:29:40.766511756-05:00 - DEBUG - flushed 16384 bytes
2024-03-28T02:29:40.812815506-05:00 - TRACE - Conn::read_head
2024-03-28T02:29:40.812868036-05:00 - DEBUG - flushed 13366 bytes
2024-03-28T02:29:40.812884526-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:29:41.159713213-05:00 - TRACE - Conn::read_head
2024-03-28T02:29:41.159823342-05:00 - TRACE - received 1369 bytes
2024-03-28T02:29:41.159841562-05:00 - TRACE - parse_headers;
2024-03-28T02:29:41.159851762-05:00 - TRACE - -> parse_headers;
2024-03-28T02:29:41.159876422-05:00 - TRACE - Response.parse bytes=1369
2024-03-28T02:29:41.159927511-05:00 - TRACE - Response.parse Complete(1073)
2024-03-28T02:29:41.159995341-05:00 - TRACE - <- parse_headers;
2024-03-28T02:29:41.160007131-05:00 - TRACE - -- parse_headers;
2024-03-28T02:29:41.160016761-05:00 - DEBUG - parsed 21 headers
2024-03-28T02:29:41.160026050-05:00 - DEBUG - incoming body is content-length (381 bytes)
2024-03-28T02:29:41.160052750-05:00 - TRACE - decode; state=Length(381)
2024-03-28T02:29:41.160072680-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(85)), writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:29:41.160398887-05:00 - TRACE - decode; state=Length(85)
2024-03-28T02:29:41.160455147-05:00 - TRACE - received 85 bytes
2024-03-28T02:29:41.160480907-05:00 - DEBUG - incoming body completed
2024-03-28T02:29:41.160508496-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-28T02:29:41.160544616-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:29:41.160578066-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:29:41.160599736-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-28T02:29:41.160617485-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-28T02:29:41.160682485-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:29:41.160707125-05:00 - ERROR - Something went wrong with the request
2024-03-28T02:29:41.160728554-05:00 - TRACE - idle interval checking for expired
2024-03-28T02:29:41.160828964-05:00 - ERROR - json {"error":{"code":"rate_limit_exceeded","message":"Rate limit reached for gpt-3.5-turbo in organization org-uQNWw54MhbCCPVjWeADBEmMB on tokens per min (TPM): Limit 60000, Used 47200, Requested 54535. Please try again in 41.735s. Visit https://platform.openai.com/account/rate-limits to learn more.","param":null,"type":"tokens"}}
2024-03-28T02:29:41.160914133-05:00 - TRACE - pool closed, canceling idle interval
2024-03-28T02:29:41.161040262-05:00 - TRACE - client tx closed
2024-03-28T02:29:41.161078371-05:00 - TRACE - State::close_read()
2024-03-28T02:29:41.161097831-05:00 - TRACE - State::close_write()
2024-03-28T02:29:41.161111851-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-28T02:29:41.161155631-05:00 - TRACE - shut down IO complete
2024-03-28T02:29:55.029549152-05:00 - INFO - booting up
2024-03-28T02:29:55.030642253-05:00 - INFO - user request this alias to be reverted alacritty.toml 
2024-03-28T02:29:55.030685203-05:00 - INFO - checking if alias exist, so it could be reverted
2024-03-28T02:29:55.030693593-05:00 - INFO - content alias is alacritty.toml 
2024-03-28T02:29:55.030821572-05:00 - INFO - sucessfully reverted the original file to its backup file
2024-03-28T02:29:59.864442890-05:00 - INFO - booting up
2024-03-28T02:29:59.865546830-05:00 - INFO - alias was selected
2024-03-28T02:29:59.865594360-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-28T02:29:59.885344604-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-28T02:29:59.885399823-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-28T02:29:59.885431933-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-28T02:29:59.885533562-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-28T02:29:59.890734109-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-28T02:29:59.897075295-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-28T02:29:59.911615322-05:00 - TRACE - client handshake Http1
2024-03-28T02:29:59.911641202-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-28T02:29:59.911736841-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-28T02:29:59.911807481-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-28T02:29:59.911879830-05:00 - TRACE - encode_headers;
2024-03-28T02:29:59.911895540-05:00 - TRACE - -> encode_headers;
2024-03-28T02:29:59.911904240-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3858))
2024-03-28T02:29:59.911933130-05:00 - TRACE - <- encode_headers;
2024-03-28T02:29:59.911946610-05:00 - TRACE - -- encode_headers;
2024-03-28T02:29:59.911956839-05:00 - TRACE - sized write, len = 3858
2024-03-28T02:29:59.911965229-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3858
2024-03-28T02:29:59.912018709-05:00 - DEBUG - flushed 4060 bytes
2024-03-28T02:29:59.912032659-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:30:02.666029085-05:00 - TRACE - Conn::read_head
2024-03-28T02:30:02.666259863-05:00 - TRACE - received 1369 bytes
2024-03-28T02:30:02.666281613-05:00 - TRACE - parse_headers;
2024-03-28T02:30:02.666292363-05:00 - TRACE - -> parse_headers;
2024-03-28T02:30:02.666324263-05:00 - TRACE - Response.parse bytes=1369
2024-03-28T02:30:02.666377192-05:00 - TRACE - Response.parse Complete(1204)
2024-03-28T02:30:02.666453291-05:00 - TRACE - <- parse_headers;
2024-03-28T02:30:02.666470121-05:00 - TRACE - -- parse_headers;
2024-03-28T02:30:02.666480011-05:00 - DEBUG - parsed 24 headers
2024-03-28T02:30:02.666489361-05:00 - DEBUG - incoming body is content-length (762 bytes)
2024-03-28T02:30:02.666520101-05:00 - TRACE - decode; state=Length(762)
2024-03-28T02:30:02.666544361-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(597)), writing: KeepAlive, keep_alive: Busy }
2024-03-28T02:30:02.666604300-05:00 - TRACE - decode; state=Length(597)
2024-03-28T02:30:02.666653640-05:00 - TRACE - received 597 bytes
2024-03-28T02:30:02.666669790-05:00 - DEBUG - incoming body completed
2024-03-28T02:30:02.666702139-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-28T02:30:02.666739979-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:30:02.666761429-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:30:02.666782419-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-28T02:30:02.666802158-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-28T02:30:02.666879368-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-28T02:30:02.666908058-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "[[keyboard.bindings]]\naction = \"IncreaseFontSize\"\nkey = \"ZoomIn\"\nmods = \"Control\"",
      "new": "[[keyboard.bindings]]\naction = \"ZoomIn\"\nkey = \"PageUp\"\nmods = \"Control\""
    }
  }
}
2024-03-28T02:30:02.666953537-05:00 - INFO - key change1
2024-03-28T02:30:02.666964867-05:00 - INFO - value "[[keyboard.bindings]]\naction = \"IncreaseFontSize\"\nkey = \"ZoomIn\"\nmods = \"Control\""
2024-03-28T02:30:02.667007797-05:00 - INFO - old value is "[[keyboard.bindings]]\naction = \"IncreaseFontSize\"\nkey = \"ZoomIn\"\nmods = \"Control\""
2024-03-28T02:30:02.667024547-05:00 - INFO - writing to file
2024-03-28T02:30:02.667143346-05:00 - INFO - New value: [[keyboard.bindings]]
action = "ZoomIn"
key = "PageUp"
mods = "Control"
2024-03-28T02:30:02.667186535-05:00 - INFO - 

2024-03-28T02:30:02.667195035-05:00 - INFO - is bool true true
2024-03-28T02:30:02.667219785-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"[[keyboard.bindings]]\\naction = \\\"IncreaseFontSize\\\"\\nkey = \\\"ZoomIn\\\"\\nmods = \\\"Control\\\"\",\n      \"new\": \"[[keyboard.bindings]]\\naction = \\\"ZoomIn\\\"\\nkey = \\\"PageUp\\\"\\nmods = \\\"Control\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711611000,"id":"chatcmpl-97eUyN5IlJM3ZIPNFHW6bdXgNoCjL","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":80,"prompt_tokens":1064,"total_tokens":1144}}
2024-03-28T02:30:07.153360011-05:00 - INFO - booting up
2024-03-28T02:30:07.154478321-05:00 - INFO - user request this alias to be reverted alacritty.toml 
2024-03-28T02:30:07.154517781-05:00 - INFO - checking if alias exist, so it could be reverted
2024-03-28T02:30:07.154526181-05:00 - INFO - content alias is alacritty.toml 
2024-03-28T02:30:07.154611790-05:00 - INFO - sucessfully reverted the original file to its backup file
2024-03-29T15:18:28.529552092-05:00 - INFO - booting up
2024-03-29T15:18:28.533833592-05:00 - INFO - alias was selected
2024-03-29T15:18:28.533996490-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-29T15:18:28.557610238-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-29T15:18:28.557689827-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-29T15:18:28.557818266-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-29T15:18:28.557935185-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-29T15:18:33.574683129-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-29T15:18:33.580314616-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-29T15:18:33.594055137-05:00 - TRACE - client handshake Http1
2024-03-29T15:18:33.594218735-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-29T15:18:33.594431503-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-29T15:18:33.594514912-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-29T15:18:33.594605101-05:00 - TRACE - encode_headers;
2024-03-29T15:18:33.594621221-05:00 - TRACE - -> encode_headers;
2024-03-29T15:18:33.594636871-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3872))
2024-03-29T15:18:33.594675811-05:00 - TRACE - <- encode_headers;
2024-03-29T15:18:33.594684381-05:00 - TRACE - -- encode_headers;
2024-03-29T15:18:33.594695330-05:00 - TRACE - sized write, len = 3872
2024-03-29T15:18:33.594703900-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3872
2024-03-29T15:18:33.594762050-05:00 - DEBUG - flushed 4074 bytes
2024-03-29T15:18:33.594776560-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-29T15:18:35.093633212-05:00 - TRACE - Conn::read_head
2024-03-29T15:18:35.093821850-05:00 - TRACE - received 1369 bytes
2024-03-29T15:18:35.093840540-05:00 - TRACE - parse_headers;
2024-03-29T15:18:35.093850890-05:00 - TRACE - -> parse_headers;
2024-03-29T15:18:35.093874649-05:00 - TRACE - Response.parse bytes=1369
2024-03-29T15:18:35.093989608-05:00 - TRACE - Response.parse Complete(1202)
2024-03-29T15:18:35.094068388-05:00 - TRACE - <- parse_headers;
2024-03-29T15:18:35.094080747-05:00 - TRACE - -- parse_headers;
2024-03-29T15:18:35.094090507-05:00 - DEBUG - parsed 24 headers
2024-03-29T15:18:35.094099997-05:00 - DEBUG - incoming body is content-length (594 bytes)
2024-03-29T15:18:35.094126857-05:00 - TRACE - decode; state=Length(594)
2024-03-29T15:18:35.094148277-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(427)), writing: KeepAlive, keep_alive: Busy }
2024-03-29T15:18:35.094207276-05:00 - TRACE - decode; state=Length(427)
2024-03-29T15:18:35.094252276-05:00 - TRACE - received 427 bytes
2024-03-29T15:18:35.094275266-05:00 - DEBUG - incoming body completed
2024-03-29T15:18:35.094301555-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-29T15:18:35.094363745-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-29T15:18:35.094396335-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-29T15:18:35.094418344-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-29T15:18:35.094446294-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-29T15:18:35.094526633-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-29T15:18:35.094551283-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "\"0x1a1a1a\"",
      "new": "\"0x800080\""
    }
  }
}
2024-03-29T15:18:35.094588353-05:00 - INFO - key change1
2024-03-29T15:18:35.094598043-05:00 - INFO - value "\"0x1a1a1a\""
2024-03-29T15:18:35.094624652-05:00 - INFO - old value is "\"0x1a1a1a\""
2024-03-29T15:18:35.094637712-05:00 - INFO - writing to file
2024-03-29T15:18:35.094735471-05:00 - INFO - New value: "0x800080"
2024-03-29T15:18:35.094772761-05:00 - INFO - 

2024-03-29T15:18:35.094781221-05:00 - INFO - is bool true true
2024-03-29T15:18:35.094812951-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"\\\"0x1a1a1a\\\"\",\n      \"new\": \"\\\"0x800080\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711743514,"id":"chatcmpl-98CyInL3wMCXl87o3Bh3gjyMapiUd","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":42,"prompt_tokens":1066,"total_tokens":1108}}
2024-03-29T15:18:46.039094434-05:00 - INFO - booting up
2024-03-29T15:18:46.040235833-05:00 - INFO - alias was selected
2024-03-29T15:18:46.040283033-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-29T15:18:46.060431533-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-29T15:18:46.060487662-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-29T15:18:46.060521482-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-29T15:18:46.060635931-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-29T15:18:46.065756123-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-29T15:18:46.072429360-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-29T15:18:46.084899902-05:00 - TRACE - client handshake Http1
2024-03-29T15:18:46.084927792-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-29T15:18:46.085028041-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-29T15:18:46.085099161-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-29T15:18:46.085174410-05:00 - TRACE - encode_headers;
2024-03-29T15:18:46.085190460-05:00 - TRACE - -> encode_headers;
2024-03-29T15:18:46.085199190-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3876))
2024-03-29T15:18:46.085233009-05:00 - TRACE - <- encode_headers;
2024-03-29T15:18:46.085241709-05:00 - TRACE - -- encode_headers;
2024-03-29T15:18:46.085252169-05:00 - TRACE - sized write, len = 3876
2024-03-29T15:18:46.085260999-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3876
2024-03-29T15:18:46.085313649-05:00 - DEBUG - flushed 4078 bytes
2024-03-29T15:18:46.085328148-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-29T15:18:46.970038389-05:00 - TRACE - Conn::read_head
2024-03-29T15:18:46.970193257-05:00 - TRACE - received 1369 bytes
2024-03-29T15:18:46.970209477-05:00 - TRACE - parse_headers;
2024-03-29T15:18:46.970217897-05:00 - TRACE - -> parse_headers;
2024-03-29T15:18:46.970238577-05:00 - TRACE - Response.parse bytes=1369
2024-03-29T15:18:46.970278597-05:00 - TRACE - Response.parse Complete(1202)
2024-03-29T15:18:46.970334626-05:00 - TRACE - <- parse_headers;
2024-03-29T15:18:46.970358816-05:00 - TRACE - -- parse_headers;
2024-03-29T15:18:46.970367516-05:00 - DEBUG - parsed 24 headers
2024-03-29T15:18:46.970375286-05:00 - DEBUG - incoming body is content-length (610 bytes)
2024-03-29T15:18:46.970397875-05:00 - TRACE - decode; state=Length(610)
2024-03-29T15:18:46.970415895-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(443)), writing: KeepAlive, keep_alive: Busy }
2024-03-29T15:18:46.970503835-05:00 - TRACE - decode; state=Length(443)
2024-03-29T15:18:46.970633103-05:00 - TRACE - received 443 bytes
2024-03-29T15:18:46.970652063-05:00 - DEBUG - incoming body completed
2024-03-29T15:18:46.970684393-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-29T15:18:46.970723252-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-29T15:18:46.970745572-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-29T15:18:46.970767122-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-29T15:18:46.970781962-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-29T15:18:46.970836141-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-29T15:18:46.970855431-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "black = \"0x000000\"",
      "new": "black = \"0x000000\""
    }
  }
}
2024-03-29T15:18:46.970883341-05:00 - INFO - key change1
2024-03-29T15:18:46.970906441-05:00 - INFO - value "black = \"0x000000\""
2024-03-29T15:18:46.970927291-05:00 - INFO - old value is "black = \"0x000000\""
2024-03-29T15:18:46.970936550-05:00 - INFO - writing to file
2024-03-29T15:18:46.971016660-05:00 - INFO - New value: black = "0x000000"
2024-03-29T15:18:46.971039529-05:00 - INFO - 

2024-03-29T15:18:46.971045969-05:00 - INFO - is bool true true
2024-03-29T15:18:46.971069149-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"black = \\\"0x000000\\\"\",\n      \"new\": \"black = \\\"0x000000\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711743526,"id":"chatcmpl-98CyUIwc4M64onK0hYvogTatZN1BU","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":44,"prompt_tokens":1063,"total_tokens":1107}}
2024-03-29T15:18:46.971113509-05:00 - TRACE - client tx closed
2024-03-29T15:18:46.971132679-05:00 - TRACE - State::close_read()
2024-03-29T15:18:46.971143808-05:00 - TRACE - State::close_write()
2024-03-29T15:18:46.971155568-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-29T15:18:46.971425966-05:00 - TRACE - shut down IO complete
2024-03-29T15:18:57.470245934-05:00 - INFO - booting up
2024-03-29T15:18:57.471373274-05:00 - INFO - alias was selected
2024-03-29T15:18:57.471421703-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-29T15:18:57.490143817-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-29T15:18:57.490197456-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-29T15:18:57.490236106-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-29T15:18:57.490347455-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-29T15:18:57.494558055-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-29T15:18:57.501950137-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-29T15:18:57.520918359-05:00 - TRACE - client handshake Http1
2024-03-29T15:18:57.520949778-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-29T15:18:57.521043888-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-29T15:18:57.521114397-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-29T15:18:57.521189226-05:00 - TRACE - encode_headers;
2024-03-29T15:18:57.521204956-05:00 - TRACE - -> encode_headers;
2024-03-29T15:18:57.521213976-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3875))
2024-03-29T15:18:57.521238236-05:00 - TRACE - <- encode_headers;
2024-03-29T15:18:57.521251206-05:00 - TRACE - -- encode_headers;
2024-03-29T15:18:57.521261606-05:00 - TRACE - sized write, len = 3875
2024-03-29T15:18:57.521269945-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3875
2024-03-29T15:18:57.521321115-05:00 - DEBUG - flushed 4077 bytes
2024-03-29T15:18:57.521335205-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-29T15:18:59.362453779-05:00 - TRACE - Conn::read_head
2024-03-29T15:18:59.362605807-05:00 - TRACE - received 1369 bytes
2024-03-29T15:18:59.362621337-05:00 - TRACE - parse_headers;
2024-03-29T15:18:59.362629747-05:00 - TRACE - -> parse_headers;
2024-03-29T15:18:59.362648657-05:00 - TRACE - Response.parse bytes=1369
2024-03-29T15:18:59.362693347-05:00 - TRACE - Response.parse Complete(1203)
2024-03-29T15:18:59.362756186-05:00 - TRACE - <- parse_headers;
2024-03-29T15:18:59.362769646-05:00 - TRACE - -- parse_headers;
2024-03-29T15:18:59.362777636-05:00 - DEBUG - parsed 24 headers
2024-03-29T15:18:59.362785206-05:00 - DEBUG - incoming body is content-length (739 bytes)
2024-03-29T15:18:59.362807715-05:00 - TRACE - decode; state=Length(739)
2024-03-29T15:18:59.362826085-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(573)), writing: KeepAlive, keep_alive: Busy }
2024-03-29T15:18:59.362920414-05:00 - TRACE - decode; state=Length(573)
2024-03-29T15:18:59.363048983-05:00 - TRACE - received 573 bytes
2024-03-29T15:18:59.363066663-05:00 - DEBUG - incoming body completed
2024-03-29T15:18:59.363098323-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-29T15:18:59.363136982-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-29T15:18:59.363160002-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-29T15:18:59.363184092-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-29T15:18:59.363201832-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-29T15:18:59.363267921-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-29T15:18:59.363290981-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "black = \"0x000000\"",
      "new": "black = \"0x002b36\""
    },
    "change2": {
      "old": "background = \"0x800080\"",
      "new": "background = \"0x000000\""
    }
  }
}
2024-03-29T15:18:59.363336091-05:00 - INFO - key change1
2024-03-29T15:18:59.363352900-05:00 - INFO - value "black = \"0x000000\""
2024-03-29T15:18:59.363392340-05:00 - INFO - old value is "black = \"0x000000\""
2024-03-29T15:18:59.363407580-05:00 - INFO - writing to file
2024-03-29T15:18:59.363518119-05:00 - INFO - key change2
2024-03-29T15:18:59.363534759-05:00 - INFO - value "background = \"0x800080\""
2024-03-29T15:18:59.363564808-05:00 - INFO - old value is "background = \"0x800080\""
2024-03-29T15:18:59.363587948-05:00 - INFO - writing to file
2024-03-29T15:18:59.364138433-05:00 - INFO - New value: black = "0x002b36"
2024-03-29T15:18:59.364168553-05:00 - INFO - 

2024-03-29T15:18:59.364179243-05:00 - INFO - is bool true true
2024-03-29T15:18:59.364203672-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"black = \\\"0x000000\\\"\",\n      \"new\": \"black = \\\"0x002b36\\\"\"\n    },\n    \"change2\": {\n      \"old\": \"background = \\\"0x800080\\\"\",\n      \"new\": \"background = \\\"0x000000\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711743538,"id":"chatcmpl-98Cygo82Avc7nAtiYF39youqeIzPf","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":80,"prompt_tokens":1063,"total_tokens":1143}}
2024-03-29T15:19:04.332571717-05:00 - INFO - booting up
2024-03-29T15:19:04.333811066-05:00 - INFO - alias was selected
2024-03-29T15:19:04.333858165-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-29T15:19:04.352503320-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-29T15:19:04.352634018-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-29T15:19:04.352668048-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-29T15:19:04.352762227-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-29T15:19:04.358143228-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-29T15:19:04.365038573-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-29T15:19:04.379703815-05:00 - TRACE - client handshake Http1
2024-03-29T15:19:04.379739035-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-29T15:19:04.379840014-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-29T15:19:04.379927513-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-29T15:19:04.380022462-05:00 - TRACE - encode_headers;
2024-03-29T15:19:04.380038242-05:00 - TRACE - -> encode_headers;
2024-03-29T15:19:04.380047112-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3869))
2024-03-29T15:19:04.380075852-05:00 - TRACE - <- encode_headers;
2024-03-29T15:19:04.380093242-05:00 - TRACE - -- encode_headers;
2024-03-29T15:19:04.380109861-05:00 - TRACE - sized write, len = 3869
2024-03-29T15:19:04.380118341-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3869
2024-03-29T15:19:04.380170651-05:00 - DEBUG - flushed 4071 bytes
2024-03-29T15:19:04.380184691-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-29T15:19:05.300389197-05:00 - TRACE - Conn::read_head
2024-03-29T15:19:05.300597335-05:00 - TRACE - received 1369 bytes
2024-03-29T15:19:05.300618035-05:00 - TRACE - parse_headers;
2024-03-29T15:19:05.300628745-05:00 - TRACE - -> parse_headers;
2024-03-29T15:19:05.300653634-05:00 - TRACE - Response.parse bytes=1369
2024-03-29T15:19:05.300701684-05:00 - TRACE - Response.parse Complete(1204)
2024-03-29T15:19:05.300770033-05:00 - TRACE - <- parse_headers;
2024-03-29T15:19:05.300781193-05:00 - TRACE - -- parse_headers;
2024-03-29T15:19:05.300793183-05:00 - DEBUG - parsed 24 headers
2024-03-29T15:19:05.300802523-05:00 - DEBUG - incoming body is content-length (594 bytes)
2024-03-29T15:19:05.300829533-05:00 - TRACE - decode; state=Length(594)
2024-03-29T15:19:05.300850773-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(429)), writing: KeepAlive, keep_alive: Busy }
2024-03-29T15:19:05.300926402-05:00 - TRACE - decode; state=Length(429)
2024-03-29T15:19:05.301025571-05:00 - TRACE - received 429 bytes
2024-03-29T15:19:05.301040461-05:00 - DEBUG - incoming body completed
2024-03-29T15:19:05.301066841-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-29T15:19:05.301105550-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-29T15:19:05.301135630-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-29T15:19:05.301165530-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-29T15:19:05.301181240-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-29T15:19:05.301252769-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-29T15:19:05.301278979-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "\"0x000000\"",
      "new": "\"0xFF0000\""
    }
  }
}
2024-03-29T15:19:05.301305908-05:00 - INFO - key change1
2024-03-29T15:19:05.301313888-05:00 - INFO - value "\"0x000000\""
2024-03-29T15:19:05.301346638-05:00 - INFO - old value is "\"0x000000\""
2024-03-29T15:19:05.301359048-05:00 - INFO - writing to file
2024-03-29T15:19:05.301456597-05:00 - INFO - New value: "0xFF0000"
2024-03-29T15:19:05.301487537-05:00 - INFO - 

2024-03-29T15:19:05.301495847-05:00 - INFO - is bool true true
2024-03-29T15:19:05.301528856-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"\\\"0x000000\\\"\",\n      \"new\": \"\\\"0xFF0000\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711743544,"id":"chatcmpl-98CymZSNCKX8SZfBMafpUpdKOaHJ1","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":38,"prompt_tokens":1063,"total_tokens":1101}}
2024-03-29T15:19:05.301593726-05:00 - TRACE - client tx closed
2024-03-29T15:19:05.301617775-05:00 - TRACE - State::close_read()
2024-03-29T15:19:05.301632835-05:00 - TRACE - State::close_write()
2024-03-29T15:19:05.301648395-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-29T15:19:05.301698215-05:00 - TRACE - shut down IO complete
2024-03-29T15:19:10.205700016-05:00 - INFO - booting up
2024-03-29T15:19:10.206899255-05:00 - INFO - alias was selected
2024-03-29T15:19:10.206954674-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-29T15:19:10.224933885-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-29T15:19:10.224995914-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-29T15:19:10.225032344-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-29T15:19:10.225133443-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-29T15:19:10.229367703-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-29T15:19:10.237915323-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-29T15:19:10.250720952-05:00 - TRACE - client handshake Http1
2024-03-29T15:19:10.250755522-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-29T15:19:10.250847131-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-29T15:19:10.250912830-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-29T15:19:10.250989089-05:00 - TRACE - encode_headers;
2024-03-29T15:19:10.251004889-05:00 - TRACE - -> encode_headers;
2024-03-29T15:19:10.251013549-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3871))
2024-03-29T15:19:10.251038509-05:00 - TRACE - <- encode_headers;
2024-03-29T15:19:10.251046459-05:00 - TRACE - -- encode_headers;
2024-03-29T15:19:10.251056259-05:00 - TRACE - sized write, len = 3871
2024-03-29T15:19:10.251064209-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3871
2024-03-29T15:19:10.251116148-05:00 - DEBUG - flushed 4073 bytes
2024-03-29T15:19:10.251130328-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-29T15:19:11.649201931-05:00 - TRACE - Conn::read_head
2024-03-29T15:19:11.649395669-05:00 - TRACE - received 1369 bytes
2024-03-29T15:19:11.649980814-05:00 - TRACE - parse_headers;
2024-03-29T15:19:11.650004373-05:00 - TRACE - -> parse_headers;
2024-03-29T15:19:11.650029673-05:00 - TRACE - Response.parse bytes=1369
2024-03-29T15:19:11.650085833-05:00 - TRACE - Response.parse Complete(1204)
2024-03-29T15:19:11.650163502-05:00 - TRACE - <- parse_headers;
2024-03-29T15:19:11.650176102-05:00 - TRACE - -- parse_headers;
2024-03-29T15:19:11.650185852-05:00 - DEBUG - parsed 24 headers
2024-03-29T15:19:11.650195302-05:00 - DEBUG - incoming body is content-length (620 bytes)
2024-03-29T15:19:11.650223931-05:00 - TRACE - decode; state=Length(620)
2024-03-29T15:19:11.650245911-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(455)), writing: KeepAlive, keep_alive: Busy }
2024-03-29T15:19:11.650324570-05:00 - TRACE - decode; state=Length(455)
2024-03-29T15:19:11.650523278-05:00 - TRACE - received 455 bytes
2024-03-29T15:19:11.650541628-05:00 - DEBUG - incoming body completed
2024-03-29T15:19:11.650574478-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-29T15:19:11.650613328-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-29T15:19:11.650636417-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-29T15:19:11.650658687-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-29T15:19:11.650678337-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-29T15:19:11.650746446-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-29T15:19:11.650768036-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "background = \"0xFF0000\"",
      "new": "background = \"0x002b36\""
    }
  }
}
2024-03-29T15:19:11.650816356-05:00 - INFO - key change1
2024-03-29T15:19:11.650827996-05:00 - INFO - value "background = \"0xFF0000\""
2024-03-29T15:19:11.650861885-05:00 - INFO - old value is "background = \"0xFF0000\""
2024-03-29T15:19:11.650871715-05:00 - INFO - writing to file
2024-03-29T15:19:11.650948014-05:00 - INFO - New value: background = "0x002b36"
2024-03-29T15:19:11.650976854-05:00 - INFO - 

2024-03-29T15:19:11.650983864-05:00 - INFO - is bool true true
2024-03-29T15:19:11.651005484-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"background = \\\"0xFF0000\\\"\",\n      \"new\": \"background = \\\"0x002b36\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711743550,"id":"chatcmpl-98Cyst58FkqR6an0NsHdWG3W8uUHq","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":45,"prompt_tokens":1063,"total_tokens":1108}}
2024-03-29T15:19:32.634189549-05:00 - INFO - booting up
2024-03-29T15:19:32.635398127-05:00 - INFO - alias was selected
2024-03-29T15:19:32.635454177-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-29T15:19:32.654593677-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-29T15:19:32.654649456-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-29T15:19:32.654682876-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-29T15:19:32.654796195-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-29T15:19:32.671676566-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-29T15:19:32.677287553-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-29T15:19:32.691919797-05:00 - TRACE - client handshake Http1
2024-03-29T15:19:32.691947886-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-29T15:19:32.692043536-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-29T15:19:32.692111735-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-29T15:19:32.692185724-05:00 - TRACE - encode_headers;
2024-03-29T15:19:32.692201624-05:00 - TRACE - -> encode_headers;
2024-03-29T15:19:32.692210304-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3898))
2024-03-29T15:19:32.692240044-05:00 - TRACE - <- encode_headers;
2024-03-29T15:19:32.692248294-05:00 - TRACE - -- encode_headers;
2024-03-29T15:19:32.692258474-05:00 - TRACE - sized write, len = 3898
2024-03-29T15:19:32.692267033-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3898
2024-03-29T15:19:32.692319703-05:00 - DEBUG - flushed 4100 bytes
2024-03-29T15:19:32.692333773-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-29T15:19:34.382192683-05:00 - TRACE - Conn::read_head
2024-03-29T15:19:34.382460621-05:00 - TRACE - received 1369 bytes
2024-03-29T15:19:34.382483770-05:00 - TRACE - parse_headers;
2024-03-29T15:19:34.382494680-05:00 - TRACE - -> parse_headers;
2024-03-29T15:19:34.382516250-05:00 - TRACE - Response.parse bytes=1369
2024-03-29T15:19:34.382566310-05:00 - TRACE - Response.parse Complete(1202)
2024-03-29T15:19:34.382650429-05:00 - TRACE - <- parse_headers;
2024-03-29T15:19:34.382662359-05:00 - TRACE - -- parse_headers;
2024-03-29T15:19:34.382672159-05:00 - DEBUG - parsed 24 headers
2024-03-29T15:19:34.382681568-05:00 - DEBUG - incoming body is content-length (788 bytes)
2024-03-29T15:19:34.382717728-05:00 - TRACE - decode; state=Length(788)
2024-03-29T15:19:34.382744938-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(621)), writing: KeepAlive, keep_alive: Busy }
2024-03-29T15:19:34.382849997-05:00 - TRACE - decode; state=Length(621)
2024-03-29T15:19:34.382975696-05:00 - TRACE - received 621 bytes
2024-03-29T15:19:34.382993646-05:00 - DEBUG - incoming body completed
2024-03-29T15:19:34.383040995-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-29T15:19:34.383082175-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-29T15:19:34.383105264-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-29T15:19:34.383128414-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-29T15:19:34.383150114-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-29T15:19:34.383220553-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-29T15:19:34.383248723-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "[[keyboard.bindings]]\naction = \"IncreaseFontSize\"\nkey = \"ZoomIn\"\nmods = \"Control\"",
      "new": "[[keyboard.bindings]]\naction = \"ScrollPageUp\"\nkey = \"PageUp\"\nmode = \"~Alt\"\nmods = \"Shift\""
    }
  }
}
2024-03-29T15:19:34.383300323-05:00 - INFO - key change1
2024-03-29T15:19:34.383321032-05:00 - INFO - value "[[keyboard.bindings]]\naction = \"IncreaseFontSize\"\nkey = \"ZoomIn\"\nmods = \"Control\""
2024-03-29T15:19:34.383363732-05:00 - INFO - old value is "[[keyboard.bindings]]\naction = \"IncreaseFontSize\"\nkey = \"ZoomIn\"\nmods = \"Control\""
2024-03-29T15:19:34.383380522-05:00 - INFO - writing to file
2024-03-29T15:19:34.383488281-05:00 - INFO - New value: [[keyboard.bindings]]
action = "ScrollPageUp"
key = "PageUp"
mode = "~Alt"
mods = "Shift"
2024-03-29T15:19:34.383526311-05:00 - INFO - 

2024-03-29T15:19:34.383542890-05:00 - INFO - is bool true true
2024-03-29T15:19:34.383580450-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"[[keyboard.bindings]]\\naction = \\\"IncreaseFontSize\\\"\\nkey = \\\"ZoomIn\\\"\\nmods = \\\"Control\\\"\",\n      \"new\": \"[[keyboard.bindings]]\\naction = \\\"ScrollPageUp\\\"\\nkey = \\\"PageUp\\\"\\nmode = \\\"~Alt\\\"\\nmods = \\\"Shift\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711743573,"id":"chatcmpl-98CzFlv4vzqV67gWaU66QXCwtjceG","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":88,"prompt_tokens":1071,"total_tokens":1159}}
2024-03-29T15:20:19.111916904-05:00 - INFO - booting up
2024-03-29T15:20:19.113077353-05:00 - INFO - list of all aliases [ConfigFile { alias: "alacritty.toml", realpath: "/home/osamanoo/.config/alacritty/alacritty.toml", iteration: 1, backup_location: "backup_config/alacritty.toml", ts: "2024-03-25 04:06:09.037046463 UTC" }, ConfigFile { alias: "fish_config", realpath: "/home/osamanoo/.config/fish/config.fish", iteration: 1, backup_location: "backup_config/config.fish", ts: "2024-03-28 06:43:22.516949493 UTC" }, ConfigFile { alias: "fisher_fish", realpath: "/home/osamanoo/.config/fish/completions/fisher.fish", iteration: 1, backup_location: "backup_config/fisher.fish", ts: "2024-03-28 06:56:46.983009804 UTC" }]
2024-03-29T15:44:02.505663148-05:00 - INFO - booting up
2024-03-29T15:44:02.506888598-05:00 - INFO - user request this alias to be reverted alacritty.toml 
2024-03-29T15:44:02.506935087-05:00 - INFO - checking if alias exist, so it could be reverted
2024-03-29T15:44:02.506944627-05:00 - INFO - content alias is alacritty.toml 
2024-03-29T15:44:02.507040836-05:00 - INFO - sucessfully reverted the original file to its backup file
2024-03-30T23:43:02.466885232-05:00 - INFO - booting up
2024-03-30T23:43:18.378795205-05:00 - INFO - booting up
2024-03-30T23:43:18.379961124-05:00 - ERROR - no arg match
2024-03-30T23:43:30.345118544-05:00 - INFO - booting up
2024-03-30T23:43:30.346534312-05:00 - ERROR - no arg match
2024-03-30T23:45:14.938576008-05:00 - INFO - booting up
2024-03-30T23:45:58.652561575-05:00 - INFO - booting up
2024-03-30T23:47:24.536451276-05:00 - INFO - booting up
2024-03-30T23:49:44.579423150-05:00 - INFO - booting up
2024-03-30T23:50:34.467913121-05:00 - INFO - booting up
2024-03-31T14:57:00.140147612-05:00 - INFO - booting up
2024-03-31T14:57:00.141586398-05:00 - INFO - list of all aliases [ConfigFile { alias: "alacritty.toml", realpath: "/home/osamanoo/.config/alacritty/alacritty.toml", iteration: 1, backup_location: "backup_config/alacritty.toml", ts: "2024-03-25 04:06:09.037046463 UTC" }, ConfigFile { alias: "fish_config", realpath: "/home/osamanoo/.config/fish/config.fish", iteration: 1, backup_location: "backup_config/config.fish", ts: "2024-03-28 06:43:22.516949493 UTC" }, ConfigFile { alias: "fisher_fish", realpath: "/home/osamanoo/.config/fish/completions/fisher.fish", iteration: 1, backup_location: "backup_config/fisher.fish", ts: "2024-03-28 06:56:46.983009804 UTC" }]
2024-03-31T14:57:24.874812676-05:00 - INFO - booting up
2024-03-31T14:57:52.630572076-05:00 - INFO - booting up
2024-03-31T14:58:55.825361979-05:00 - INFO - booting up
2024-03-31T15:00:46.136175042-05:00 - INFO - booting up
2024-03-31T15:00:58.518369943-05:00 - INFO - booting up
2024-03-31T15:01:08.907198404-05:00 - INFO - booting up
2024-03-31T15:01:36.488097493-05:00 - INFO - booting up
2024-03-31T15:02:43.449798037-05:00 - INFO - booting up
2024-03-31T15:02:43.450902456-05:00 - INFO - going to get all possible content
2024-03-31T15:02:43.470710073-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T15:02:43.470771603-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T15:02:43.470804793-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T15:02:43.470905242-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T15:02:43.484929753-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-31T15:02:43.492273835-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-31T15:02:43.507658124-05:00 - TRACE - client handshake Http1
2024-03-31T15:02:43.507689923-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T15:02:43.507794802-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T15:02:43.507867392-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T15:02:43.507938401-05:00 - TRACE - encode_headers;
2024-03-31T15:02:43.507954341-05:00 - TRACE - -> encode_headers;
2024-03-31T15:02:43.507963141-05:00 - TRACE - Client::encode method=POST, body=Some(Known(323))
2024-03-31T15:02:43.507993171-05:00 - TRACE - <- encode_headers;
2024-03-31T15:02:43.508006640-05:00 - TRACE - -- encode_headers;
2024-03-31T15:02:43.508022120-05:00 - TRACE - sized write, len = 323
2024-03-31T15:02:43.508031360-05:00 - TRACE - buffer.flatten self.len=201 buf.len=323
2024-03-31T15:02:43.508072420-05:00 - DEBUG - flushed 524 bytes
2024-03-31T15:02:43.508086130-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T15:02:45.364549413-05:00 - TRACE - Conn::read_head
2024-03-31T15:02:45.364895350-05:00 - TRACE - received 1369 bytes
2024-03-31T15:02:45.364916209-05:00 - TRACE - parse_headers;
2024-03-31T15:02:45.364926979-05:00 - TRACE - -> parse_headers;
2024-03-31T15:02:45.364950209-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T15:02:45.365011839-05:00 - TRACE - Response.parse Complete(1202)
2024-03-31T15:02:45.365096188-05:00 - TRACE - <- parse_headers;
2024-03-31T15:02:45.365105648-05:00 - TRACE - -- parse_headers;
2024-03-31T15:02:45.365113148-05:00 - DEBUG - parsed 24 headers
2024-03-31T15:02:45.365120668-05:00 - DEBUG - incoming body is content-length (945 bytes)
2024-03-31T15:02:45.365152317-05:00 - TRACE - decode; state=Length(945)
2024-03-31T15:02:45.365176047-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(778)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T15:02:45.365282096-05:00 - TRACE - decode; state=Length(778)
2024-03-31T15:02:45.365388335-05:00 - TRACE - received 778 bytes
2024-03-31T15:02:45.365407625-05:00 - DEBUG - incoming body completed
2024-03-31T15:02:45.365437815-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T15:02:45.365470484-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T15:02:45.365503304-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T15:02:45.365525224-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T15:02:45.365553163-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T15:02:45.365651993-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T15:02:45.365668872-05:00 - INFO - json response {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"1. Changing the font size\n2. Changing the font family\n3. Adjusting the background color\n4. Modifying the foreground color\n5. Enabling or disabling mouse support\n6. Adjusting the opacity/transparency level\n7. Changing the cursor style\n8. Configuring keybindings\n9. Enabling or disabling anti-aliasing\n10. Adjusting the window dimensions\n11. Changing the scrollback buffer size\n12. Enabling or disabling vi mode\n13. Configuring keybindings for specific actions or commands","role":"assistant"}}],"created":1711915363,"id":"chatcmpl-98vg35OtzHsoIBa9s3QFYjsGkvcU8","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":111,"prompt_tokens":52,"total_tokens":163}}
2024-03-31T15:02:45.365783971-05:00 - TRACE - client tx closed
2024-03-31T15:02:45.365800921-05:00 - TRACE - State::close_read()
2024-03-31T15:02:45.365814241-05:00 - TRACE - State::close_write()
2024-03-31T15:02:45.365827831-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-31T15:02:45.365883400-05:00 - TRACE - shut down IO complete
2024-03-31T15:03:22.831622698-05:00 - INFO - booting up
2024-03-31T15:03:22.832726378-05:00 - INFO - list of all aliases [ConfigFile { alias: "alacritty.toml", realpath: "/home/osamanoo/.config/alacritty/alacritty.toml", iteration: 1, backup_location: "backup_config/alacritty.toml", ts: "2024-03-25 04:06:09.037046463 UTC" }, ConfigFile { alias: "fish_config", realpath: "/home/osamanoo/.config/fish/config.fish", iteration: 1, backup_location: "backup_config/config.fish", ts: "2024-03-28 06:43:22.516949493 UTC" }, ConfigFile { alias: "fisher_fish", realpath: "/home/osamanoo/.config/fish/completions/fisher.fish", iteration: 1, backup_location: "backup_config/fisher.fish", ts: "2024-03-28 06:56:46.983009804 UTC" }]
2024-03-31T15:03:31.606270746-05:00 - INFO - booting up
2024-03-31T15:03:31.607540045-05:00 - INFO - going to get all possible content
2024-03-31T15:03:42.817355571-05:00 - INFO - booting up
2024-03-31T15:03:42.818598759-05:00 - INFO - list of all aliases [ConfigFile { alias: "alacritty.toml", realpath: "/home/osamanoo/.config/alacritty/alacritty.toml", iteration: 1, backup_location: "backup_config/alacritty.toml", ts: "2024-03-25 04:06:09.037046463 UTC" }, ConfigFile { alias: "fish_config", realpath: "/home/osamanoo/.config/fish/config.fish", iteration: 1, backup_location: "backup_config/config.fish", ts: "2024-03-28 06:43:22.516949493 UTC" }, ConfigFile { alias: "fisher_fish", realpath: "/home/osamanoo/.config/fish/completions/fisher.fish", iteration: 1, backup_location: "backup_config/fisher.fish", ts: "2024-03-28 06:56:46.983009804 UTC" }]
2024-03-31T15:03:48.529096763-05:00 - INFO - booting up
2024-03-31T15:03:48.530247382-05:00 - INFO - going to get all possible content
2024-03-31T15:03:48.550446077-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T15:03:48.550502396-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T15:03:48.550533036-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T15:03:48.550627835-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T15:03:48.581037368-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-31T15:03:48.585599925-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-31T15:03:48.600220502-05:00 - TRACE - client handshake Http1
2024-03-31T15:03:48.600252641-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T15:03:48.600350151-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T15:03:48.600416090-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T15:03:48.600529339-05:00 - TRACE - encode_headers;
2024-03-31T15:03:48.600655208-05:00 - TRACE - -> encode_headers;
2024-03-31T15:03:48.600666288-05:00 - TRACE - Client::encode method=POST, body=Some(Known(327))
2024-03-31T15:03:48.600697887-05:00 - TRACE - <- encode_headers;
2024-03-31T15:03:48.600707057-05:00 - TRACE - -- encode_headers;
2024-03-31T15:03:48.600717597-05:00 - TRACE - sized write, len = 327
2024-03-31T15:03:48.600726497-05:00 - TRACE - buffer.flatten self.len=201 buf.len=327
2024-03-31T15:03:48.600770217-05:00 - DEBUG - flushed 528 bytes
2024-03-31T15:03:48.600784187-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T15:03:49.772225645-05:00 - TRACE - Conn::read_head
2024-03-31T15:03:49.772478813-05:00 - TRACE - received 1369 bytes
2024-03-31T15:03:49.772498943-05:00 - TRACE - parse_headers;
2024-03-31T15:03:49.772509583-05:00 - TRACE - -> parse_headers;
2024-03-31T15:03:49.772532563-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T15:03:49.772585402-05:00 - TRACE - Response.parse Complete(1201)
2024-03-31T15:03:49.772676711-05:00 - TRACE - <- parse_headers;
2024-03-31T15:03:49.772688501-05:00 - TRACE - -- parse_headers;
2024-03-31T15:03:49.772698401-05:00 - DEBUG - parsed 24 headers
2024-03-31T15:03:49.772708041-05:00 - DEBUG - incoming body is content-length (787 bytes)
2024-03-31T15:03:49.772745461-05:00 - TRACE - decode; state=Length(787)
2024-03-31T15:03:49.772774280-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(619)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T15:03:49.772874560-05:00 - TRACE - decode; state=Length(619)
2024-03-31T15:03:49.772945159-05:00 - TRACE - received 619 bytes
2024-03-31T15:03:49.772960679-05:00 - DEBUG - incoming body completed
2024-03-31T15:03:49.772988679-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T15:03:49.773023808-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T15:03:49.773041388-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T15:03:49.773061068-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T15:03:49.773079718-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T15:03:49.773152207-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T15:03:49.773178077-05:00 - INFO - json response {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"- Add a new completion for a specific fisher command\n- Edit an existing completion for a fisher command\n- Remove a completion for a fisher command\n- Configure suggestions or options for fisher commands in completions\n- Disable completions for certain fisher commands\n- Adjust the order of completions for fisher commands","role":"assistant"}}],"created":1711915428,"id":"chatcmpl-98vh6M72ZsQDPv9mkxpLnoj5rUmhm","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":67,"prompt_tokens":51,"total_tokens":118}}
2024-03-31T15:03:49.773288376-05:00 - TRACE - client tx closed
2024-03-31T15:03:49.773306426-05:00 - TRACE - State::close_read()
2024-03-31T15:03:49.773319416-05:00 - TRACE - State::close_write()
2024-03-31T15:03:49.773333005-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-31T15:03:49.773381245-05:00 - TRACE - shut down IO complete
2024-03-31T17:33:19.747644975-05:00 - INFO - booting up
2024-03-31T17:33:19.748975143-05:00 - INFO - going to get all possible content
2024-03-31T17:33:19.769610727-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T17:33:19.769669487-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T17:33:19.769704127-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T17:33:19.769809026-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T17:33:19.783433705-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-31T17:33:19.789212163-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-31T17:33:19.804083230-05:00 - TRACE - client handshake Http1
2024-03-31T17:33:19.804132669-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T17:33:19.804236328-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T17:33:19.804312248-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T17:33:19.804609535-05:00 - TRACE - encode_headers;
2024-03-31T17:33:19.804678294-05:00 - TRACE - -> encode_headers;
2024-03-31T17:33:19.804687784-05:00 - TRACE - Client::encode method=POST, body=Some(Known(323))
2024-03-31T17:33:19.804714364-05:00 - TRACE - <- encode_headers;
2024-03-31T17:33:19.804722574-05:00 - TRACE - -- encode_headers;
2024-03-31T17:33:19.804732934-05:00 - TRACE - sized write, len = 323
2024-03-31T17:33:19.804741004-05:00 - TRACE - buffer.flatten self.len=201 buf.len=323
2024-03-31T17:33:19.804782374-05:00 - DEBUG - flushed 524 bytes
2024-03-31T17:33:19.804795903-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:33:22.641586811-05:00 - TRACE - Conn::read_head
2024-03-31T17:33:22.641764749-05:00 - TRACE - received 1369 bytes
2024-03-31T17:33:22.641782829-05:00 - TRACE - parse_headers;
2024-03-31T17:33:22.641791199-05:00 - TRACE - -> parse_headers;
2024-03-31T17:33:22.641808699-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T17:33:22.641848778-05:00 - TRACE - Response.parse Complete(1202)
2024-03-31T17:33:22.641915928-05:00 - TRACE - <- parse_headers;
2024-03-31T17:33:22.641925708-05:00 - TRACE - -- parse_headers;
2024-03-31T17:33:22.641933617-05:00 - DEBUG - parsed 24 headers
2024-03-31T17:33:22.641941637-05:00 - DEBUG - incoming body is content-length (802 bytes)
2024-03-31T17:33:22.641966997-05:00 - TRACE - decode; state=Length(802)
2024-03-31T17:33:22.641986437-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(635)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:33:22.642164875-05:00 - TRACE - decode; state=Length(635)
2024-03-31T17:33:22.642233315-05:00 - TRACE - received 635 bytes
2024-03-31T17:33:22.642250905-05:00 - DEBUG - incoming body completed
2024-03-31T17:33:22.642281364-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T17:33:22.642319154-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:33:22.642340624-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:33:22.642362414-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T17:33:22.642381553-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T17:33:22.642450683-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:33:22.642474383-05:00 - INFO - json response {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"1. Font settings\n2. Colors settings\n3. Window transparency settings\n4. Cursor settings\n5. Key bindings\n6. Scroll settings\n7. Window size and position settings\n8. Bell settings\n9. Shell startup command\n10. Mouse settings\n11. Clipboard settings\n12. Rendering settings\n13. Vi mode settings\n14. Selection settings\n15. Tab settings","role":"assistant"}}],"created":1711924400,"id":"chatcmpl-98y1ovD4JEmStwOSBqfbBk5Y0hwoY","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":80,"prompt_tokens":52,"total_tokens":132}}
2024-03-31T17:33:22.642568532-05:00 - TRACE - client tx closed
2024-03-31T17:33:22.642581942-05:00 - TRACE - State::close_read()
2024-03-31T17:33:22.642592912-05:00 - TRACE - State::close_write()
2024-03-31T17:33:22.642603671-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-31T17:33:22.642639591-05:00 - TRACE - shut down IO complete
2024-03-31T17:34:04.815112602-05:00 - INFO - booting up
2024-03-31T17:34:04.816194362-05:00 - INFO - going to get all possible content
2024-03-31T17:34:04.834403609-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T17:34:04.834459918-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T17:34:04.834490648-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T17:34:04.834594207-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T17:34:04.839347516-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-31T17:34:04.845367372-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-31T17:34:04.858377545-05:00 - TRACE - client handshake Http1
2024-03-31T17:34:04.858409535-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T17:34:04.858514534-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T17:34:04.858582543-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T17:34:04.858652623-05:00 - TRACE - encode_headers;
2024-03-31T17:34:04.858668142-05:00 - TRACE - -> encode_headers;
2024-03-31T17:34:04.858676592-05:00 - TRACE - Client::encode method=POST, body=Some(Known(331))
2024-03-31T17:34:04.858701612-05:00 - TRACE - <- encode_headers;
2024-03-31T17:34:04.858709862-05:00 - TRACE - -- encode_headers;
2024-03-31T17:34:04.858719882-05:00 - TRACE - sized write, len = 331
2024-03-31T17:34:04.858728202-05:00 - TRACE - buffer.flatten self.len=201 buf.len=331
2024-03-31T17:34:04.858767862-05:00 - DEBUG - flushed 532 bytes
2024-03-31T17:34:04.858781281-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:34:07.612909498-05:00 - TRACE - Conn::read_head
2024-03-31T17:34:07.613123026-05:00 - TRACE - received 1369 bytes
2024-03-31T17:34:07.613142766-05:00 - TRACE - parse_headers;
2024-03-31T17:34:07.613153236-05:00 - TRACE - -> parse_headers;
2024-03-31T17:34:07.613175365-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T17:34:07.613238705-05:00 - TRACE - Response.parse Complete(1203)
2024-03-31T17:34:07.613323724-05:00 - TRACE - <- parse_headers;
2024-03-31T17:34:07.613335964-05:00 - TRACE - -- parse_headers;
2024-03-31T17:34:07.613345884-05:00 - DEBUG - parsed 24 headers
2024-03-31T17:34:07.613355874-05:00 - DEBUG - incoming body is content-length (1083 bytes)
2024-03-31T17:34:07.613387973-05:00 - TRACE - decode; state=Length(1083)
2024-03-31T17:34:07.613413203-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(917)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:34:07.613523212-05:00 - TRACE - decode; state=Length(917)
2024-03-31T17:34:07.613633841-05:00 - TRACE - received 917 bytes
2024-03-31T17:34:07.613649551-05:00 - DEBUG - incoming body completed
2024-03-31T17:34:07.613692791-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T17:34:07.613725980-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:34:07.613744080-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:34:07.613762710-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T17:34:07.613779450-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T17:34:07.613837649-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:34:07.613857259-05:00 - INFO - json response {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"1. font: changing the font used by Alacritty\n2. colors: customizing the color scheme of the terminal\n3. dimensions: adjusting the dimensions of the terminal window\n4. keybindings: defining custom keybindings for various actions\n5. cursor: customizing the appearance of the cursor in the terminal\n6. padding: adjusting the padding around the terminal content\n7. background_opacity: setting the opacity of the terminal background\n8. scrollback: configuring the amount of scrollback history\n9. mouse: enabling or disabling mouse support in the terminal\n10. shell: specifying the default shell to use with Alacritty","role":"assistant"}}],"created":1711924445,"id":"chatcmpl-98y2X6SbXiBZCo7jEZMF03iIbF9p7","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":133,"prompt_tokens":54,"total_tokens":187}}
2024-03-31T17:34:07.613975358-05:00 - TRACE - client tx closed
2024-03-31T17:34:07.613996378-05:00 - TRACE - State::close_read()
2024-03-31T17:34:07.614010688-05:00 - TRACE - State::close_write()
2024-03-31T17:34:07.614024648-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-31T17:34:07.614075897-05:00 - TRACE - shut down IO complete
2024-03-31T17:35:26.419450061-05:00 - INFO - booting up
2024-03-31T17:35:26.420583731-05:00 - INFO - alias was selected
2024-03-31T17:35:26.421399823-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-31T17:35:26.439555780-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T17:35:26.439610080-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T17:35:26.439645129-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T17:35:26.439744569-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T17:35:26.456821565-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-31T17:35:26.465706145-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-31T17:35:26.483030940-05:00 - TRACE - client handshake Http1
2024-03-31T17:35:26.483060020-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T17:35:26.483155799-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T17:35:26.483228558-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T17:35:26.483305557-05:00 - TRACE - encode_headers;
2024-03-31T17:35:26.483321227-05:00 - TRACE - -> encode_headers;
2024-03-31T17:35:26.483329807-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3881))
2024-03-31T17:35:26.483354547-05:00 - TRACE - <- encode_headers;
2024-03-31T17:35:26.483362627-05:00 - TRACE - -- encode_headers;
2024-03-31T17:35:26.483376557-05:00 - TRACE - sized write, len = 3881
2024-03-31T17:35:26.483384907-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3881
2024-03-31T17:35:26.483437146-05:00 - DEBUG - flushed 4083 bytes
2024-03-31T17:35:26.483451156-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:35:28.609989846-05:00 - TRACE - Conn::read_head
2024-03-31T17:35:28.610190064-05:00 - TRACE - received 1369 bytes
2024-03-31T17:35:28.610208844-05:00 - TRACE - parse_headers;
2024-03-31T17:35:28.610219094-05:00 - TRACE - -> parse_headers;
2024-03-31T17:35:28.610240124-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T17:35:28.610298353-05:00 - TRACE - Response.parse Complete(1203)
2024-03-31T17:35:28.610373173-05:00 - TRACE - <- parse_headers;
2024-03-31T17:35:28.610384802-05:00 - TRACE - -- parse_headers;
2024-03-31T17:35:28.610394602-05:00 - DEBUG - parsed 24 headers
2024-03-31T17:35:28.610403922-05:00 - DEBUG - incoming body is content-length (866 bytes)
2024-03-31T17:35:28.610430972-05:00 - TRACE - decode; state=Length(866)
2024-03-31T17:35:28.610452362-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(700)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:35:28.610544201-05:00 - TRACE - decode; state=Length(700)
2024-03-31T17:35:28.610673400-05:00 - TRACE - received 700 bytes
2024-03-31T17:35:28.610701070-05:00 - DEBUG - incoming body completed
2024-03-31T17:35:28.610736229-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T17:35:28.610774619-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:35:28.610796669-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:35:28.610830878-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T17:35:28.610857738-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T17:35:28.610958487-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:35:28.610986517-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "background = \"0x1a1a1a\"\nforeground = \"0x8caad2\"",
      "new": "background = \"0x1a1a1a22\"\nforeground = \"0x8caad222\""
    },
    "change2": {
      "old": "cursor = \"0x93a1a1\"\ntext = \"0x002b36\"",
      "new": "cursor = \"0x93a1a166\"\ntext = \"0x002b3622\""
    }
  }
}
2024-03-31T17:35:28.611038457-05:00 - INFO - key change1
2024-03-31T17:35:28.611049387-05:00 - INFO - value "background = \"0x1a1a1a\"\nforeground = \"0x8caad2\""
2024-03-31T17:35:28.611088536-05:00 - INFO - old value is "background = \"0x1a1a1a\"\nforeground = \"0x8caad2\""
2024-03-31T17:35:28.611108756-05:00 - INFO - writing to file
2024-03-31T17:35:28.612033428-05:00 - TRACE - idle interval checking for expired
2024-03-31T17:35:28.612113067-05:00 - INFO - key change2
2024-03-31T17:35:28.612128327-05:00 - INFO - value "cursor = \"0x93a1a1\"\ntext = \"0x002b36\""
2024-03-31T17:35:28.612150967-05:00 - INFO - old value is "cursor = \"0x93a1a1\"\ntext = \"0x002b36\""
2024-03-31T17:35:28.612159657-05:00 - INFO - writing to file
2024-03-31T17:35:28.612220656-05:00 - INFO - New value: background = "0x1a1a1a22"
foreground = "0x8caad222"
2024-03-31T17:35:28.612246206-05:00 - INFO - 

2024-03-31T17:35:28.612252546-05:00 - INFO - is bool true true
2024-03-31T17:35:28.612277636-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"background = \\\"0x1a1a1a\\\"\\nforeground = \\\"0x8caad2\\\"\",\n      \"new\": \"background = \\\"0x1a1a1a22\\\"\\nforeground = \\\"0x8caad222\\\"\"\n    },\n    \"change2\": {\n      \"old\": \"cursor = \\\"0x93a1a1\\\"\\ntext = \\\"0x002b36\\\"\",\n      \"new\": \"cursor = \\\"0x93a1a166\\\"\\ntext = \\\"0x002b3622\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711924526,"id":"chatcmpl-98y3q7PT9wjrqKUP0muoRPHiOxVeu","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":137,"prompt_tokens":1067,"total_tokens":1204}}
2024-03-31T17:35:50.711917361-05:00 - INFO - booting up
2024-03-31T17:35:50.713019141-05:00 - INFO - user request this alias to be reverted alacritty.toml 
2024-03-31T17:35:50.713062351-05:00 - INFO - checking if alias exist, so it could be reverted
2024-03-31T17:35:50.713070751-05:00 - INFO - content alias is alacritty.toml 
2024-03-31T17:35:50.713165180-05:00 - INFO - sucessfully reverted the original file to its backup file
2024-03-31T17:35:58.104633240-05:00 - INFO - booting up
2024-03-31T17:35:58.105972408-05:00 - INFO - alias was selected
2024-03-31T17:35:58.106024997-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-31T17:35:58.124488672-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T17:35:58.124543141-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T17:35:58.124578471-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T17:35:58.124684740-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T17:35:58.129991472-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-31T17:35:58.143105084-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-31T17:35:58.160691207-05:00 - TRACE - client handshake Http1
2024-03-31T17:35:58.160716816-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T17:35:58.160995544-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T17:35:58.161068873-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T17:35:58.161147902-05:00 - TRACE - encode_headers;
2024-03-31T17:35:58.161163532-05:00 - TRACE - -> encode_headers;
2024-03-31T17:35:58.161172002-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3881))
2024-03-31T17:35:58.161196952-05:00 - TRACE - <- encode_headers;
2024-03-31T17:35:58.161207252-05:00 - TRACE - -- encode_headers;
2024-03-31T17:35:58.161219902-05:00 - TRACE - sized write, len = 3881
2024-03-31T17:35:58.161228532-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3881
2024-03-31T17:35:58.161281541-05:00 - DEBUG - flushed 4083 bytes
2024-03-31T17:35:58.161295551-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:35:59.432540440-05:00 - TRACE - Conn::read_head
2024-03-31T17:35:59.432732489-05:00 - TRACE - received 1369 bytes
2024-03-31T17:35:59.432751468-05:00 - TRACE - parse_headers;
2024-03-31T17:35:59.432761888-05:00 - TRACE - -> parse_headers;
2024-03-31T17:35:59.432785728-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T17:35:59.432839268-05:00 - TRACE - Response.parse Complete(1202)
2024-03-31T17:35:59.432914127-05:00 - TRACE - <- parse_headers;
2024-03-31T17:35:59.432925957-05:00 - TRACE - -- parse_headers;
2024-03-31T17:35:59.432935637-05:00 - DEBUG - parsed 24 headers
2024-03-31T17:35:59.432944957-05:00 - DEBUG - incoming body is content-length (626 bytes)
2024-03-31T17:35:59.432972416-05:00 - TRACE - decode; state=Length(626)
2024-03-31T17:35:59.432993846-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(459)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:35:59.433087235-05:00 - TRACE - decode; state=Length(459)
2024-03-31T17:35:59.433225764-05:00 - TRACE - received 459 bytes
2024-03-31T17:35:59.433243124-05:00 - DEBUG - incoming body completed
2024-03-31T17:35:59.433274394-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T17:35:59.433312333-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:35:59.433333973-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:35:59.433355153-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T17:35:59.433373633-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T17:35:59.433493372-05:00 - INFO - text full json ```json
{
  "results": {
    "change1": {
      "old": "black = \"0x000000\"",
      "new": "black = \"0x00000000\""
    }
  }
}
```
2024-03-31T17:35:59.433525672-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:40:35.361521855-05:00 - INFO - booting up
2024-03-31T17:40:35.362665204-05:00 - INFO - alias was selected
2024-03-31T17:40:35.362716583-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-31T17:40:35.380998148-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T17:40:35.381052778-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T17:40:35.381085557-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T17:40:35.381191226-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T17:40:35.398204814-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-31T17:40:35.406551667-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-31T17:40:35.424574725-05:00 - TRACE - client handshake Http1
2024-03-31T17:40:35.424616985-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T17:40:35.424755973-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T17:40:35.424827923-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T17:40:35.424977431-05:00 - TRACE - encode_headers;
2024-03-31T17:40:35.425050371-05:00 - TRACE - -> encode_headers;
2024-03-31T17:40:35.425060061-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3886))
2024-03-31T17:40:35.425091360-05:00 - TRACE - <- encode_headers;
2024-03-31T17:40:35.425100370-05:00 - TRACE - -- encode_headers;
2024-03-31T17:40:35.425111160-05:00 - TRACE - sized write, len = 3886
2024-03-31T17:40:35.425119570-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3886
2024-03-31T17:40:35.425173350-05:00 - DEBUG - flushed 4088 bytes
2024-03-31T17:40:35.425187370-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:40:36.324773766-05:00 - TRACE - Conn::read_head
2024-03-31T17:40:36.324979324-05:00 - TRACE - received 1369 bytes
2024-03-31T17:40:36.324999874-05:00 - TRACE - parse_headers;
2024-03-31T17:40:36.325010513-05:00 - TRACE - -> parse_headers;
2024-03-31T17:40:36.325034553-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T17:40:36.325092973-05:00 - TRACE - Response.parse Complete(1202)
2024-03-31T17:40:36.325166492-05:00 - TRACE - <- parse_headers;
2024-03-31T17:40:36.325178782-05:00 - TRACE - -- parse_headers;
2024-03-31T17:40:36.325188522-05:00 - DEBUG - parsed 24 headers
2024-03-31T17:40:36.325198132-05:00 - DEBUG - incoming body is content-length (656 bytes)
2024-03-31T17:40:36.325225062-05:00 - TRACE - decode; state=Length(656)
2024-03-31T17:40:36.325246211-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(489)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:40:36.325374150-05:00 - TRACE - decode; state=Length(489)
2024-03-31T17:40:36.325446710-05:00 - TRACE - received 489 bytes
2024-03-31T17:40:36.325464489-05:00 - DEBUG - incoming body completed
2024-03-31T17:40:36.325524419-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T17:40:36.325574558-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:40:36.325596478-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:40:36.325618208-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T17:40:36.325636208-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T17:40:36.325702377-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:40:36.325725897-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "background = \"0x1a1a1a\"",
      "new": "background = \"0x1a1a1a00\"" //close to transparent background
    }
  }
}
2024-03-31T17:40:36.325904905-05:00 - TRACE - client tx closed
2024-03-31T17:40:36.325933135-05:00 - TRACE - State::close_read()
2024-03-31T17:40:36.325951885-05:00 - TRACE - State::close_write()
2024-03-31T17:40:36.325969945-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-31T17:40:36.326024174-05:00 - TRACE - shut down IO complete
2024-03-31T17:40:46.363139129-05:00 - INFO - booting up
2024-03-31T17:40:46.364284379-05:00 - INFO - alias was selected
2024-03-31T17:40:46.364337479-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-31T17:40:46.382556775-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T17:40:46.382613054-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T17:40:46.382648134-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T17:40:46.382765653-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T17:40:46.387158272-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-31T17:40:46.393519945-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-31T17:40:46.407317351-05:00 - TRACE - client handshake Http1
2024-03-31T17:40:46.407342611-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T17:40:46.407618278-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T17:40:46.407687088-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T17:40:46.407765337-05:00 - TRACE - encode_headers;
2024-03-31T17:40:46.407781127-05:00 - TRACE - -> encode_headers;
2024-03-31T17:40:46.407789707-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3886))
2024-03-31T17:40:46.407814417-05:00 - TRACE - <- encode_headers;
2024-03-31T17:40:46.407822367-05:00 - TRACE - -- encode_headers;
2024-03-31T17:40:46.407832486-05:00 - TRACE - sized write, len = 3886
2024-03-31T17:40:46.407840826-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3886
2024-03-31T17:40:46.407892986-05:00 - DEBUG - flushed 4088 bytes
2024-03-31T17:40:46.407907126-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:40:47.590685416-05:00 - TRACE - Conn::read_head
2024-03-31T17:40:47.590875054-05:00 - TRACE - received 1369 bytes
2024-03-31T17:40:47.590893414-05:00 - TRACE - parse_headers;
2024-03-31T17:40:47.590903534-05:00 - TRACE - -> parse_headers;
2024-03-31T17:40:47.590927433-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T17:40:47.590984263-05:00 - TRACE - Response.parse Complete(1202)
2024-03-31T17:40:47.591060442-05:00 - TRACE - <- parse_headers;
2024-03-31T17:40:47.591072722-05:00 - TRACE - -- parse_headers;
2024-03-31T17:40:47.591082352-05:00 - DEBUG - parsed 24 headers
2024-03-31T17:40:47.591091882-05:00 - DEBUG - incoming body is content-length (622 bytes)
2024-03-31T17:40:47.591119162-05:00 - TRACE - decode; state=Length(622)
2024-03-31T17:40:47.591139832-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(455)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:40:47.591235281-05:00 - TRACE - decode; state=Length(455)
2024-03-31T17:40:47.591362600-05:00 - TRACE - received 455 bytes
2024-03-31T17:40:47.591389799-05:00 - DEBUG - incoming body completed
2024-03-31T17:40:47.591424679-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T17:40:47.591462889-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:40:47.591484888-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:40:47.591506188-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T17:40:47.591524548-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T17:40:47.591591727-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:40:47.591612277-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "background = \"0x1a1a1a\"",
      "new": "background = \"0x1a1a1a1a\""
    }
  }
}
2024-03-31T17:40:47.591660617-05:00 - INFO - key change1
2024-03-31T17:40:47.591681227-05:00 - INFO - value "background = \"0x1a1a1a\""
2024-03-31T17:40:47.591714846-05:00 - INFO - old value is "background = \"0x1a1a1a\""
2024-03-31T17:40:47.591728056-05:00 - INFO - writing to file
2024-03-31T17:40:47.591823415-05:00 - INFO - New value: background = "0x1a1a1a1a"
2024-03-31T17:40:47.591853455-05:00 - INFO - 

2024-03-31T17:40:47.591861525-05:00 - INFO - is bool true true
2024-03-31T17:40:47.591891685-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"background = \\\"0x1a1a1a\\\"\",\n      \"new\": \"background = \\\"0x1a1a1a1a\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711924846,"id":"chatcmpl-98y90lcy4egFFuhgdrqLHAqOgEZIz","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":54,"prompt_tokens":1068,"total_tokens":1122}}
2024-03-31T17:40:59.163991353-05:00 - INFO - booting up
2024-03-31T17:40:59.165201452-05:00 - INFO - user request this alias to be reverted alacritty.toml 
2024-03-31T17:40:59.165249502-05:00 - INFO - checking if alias exist, so it could be reverted
2024-03-31T17:40:59.165259592-05:00 - INFO - content alias is alacritty.toml 
2024-03-31T17:40:59.165380241-05:00 - INFO - sucessfully reverted the original file to its backup file
2024-03-31T17:48:41.057820880-05:00 - INFO - booting up
2024-03-31T17:48:41.058938230-05:00 - INFO - going to get all possible content
2024-03-31T17:48:41.077394204-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T17:48:41.077450524-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T17:48:41.077493314-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T17:48:41.077608732-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T17:48:41.094035853-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-31T17:48:41.103276550-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-31T17:48:41.116272152-05:00 - TRACE - client handshake Http1
2024-03-31T17:48:41.116298112-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T17:48:41.116405341-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T17:48:41.116474520-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T17:48:41.116544399-05:00 - TRACE - encode_headers;
2024-03-31T17:48:41.116560079-05:00 - TRACE - -> encode_headers;
2024-03-31T17:48:41.116568449-05:00 - TRACE - Client::encode method=POST, body=Some(Known(331))
2024-03-31T17:48:41.116593769-05:00 - TRACE - <- encode_headers;
2024-03-31T17:48:41.116602149-05:00 - TRACE - -- encode_headers;
2024-03-31T17:48:41.116612269-05:00 - TRACE - sized write, len = 331
2024-03-31T17:48:41.116621009-05:00 - TRACE - buffer.flatten self.len=201 buf.len=331
2024-03-31T17:48:41.116661608-05:00 - DEBUG - flushed 532 bytes
2024-03-31T17:48:41.116675008-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:48:43.753816894-05:00 - TRACE - Conn::read_head
2024-03-31T17:48:43.754010292-05:00 - TRACE - received 1369 bytes
2024-03-31T17:48:43.754029232-05:00 - TRACE - parse_headers;
2024-03-31T17:48:43.754039622-05:00 - TRACE - -> parse_headers;
2024-03-31T17:48:43.754063922-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T17:48:43.754117481-05:00 - TRACE - Response.parse Complete(1203)
2024-03-31T17:48:43.754196350-05:00 - TRACE - <- parse_headers;
2024-03-31T17:48:43.754208550-05:00 - TRACE - -- parse_headers;
2024-03-31T17:48:43.754218200-05:00 - DEBUG - parsed 24 headers
2024-03-31T17:48:43.754227650-05:00 - DEBUG - incoming body is content-length (1406 bytes)
2024-03-31T17:48:43.754254660-05:00 - TRACE - decode; state=Length(1406)
2024-03-31T17:48:43.754275970-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(1240)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:48:43.754364819-05:00 - TRACE - decode; state=Length(1240)
2024-03-31T17:48:43.754521367-05:00 - TRACE - received 1240 bytes
2024-03-31T17:48:43.754538287-05:00 - DEBUG - incoming body completed
2024-03-31T17:48:43.754564247-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T17:48:43.754596947-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:48:43.754615126-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:48:43.754633706-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T17:48:43.754649016-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T17:48:43.754711766-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:48:43.754729405-05:00 - INFO - json response {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"1. Changing the font size\n2. Setting the color scheme\n3. Adjusting the opacity of the terminal window\n4. Setting key bindings for specific actions\n5. Configuring the scrollback buffer size\n6. Enabling or disabling specific features such as ligatures\n7. Modifying the cursor style and behavior\n8. Specifying the shell program to be used\n9. Configuring mouse input options\n10. Customizing window dimensions and position\n11. Adjusting the scroll speed\n12. Configuring the terminal bell behavior\n13. Setting the default program to be launched\n14. Configuring tab behavior and appearance\n15. Setting specific window title behavior\n16. Configuring the display refresh rate\n17. Changing the background image\n18. Customizing the border appearance of the terminal window\n19. Modifying the padding and margins of the terminal window\n20. Setting specific options for fullscreen mode\n21. Configuring the tab bar appearance and behavior","role":"assistant"}}],"created":1711925321,"id":"chatcmpl-98yGfl1gWtlNO0mcwAYTxXg4UMojO","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_b28b39ffa8","usage":{"completion_tokens":199,"prompt_tokens":54,"total_tokens":253}}
2024-03-31T17:48:43.754855864-05:00 - TRACE - client tx closed
2024-03-31T17:48:43.754875954-05:00 - TRACE - State::close_read()
2024-03-31T17:48:43.754890494-05:00 - TRACE - State::close_write()
2024-03-31T17:48:43.754904974-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-31T17:48:43.754951153-05:00 - TRACE - shut down IO complete
2024-03-31T17:55:52.304639584-05:00 - INFO - booting up
2024-03-31T17:55:52.306272100-05:00 - INFO - alias was selected
2024-03-31T17:55:52.306347679-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-31T17:55:52.336822064-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T17:55:52.336908263-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T17:55:52.336953413-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T17:55:52.337080001-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T17:55:52.354423224-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-31T17:55:52.362338183-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-31T17:55:52.377082069-05:00 - TRACE - client handshake Http1
2024-03-31T17:55:52.377120568-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T17:55:52.377270487-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T17:55:52.377374736-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T17:55:52.377639504-05:00 - TRACE - encode_headers;
2024-03-31T17:55:52.377733133-05:00 - TRACE - -> encode_headers;
2024-03-31T17:55:52.377745613-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3895))
2024-03-31T17:55:52.377778183-05:00 - TRACE - <- encode_headers;
2024-03-31T17:55:52.377789032-05:00 - TRACE - -- encode_headers;
2024-03-31T17:55:52.377802412-05:00 - TRACE - sized write, len = 3895
2024-03-31T17:55:52.377813062-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3895
2024-03-31T17:55:52.377883432-05:00 - DEBUG - flushed 4097 bytes
2024-03-31T17:55:52.377901701-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:55:54.554610661-05:00 - TRACE - Conn::read_head
2024-03-31T17:55:54.554799809-05:00 - TRACE - received 1369 bytes
2024-03-31T17:55:54.554818659-05:00 - TRACE - parse_headers;
2024-03-31T17:55:54.554829219-05:00 - TRACE - -> parse_headers;
2024-03-31T17:55:54.554851219-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T17:55:54.554906458-05:00 - TRACE - Response.parse Complete(1203)
2024-03-31T17:55:54.554975428-05:00 - TRACE - <- parse_headers;
2024-03-31T17:55:54.554987108-05:00 - TRACE - -- parse_headers;
2024-03-31T17:55:54.554996938-05:00 - DEBUG - parsed 24 headers
2024-03-31T17:55:54.555006487-05:00 - DEBUG - incoming body is content-length (859 bytes)
2024-03-31T17:55:54.555033207-05:00 - TRACE - decode; state=Length(859)
2024-03-31T17:55:54.555054527-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(693)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:55:54.555125996-05:00 - TRACE - decode; state=Length(693)
2024-03-31T17:55:54.555214056-05:00 - TRACE - received 693 bytes
2024-03-31T17:55:54.555232595-05:00 - DEBUG - incoming body completed
2024-03-31T17:55:54.555264395-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T17:55:54.555302535-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:55:54.555325025-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:55:54.555346704-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T17:55:54.555364934-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T17:55:54.555486163-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:55:54.555515643-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "black = \"0x000000\"",
      "new": "black = \"0x008000\""
    },
    "change2": {
      "old": "black = \"0x002b36\"",
      "new": "black = \"0x008000\""
    },
    "change3": {
      "old": "foreground = \"0x8caad2\"",
      "new": "foreground = \"0x008000\""
    }
  }
}
2024-03-31T17:55:54.555571742-05:00 - INFO - key change1
2024-03-31T17:55:54.555582972-05:00 - INFO - value "black = \"0x000000\""
2024-03-31T17:55:54.555615272-05:00 - INFO - old value is "black = \"0x000000\""
2024-03-31T17:55:54.555629382-05:00 - INFO - writing to file
2024-03-31T17:55:54.555753931-05:00 - INFO - key change2
2024-03-31T17:55:54.555772441-05:00 - INFO - value "black = \"0x002b36\""
2024-03-31T17:55:54.555805870-05:00 - INFO - old value is "black = \"0x002b36\""
2024-03-31T17:55:54.555819250-05:00 - INFO - writing to file
2024-03-31T17:55:54.556555254-05:00 - INFO - key change3
2024-03-31T17:55:54.556578453-05:00 - INFO - value "foreground = \"0x8caad2\""
2024-03-31T17:55:54.556613993-05:00 - INFO - old value is "foreground = \"0x8caad2\""
2024-03-31T17:55:54.556628813-05:00 - INFO - writing to file
2024-03-31T17:55:54.556652003-05:00 - TRACE - idle interval checking for expired
2024-03-31T17:55:54.556741432-05:00 - INFO - New value: black = "0x008000"
2024-03-31T17:55:54.556784171-05:00 - INFO - 

2024-03-31T17:55:54.556794841-05:00 - INFO - is bool true true
2024-03-31T17:55:54.556835531-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"black = \\\"0x000000\\\"\",\n      \"new\": \"black = \\\"0x008000\\\"\"\n    },\n    \"change2\": {\n      \"old\": \"black = \\\"0x002b36\\\"\",\n      \"new\": \"black = \\\"0x008000\\\"\"\n    },\n    \"change3\": {\n      \"old\": \"foreground = \\\"0x8caad2\\\"\",\n      \"new\": \"foreground = \\\"0x008000\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711925752,"id":"chatcmpl-98yNcO9ZLTa1irsQUN83naPmpAfwM","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":117,"prompt_tokens":1075,"total_tokens":1192}}
2024-03-31T17:55:54.556913840-05:00 - TRACE - client tx closed
2024-03-31T17:55:54.556942670-05:00 - TRACE - State::close_read()
2024-03-31T17:55:54.556959980-05:00 - TRACE - State::close_write()
2024-03-31T17:55:54.556983250-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-31T17:55:54.557038459-05:00 - TRACE - shut down IO complete
2024-03-31T17:56:09.877037547-05:00 - INFO - booting up
2024-03-31T17:56:09.878247146-05:00 - INFO - user request this alias to be reverted alacritty.toml 
2024-03-31T17:56:09.878287426-05:00 - INFO - checking if alias exist, so it could be reverted
2024-03-31T17:56:09.878295496-05:00 - INFO - content alias is alacritty.toml 
2024-03-31T17:56:09.878388935-05:00 - INFO - sucessfully reverted the original file to its backup file
2024-03-31T17:56:37.192356079-05:00 - INFO - booting up
2024-03-31T17:56:37.193565678-05:00 - INFO - alias was selected
2024-03-31T17:56:37.193617038-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-31T17:56:37.213847935-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T17:56:37.213908024-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T17:56:37.213937164-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T17:56:37.214041263-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T17:56:37.219222356-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-31T17:56:37.226443180-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-31T17:56:37.239720761-05:00 - TRACE - client handshake Http1
2024-03-31T17:56:37.239752950-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T17:56:37.239854579-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T17:56:37.239923659-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T17:56:37.239996338-05:00 - TRACE - encode_headers;
2024-03-31T17:56:37.240011798-05:00 - TRACE - -> encode_headers;
2024-03-31T17:56:37.240020338-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3873))
2024-03-31T17:56:37.240045228-05:00 - TRACE - <- encode_headers;
2024-03-31T17:56:37.240053248-05:00 - TRACE - -- encode_headers;
2024-03-31T17:56:37.240067308-05:00 - TRACE - sized write, len = 3873
2024-03-31T17:56:37.240075557-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3873
2024-03-31T17:56:37.240127597-05:00 - DEBUG - flushed 4075 bytes
2024-03-31T17:56:37.240141277-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:56:38.279549124-05:00 - TRACE - Conn::read_head
2024-03-31T17:56:38.279735022-05:00 - TRACE - received 1369 bytes
2024-03-31T17:56:38.279754392-05:00 - TRACE - parse_headers;
2024-03-31T17:56:38.279764862-05:00 - TRACE - -> parse_headers;
2024-03-31T17:56:38.279786312-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T17:56:38.279844221-05:00 - TRACE - Response.parse Complete(1202)
2024-03-31T17:56:38.279917830-05:00 - TRACE - <- parse_headers;
2024-03-31T17:56:38.279930020-05:00 - TRACE - -- parse_headers;
2024-03-31T17:56:38.279939600-05:00 - DEBUG - parsed 24 headers
2024-03-31T17:56:38.279949030-05:00 - DEBUG - incoming body is content-length (620 bytes)
2024-03-31T17:56:38.279975560-05:00 - TRACE - decode; state=Length(620)
2024-03-31T17:56:38.279995510-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(453)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:56:38.280076789-05:00 - TRACE - decode; state=Length(453)
2024-03-31T17:56:38.280201878-05:00 - TRACE - received 453 bytes
2024-03-31T17:56:38.280219888-05:00 - DEBUG - incoming body completed
2024-03-31T17:56:38.280254287-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T17:56:38.280292227-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:56:38.280314097-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:56:38.280335257-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T17:56:38.280353356-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T17:56:38.280419696-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:56:38.280444186-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "background = \"0x1a1a1a\"",
      "new": "background = \"0xdcdcdc\""
    }
  }
}
2024-03-31T17:56:38.280490365-05:00 - INFO - key change1
2024-03-31T17:56:38.280509815-05:00 - INFO - value "background = \"0x1a1a1a\""
2024-03-31T17:56:38.280536685-05:00 - INFO - old value is "background = \"0x1a1a1a\""
2024-03-31T17:56:38.280548715-05:00 - INFO - writing to file
2024-03-31T17:56:38.280652404-05:00 - INFO - New value: background = "0xdcdcdc"
2024-03-31T17:56:38.280682933-05:00 - INFO - 

2024-03-31T17:56:38.280705523-05:00 - INFO - is bool true true
2024-03-31T17:56:38.280733783-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"background = \\\"0x1a1a1a\\\"\",\n      \"new\": \"background = \\\"0xdcdcdc\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711925797,"id":"chatcmpl-98yOLuCo8Z00DcryYMpLsUBPijVdR","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":48,"prompt_tokens":1068,"total_tokens":1116}}
2024-03-31T17:56:48.791973058-05:00 - INFO - booting up
2024-03-31T17:56:48.793147847-05:00 - INFO - user request this alias to be reverted alacritty.toml 
2024-03-31T17:56:48.793186427-05:00 - INFO - checking if alias exist, so it could be reverted
2024-03-31T17:56:48.793194597-05:00 - INFO - content alias is alacritty.toml 
2024-03-31T17:56:48.793288546-05:00 - INFO - sucessfully reverted the original file to its backup file
2024-03-31T17:56:58.727355617-05:00 - INFO - booting up
2024-03-31T17:56:58.728553596-05:00 - INFO - alias was selected
2024-03-31T17:56:58.728605536-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-31T17:56:58.748549115-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T17:56:58.748606144-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T17:56:58.748638054-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T17:56:58.748751863-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T17:56:58.765410052-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-31T17:56:58.774659169-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-31T17:56:58.791649925-05:00 - TRACE - client handshake Http1
2024-03-31T17:56:58.791693364-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T17:56:58.791805923-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T17:56:58.791923232-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T17:56:58.791994361-05:00 - TRACE - encode_headers;
2024-03-31T17:56:58.792010551-05:00 - TRACE - -> encode_headers;
2024-03-31T17:56:58.792019421-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3878))
2024-03-31T17:56:58.792044771-05:00 - TRACE - <- encode_headers;
2024-03-31T17:56:58.792053021-05:00 - TRACE - -- encode_headers;
2024-03-31T17:56:58.792063131-05:00 - TRACE - sized write, len = 3878
2024-03-31T17:56:58.792071311-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3878
2024-03-31T17:56:58.792123200-05:00 - DEBUG - flushed 4080 bytes
2024-03-31T17:56:58.792137050-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:56:59.989167690-05:00 - TRACE - Conn::read_head
2024-03-31T17:56:59.989352588-05:00 - TRACE - received 1369 bytes
2024-03-31T17:56:59.989371548-05:00 - TRACE - parse_headers;
2024-03-31T17:56:59.989382108-05:00 - TRACE - -> parse_headers;
2024-03-31T17:56:59.989403868-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T17:56:59.989467427-05:00 - TRACE - Response.parse Complete(1202)
2024-03-31T17:56:59.989557146-05:00 - TRACE - <- parse_headers;
2024-03-31T17:56:59.989572986-05:00 - TRACE - -- parse_headers;
2024-03-31T17:56:59.989583226-05:00 - DEBUG - parsed 24 headers
2024-03-31T17:56:59.989592896-05:00 - DEBUG - incoming body is content-length (622 bytes)
2024-03-31T17:56:59.989622206-05:00 - TRACE - decode; state=Length(622)
2024-03-31T17:56:59.989643285-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(455)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:56:59.989730495-05:00 - TRACE - decode; state=Length(455)
2024-03-31T17:56:59.989864003-05:00 - TRACE - received 455 bytes
2024-03-31T17:56:59.989881973-05:00 - DEBUG - incoming body completed
2024-03-31T17:56:59.989913913-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T17:56:59.989963113-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:56:59.989986512-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:56:59.990008222-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T17:56:59.990026122-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T17:56:59.990091311-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:56:59.990108261-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "background = \"0x1a1a1a\"",
      "new": "background = \"0x1a1a1a80\""
    }
  }
}
2024-03-31T17:56:59.990143391-05:00 - INFO - key change1
2024-03-31T17:56:59.990161401-05:00 - INFO - value "background = \"0x1a1a1a\""
2024-03-31T17:56:59.990198180-05:00 - INFO - old value is "background = \"0x1a1a1a\""
2024-03-31T17:56:59.990211890-05:00 - INFO - writing to file
2024-03-31T17:56:59.990310489-05:00 - INFO - New value: background = "0x1a1a1a80"
2024-03-31T17:56:59.990340619-05:00 - INFO - 

2024-03-31T17:56:59.990348699-05:00 - INFO - is bool true true
2024-03-31T17:56:59.990377759-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"background = \\\"0x1a1a1a\\\"\",\n      \"new\": \"background = \\\"0x1a1a1a80\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711925819,"id":"chatcmpl-98yOh6b5qyFRw2H8XqyDDivoPjDfP","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":53,"prompt_tokens":1067,"total_tokens":1120}}
2024-03-31T17:57:09.075334668-05:00 - INFO - booting up
2024-03-31T17:57:09.076406468-05:00 - INFO - user request this alias to be reverted alacritty.toml 
2024-03-31T17:57:09.076449138-05:00 - INFO - checking if alias exist, so it could be reverted
2024-03-31T17:57:09.076457677-05:00 - INFO - content alias is alacritty.toml 
2024-03-31T17:57:09.076549607-05:00 - INFO - sucessfully reverted the original file to its backup file
2024-03-31T17:57:54.046322966-05:00 - INFO - booting up
2024-03-31T17:58:15.524415272-05:00 - INFO - booting up
2024-03-31T17:58:15.525540902-05:00 - INFO - alias was selected
2024-03-31T17:58:15.525600311-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-31T17:58:15.543891165-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T17:58:15.543947375-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T17:58:15.543979234-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T17:58:15.544081713-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T17:58:15.554870065-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-31T17:58:15.564355040-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-31T17:58:15.580009977-05:00 - TRACE - client handshake Http1
2024-03-31T17:58:15.580041987-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T17:58:15.580138686-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T17:58:15.580207566-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T17:58:15.580316155-05:00 - TRACE - encode_headers;
2024-03-31T17:58:15.580384814-05:00 - TRACE - -> encode_headers;
2024-03-31T17:58:15.580394304-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3894))
2024-03-31T17:58:15.580416274-05:00 - TRACE - <- encode_headers;
2024-03-31T17:58:15.580424294-05:00 - TRACE - -- encode_headers;
2024-03-31T17:58:15.580434554-05:00 - TRACE - sized write, len = 3894
2024-03-31T17:58:15.580442863-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3894
2024-03-31T17:58:15.580497473-05:00 - DEBUG - flushed 4096 bytes
2024-03-31T17:58:15.580520443-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:58:17.710941948-05:00 - TRACE - Conn::read_head
2024-03-31T17:58:17.711137846-05:00 - TRACE - received 1369 bytes
2024-03-31T17:58:17.711156736-05:00 - TRACE - parse_headers;
2024-03-31T17:58:17.711173356-05:00 - TRACE - -> parse_headers;
2024-03-31T17:58:17.711197346-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T17:58:17.711246155-05:00 - TRACE - Response.parse Complete(1203)
2024-03-31T17:58:17.711320764-05:00 - TRACE - <- parse_headers;
2024-03-31T17:58:17.711333824-05:00 - TRACE - -- parse_headers;
2024-03-31T17:58:17.711343634-05:00 - DEBUG - parsed 24 headers
2024-03-31T17:58:17.711353164-05:00 - DEBUG - incoming body is content-length (589 bytes)
2024-03-31T17:58:17.711379734-05:00 - TRACE - decode; state=Length(589)
2024-03-31T17:58:17.711400174-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(423)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:58:17.711477893-05:00 - TRACE - decode; state=Length(423)
2024-03-31T17:58:17.711569532-05:00 - TRACE - received 423 bytes
2024-03-31T17:58:17.711587532-05:00 - DEBUG - incoming body completed
2024-03-31T17:58:17.711617172-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T17:58:17.711655461-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:58:17.711678261-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:58:17.711700001-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T17:58:17.711718341-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T17:58:17.711785560-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:58:17.711804420-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "opacity = 0.8",
      "new": "opacity = 0.85"
    }
  }
}
2024-03-31T17:58:17.711835850-05:00 - INFO - key change1
2024-03-31T17:58:17.711844820-05:00 - INFO - value "opacity = 0.8"
2024-03-31T17:58:17.711882449-05:00 - INFO - old value is "opacity = 0.8"
2024-03-31T17:58:17.711898539-05:00 - INFO - writing to file
2024-03-31T17:58:17.711987138-05:00 - INFO - New value: opacity = 0.85
2024-03-31T17:58:17.712018948-05:00 - INFO - 

2024-03-31T17:58:17.712027618-05:00 - INFO - is bool true true
2024-03-31T17:58:17.712057078-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"opacity = 0.8\",\n      \"new\": \"opacity = 0.85\"\n    }\n  }\n}","role":"assistant"}}],"created":1711925895,"id":"chatcmpl-98yPvDrEuokZyx2uvn8gHUWIk8KF4","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":41,"prompt_tokens":1076,"total_tokens":1117}}
2024-03-31T17:58:25.669588180-05:00 - INFO - booting up
2024-03-31T17:58:25.670720920-05:00 - INFO - alias was selected
2024-03-31T17:58:25.670770869-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-31T17:58:25.688948654-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T17:58:25.689013064-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T17:58:25.689049303-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T17:58:25.689147552-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T17:58:25.693777291-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-31T17:58:25.705630874-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-31T17:58:25.721801898-05:00 - TRACE - client handshake Http1
2024-03-31T17:58:25.721829027-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T17:58:25.721939726-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T17:58:25.722017576-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T17:58:25.722087105-05:00 - TRACE - encode_headers;
2024-03-31T17:58:25.722103365-05:00 - TRACE - -> encode_headers;
2024-03-31T17:58:25.722112015-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3896))
2024-03-31T17:58:25.722136455-05:00 - TRACE - <- encode_headers;
2024-03-31T17:58:25.722144424-05:00 - TRACE - -- encode_headers;
2024-03-31T17:58:25.722154724-05:00 - TRACE - sized write, len = 3896
2024-03-31T17:58:25.722163094-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3896
2024-03-31T17:58:25.722214294-05:00 - DEBUG - flushed 4098 bytes
2024-03-31T17:58:25.722228114-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:58:26.830965071-05:00 - TRACE - Conn::read_head
2024-03-31T17:58:26.831149829-05:00 - TRACE - received 1369 bytes
2024-03-31T17:58:26.831169239-05:00 - TRACE - parse_headers;
2024-03-31T17:58:26.831179569-05:00 - TRACE - -> parse_headers;
2024-03-31T17:58:26.831200909-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T17:58:26.831254439-05:00 - TRACE - Response.parse Complete(1201)
2024-03-31T17:58:26.831328648-05:00 - TRACE - <- parse_headers;
2024-03-31T17:58:26.831340958-05:00 - TRACE - -- parse_headers;
2024-03-31T17:58:26.831350608-05:00 - DEBUG - parsed 24 headers
2024-03-31T17:58:26.831360108-05:00 - DEBUG - incoming body is content-length (590 bytes)
2024-03-31T17:58:26.831386397-05:00 - TRACE - decode; state=Length(590)
2024-03-31T17:58:26.831407167-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(422)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T17:58:26.831493556-05:00 - TRACE - decode; state=Length(422)
2024-03-31T17:58:26.831603385-05:00 - TRACE - received 422 bytes
2024-03-31T17:58:26.831627085-05:00 - DEBUG - incoming body completed
2024-03-31T17:58:26.831653715-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T17:58:26.831685285-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:58:26.831703185-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:58:26.831720684-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T17:58:26.831735384-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T17:58:26.831791784-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T17:58:26.831814044-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "opacity = 0.85",
      "new": "opacity = 0.75"
    }
  }
}
2024-03-31T17:58:26.831845843-05:00 - INFO - key change1
2024-03-31T17:58:26.831860073-05:00 - INFO - value "opacity = 0.85"
2024-03-31T17:58:26.831885113-05:00 - INFO - old value is "opacity = 0.85"
2024-03-31T17:58:26.831899693-05:00 - INFO - writing to file
2024-03-31T17:58:26.831996832-05:00 - INFO - New value: opacity = 0.75
2024-03-31T17:58:26.832030382-05:00 - INFO - 

2024-03-31T17:58:26.832038202-05:00 - INFO - is bool true true
2024-03-31T17:58:26.832064451-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"opacity = 0.85\",\n      \"new\": \"opacity = 0.75\"\n    }\n  }\n}","role":"assistant"}}],"created":1711925905,"id":"chatcmpl-98yQ5TemVLoCVdbk6akCILdxEt2kM","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":41,"prompt_tokens":1077,"total_tokens":1118}}
2024-03-31T18:00:08.536524420-05:00 - INFO - booting up
2024-03-31T18:00:08.537716319-05:00 - INFO - alias was selected
2024-03-31T18:00:08.537779379-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-31T18:00:08.556046303-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T18:00:08.556110412-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T18:00:08.556145502-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T18:00:08.556244981-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T18:00:08.568352781-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-31T18:00:08.574698064-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-31T18:00:08.590981276-05:00 - TRACE - client handshake Http1
2024-03-31T18:00:08.591010386-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T18:00:08.591122585-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T18:00:08.591191824-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T18:00:08.591269723-05:00 - TRACE - encode_headers;
2024-03-31T18:00:08.591285803-05:00 - TRACE - -> encode_headers;
2024-03-31T18:00:08.591294593-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3896))
2024-03-31T18:00:08.591319393-05:00 - TRACE - <- encode_headers;
2024-03-31T18:00:08.591327273-05:00 - TRACE - -- encode_headers;
2024-03-31T18:00:08.591337443-05:00 - TRACE - sized write, len = 3896
2024-03-31T18:00:08.591345723-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3896
2024-03-31T18:00:08.591396902-05:00 - DEBUG - flushed 4098 bytes
2024-03-31T18:00:08.591410722-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:00:09.535771899-05:00 - TRACE - Conn::read_head
2024-03-31T18:00:09.535944928-05:00 - TRACE - received 1369 bytes
2024-03-31T18:00:09.535961137-05:00 - TRACE - parse_headers;
2024-03-31T18:00:09.535969837-05:00 - TRACE - -> parse_headers;
2024-03-31T18:00:09.535989117-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T18:00:09.536033497-05:00 - TRACE - Response.parse Complete(1201)
2024-03-31T18:00:09.536095506-05:00 - TRACE - <- parse_headers;
2024-03-31T18:00:09.536105626-05:00 - TRACE - -- parse_headers;
2024-03-31T18:00:09.536113636-05:00 - DEBUG - parsed 24 headers
2024-03-31T18:00:09.536121406-05:00 - DEBUG - incoming body is content-length (590 bytes)
2024-03-31T18:00:09.536144896-05:00 - TRACE - decode; state=Length(590)
2024-03-31T18:00:09.536163016-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(422)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:00:09.536401723-05:00 - TRACE - decode; state=Length(422)
2024-03-31T18:00:09.536437173-05:00 - TRACE - received 422 bytes
2024-03-31T18:00:09.536449723-05:00 - DEBUG - incoming body completed
2024-03-31T18:00:09.536471333-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T18:00:09.536501423-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:00:09.536519162-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:00:09.536536212-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T18:00:09.536551362-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T18:00:09.536607852-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:00:09.536628611-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "opacity = 0.75",
      "new": "opacity = 0.70"
    }
  }
}
2024-03-31T18:00:09.536670061-05:00 - INFO - key change1
2024-03-31T18:00:09.536681741-05:00 - INFO - value "opacity = 0.75"
2024-03-31T18:00:09.536713451-05:00 - INFO - old value is "opacity = 0.75"
2024-03-31T18:00:09.536732950-05:00 - INFO - writing to file
2024-03-31T18:00:09.536849479-05:00 - INFO - New value: opacity = 0.70
2024-03-31T18:00:09.536890309-05:00 - INFO - 

2024-03-31T18:00:09.536898439-05:00 - INFO - is bool true true
2024-03-31T18:00:09.536922939-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"opacity = 0.75\",\n      \"new\": \"opacity = 0.70\"\n    }\n  }\n}","role":"assistant"}}],"created":1711926008,"id":"chatcmpl-98yRkFTGuxIkGTuIjmpONmaI7meIT","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":41,"prompt_tokens":1077,"total_tokens":1118}}
2024-03-31T18:00:09.536974658-05:00 - TRACE - client tx closed
2024-03-31T18:00:09.536991908-05:00 - TRACE - State::close_read()
2024-03-31T18:00:09.537003268-05:00 - TRACE - State::close_write()
2024-03-31T18:00:09.537015608-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-31T18:00:09.537090747-05:00 - TRACE - shut down IO complete
2024-03-31T18:00:15.243084464-05:00 - INFO - booting up
2024-03-31T18:00:15.244256423-05:00 - INFO - alias was selected
2024-03-31T18:00:15.244306963-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-31T18:00:15.262790576-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T18:00:15.262847715-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T18:00:15.262879505-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T18:00:15.262987004-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T18:00:15.267790410-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-31T18:00:15.274158562-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-31T18:00:15.287133095-05:00 - TRACE - client handshake Http1
2024-03-31T18:00:15.287168545-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T18:00:15.287444442-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T18:00:15.287541351-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T18:00:15.287612841-05:00 - TRACE - encode_headers;
2024-03-31T18:00:15.287628821-05:00 - TRACE - -> encode_headers;
2024-03-31T18:00:15.287637610-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3896))
2024-03-31T18:00:15.287662230-05:00 - TRACE - <- encode_headers;
2024-03-31T18:00:15.287670460-05:00 - TRACE - -- encode_headers;
2024-03-31T18:00:15.287681010-05:00 - TRACE - sized write, len = 3896
2024-03-31T18:00:15.287689470-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3896
2024-03-31T18:00:15.287740590-05:00 - DEBUG - flushed 4098 bytes
2024-03-31T18:00:15.287754499-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:00:16.393940490-05:00 - TRACE - Conn::read_head
2024-03-31T18:00:16.394122469-05:00 - TRACE - received 1369 bytes
2024-03-31T18:00:16.394141129-05:00 - TRACE - parse_headers;
2024-03-31T18:00:16.394151658-05:00 - TRACE - -> parse_headers;
2024-03-31T18:00:16.394177368-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T18:00:16.394235748-05:00 - TRACE - Response.parse Complete(1202)
2024-03-31T18:00:16.394312607-05:00 - TRACE - <- parse_headers;
2024-03-31T18:00:16.394324657-05:00 - TRACE - -- parse_headers;
2024-03-31T18:00:16.394334537-05:00 - DEBUG - parsed 24 headers
2024-03-31T18:00:16.394344027-05:00 - DEBUG - incoming body is content-length (569 bytes)
2024-03-31T18:00:16.394371836-05:00 - TRACE - decode; state=Length(569)
2024-03-31T18:00:16.394392886-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(402)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:00:16.394464246-05:00 - TRACE - decode; state=Length(402)
2024-03-31T18:00:16.394574265-05:00 - TRACE - received 402 bytes
2024-03-31T18:00:16.394589074-05:00 - DEBUG - incoming body completed
2024-03-31T18:00:16.394616674-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T18:00:16.394649174-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:00:16.394666954-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:00:16.394695243-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T18:00:16.394711503-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T18:00:16.394770143-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:00:16.394794762-05:00 - INFO - text full json {
  "results": {
    "change1": {"old": "opacity = 0.70", "new": "opacity = 0.65"}
  }
}
2024-03-31T18:00:16.394841622-05:00 - INFO - key change1
2024-03-31T18:00:16.394852952-05:00 - INFO - value "opacity = 0.70"
2024-03-31T18:00:16.394883292-05:00 - INFO - old value is "opacity = 0.70"
2024-03-31T18:00:16.394902511-05:00 - INFO - writing to file
2024-03-31T18:00:16.395004130-05:00 - INFO - New value: opacity = 0.65
2024-03-31T18:00:16.395037830-05:00 - INFO - 

2024-03-31T18:00:16.395053570-05:00 - INFO - is bool true true
2024-03-31T18:00:16.395077470-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\"old\": \"opacity = 0.70\", \"new\": \"opacity = 0.65\"}\n  }\n}","role":"assistant"}}],"created":1711926015,"id":"chatcmpl-98yRrLJLatasyGM4gxnzqMD1GoYnQ","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":36,"prompt_tokens":1077,"total_tokens":1113}}
2024-03-31T18:00:16.395122739-05:00 - TRACE - client tx closed
2024-03-31T18:00:16.395136209-05:00 - TRACE - State::close_read()
2024-03-31T18:00:16.395147149-05:00 - TRACE - State::close_write()
2024-03-31T18:00:16.395158379-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-31T18:00:16.395195259-05:00 - TRACE - shut down IO complete
2024-03-31T18:00:36.690300078-05:00 - INFO - booting up
2024-03-31T18:00:36.691426248-05:00 - INFO - alias was selected
2024-03-31T18:00:36.691475738-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-31T18:00:36.709982480-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T18:00:36.710037069-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T18:00:36.710073729-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T18:00:36.710182308-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T18:00:36.714574269-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-31T18:00:36.723214011-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-31T18:00:36.738856549-05:00 - TRACE - client handshake Http1
2024-03-31T18:00:36.738888778-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T18:00:36.739000887-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T18:00:36.739072937-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T18:00:36.739142876-05:00 - TRACE - encode_headers;
2024-03-31T18:00:36.739158516-05:00 - TRACE - -> encode_headers;
2024-03-31T18:00:36.739167076-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3896))
2024-03-31T18:00:36.739191746-05:00 - TRACE - <- encode_headers;
2024-03-31T18:00:36.739199816-05:00 - TRACE - -- encode_headers;
2024-03-31T18:00:36.739209736-05:00 - TRACE - sized write, len = 3896
2024-03-31T18:00:36.739217966-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3896
2024-03-31T18:00:36.739270245-05:00 - DEBUG - flushed 4098 bytes
2024-03-31T18:00:36.739284195-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:00:37.795624047-05:00 - TRACE - Conn::read_head
2024-03-31T18:00:37.795808935-05:00 - TRACE - received 1369 bytes
2024-03-31T18:00:37.795827715-05:00 - TRACE - parse_headers;
2024-03-31T18:00:37.795838215-05:00 - TRACE - -> parse_headers;
2024-03-31T18:00:37.795862415-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T18:00:37.795916394-05:00 - TRACE - Response.parse Complete(1201)
2024-03-31T18:00:37.796004163-05:00 - TRACE - <- parse_headers;
2024-03-31T18:00:37.796017953-05:00 - TRACE - -- parse_headers;
2024-03-31T18:00:37.796027673-05:00 - DEBUG - parsed 24 headers
2024-03-31T18:00:37.796037343-05:00 - DEBUG - incoming body is content-length (589 bytes)
2024-03-31T18:00:37.796064053-05:00 - TRACE - decode; state=Length(589)
2024-03-31T18:00:37.796084533-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(421)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:00:37.796174182-05:00 - TRACE - decode; state=Length(421)
2024-03-31T18:00:37.796303491-05:00 - TRACE - received 421 bytes
2024-03-31T18:00:37.796321350-05:00 - DEBUG - incoming body completed
2024-03-31T18:00:37.796353270-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T18:00:37.796391280-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:00:37.796413390-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:00:37.796434729-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T18:00:37.796452819-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T18:00:37.796519259-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:00:37.796538068-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "opacity = 0.65",
      "new": "opacity = 0.6"
    }
  }
}
2024-03-31T18:00:37.796584358-05:00 - INFO - key change1
2024-03-31T18:00:37.796606188-05:00 - INFO - value "opacity = 0.65"
2024-03-31T18:00:37.796642267-05:00 - INFO - old value is "opacity = 0.65"
2024-03-31T18:00:37.796662707-05:00 - INFO - writing to file
2024-03-31T18:00:37.796790606-05:00 - INFO - New value: opacity = 0.6
2024-03-31T18:00:37.796838816-05:00 - INFO - 

2024-03-31T18:00:37.796849125-05:00 - INFO - is bool true true
2024-03-31T18:00:37.796880105-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"opacity = 0.65\",\n      \"new\": \"opacity = 0.6\"\n    }\n  }\n}","role":"assistant"}}],"created":1711926037,"id":"chatcmpl-98ySDcMK268H7mkKmAFufJW5L2TvE","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":41,"prompt_tokens":1077,"total_tokens":1118}}
2024-03-31T18:00:55.462441503-05:00 - INFO - booting up
2024-03-31T18:00:55.463588123-05:00 - INFO - alias was selected
2024-03-31T18:00:55.463643572-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-31T18:00:55.483599571-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T18:00:55.483664991-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T18:00:55.483696831-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T18:00:55.483813950-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T18:00:55.513672218-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-31T18:00:55.521877564-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-31T18:00:55.536516942-05:00 - TRACE - client handshake Http1
2024-03-31T18:00:55.536546271-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T18:00:55.536659370-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T18:00:55.536729210-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T18:00:55.536808149-05:00 - TRACE - encode_headers;
2024-03-31T18:00:55.536823879-05:00 - TRACE - -> encode_headers;
2024-03-31T18:00:55.536832649-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3895))
2024-03-31T18:00:55.536857558-05:00 - TRACE - <- encode_headers;
2024-03-31T18:00:55.536865888-05:00 - TRACE - -- encode_headers;
2024-03-31T18:00:55.536876088-05:00 - TRACE - sized write, len = 3895
2024-03-31T18:00:55.536893938-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3895
2024-03-31T18:00:55.536947078-05:00 - DEBUG - flushed 4097 bytes
2024-03-31T18:00:55.536961237-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:00:56.639134155-05:00 - TRACE - Conn::read_head
2024-03-31T18:00:56.639328543-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:00:56.639548411-05:00 - TRACE - Conn::read_head
2024-03-31T18:00:56.639602730-05:00 - TRACE - received 1369 bytes
2024-03-31T18:00:56.639618620-05:00 - TRACE - parse_headers;
2024-03-31T18:00:56.639628970-05:00 - TRACE - -> parse_headers;
2024-03-31T18:00:56.639653630-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T18:00:56.639701539-05:00 - TRACE - Response.parse Complete(1202)
2024-03-31T18:00:56.639770659-05:00 - TRACE - <- parse_headers;
2024-03-31T18:00:56.639782218-05:00 - TRACE - -- parse_headers;
2024-03-31T18:00:56.639792108-05:00 - DEBUG - parsed 24 headers
2024-03-31T18:00:56.639801548-05:00 - DEBUG - incoming body is content-length (588 bytes)
2024-03-31T18:00:56.639832428-05:00 - TRACE - decode; state=Length(588)
2024-03-31T18:00:56.639853778-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(421)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:00:56.639882157-05:00 - TRACE - decode; state=Length(421)
2024-03-31T18:00:56.639917817-05:00 - TRACE - received 421 bytes
2024-03-31T18:00:56.639932837-05:00 - DEBUG - incoming body completed
2024-03-31T18:00:56.639966587-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T18:00:56.640002536-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:00:56.640023726-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:00:56.640043816-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T18:00:56.640061686-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T18:00:56.640128335-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:00:56.640152345-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "opacity = 0.6",
      "new": "opacity = 0.7"
    }
  }
}
2024-03-31T18:00:56.640186615-05:00 - INFO - key change1
2024-03-31T18:00:56.640195025-05:00 - INFO - value "opacity = 0.6"
2024-03-31T18:00:56.640226384-05:00 - INFO - old value is "opacity = 0.6"
2024-03-31T18:00:56.640253474-05:00 - INFO - writing to file
2024-03-31T18:00:56.640351183-05:00 - INFO - New value: opacity = 0.7
2024-03-31T18:00:56.640385193-05:00 - INFO - 

2024-03-31T18:00:56.640397403-05:00 - INFO - is bool true true
2024-03-31T18:00:56.640427942-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"opacity = 0.6\",\n      \"new\": \"opacity = 0.7\"\n    }\n  }\n}","role":"assistant"}}],"created":1711926055,"id":"chatcmpl-98ySVcwlsKtOi4Jh6jZi2akPprSpd","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":41,"prompt_tokens":1077,"total_tokens":1118}}
2024-03-31T18:01:14.000389516-05:00 - INFO - booting up
2024-03-31T18:01:14.001575566-05:00 - INFO - alias was selected
2024-03-31T18:01:14.001642555-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-31T18:01:14.021108278-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T18:01:14.021170308-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T18:01:14.021203578-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T18:01:14.021309167-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T18:01:14.069220951-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-31T18:01:14.077476507-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-31T18:01:14.091382462-05:00 - TRACE - client handshake Http1
2024-03-31T18:01:14.091415011-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T18:01:14.091527200-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T18:01:14.091596260-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T18:01:14.091673979-05:00 - TRACE - encode_headers;
2024-03-31T18:01:14.091690369-05:00 - TRACE - -> encode_headers;
2024-03-31T18:01:14.091699589-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3894))
2024-03-31T18:01:14.091727048-05:00 - TRACE - <- encode_headers;
2024-03-31T18:01:14.091735218-05:00 - TRACE - -- encode_headers;
2024-03-31T18:01:14.091745508-05:00 - TRACE - sized write, len = 3894
2024-03-31T18:01:14.091754058-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3894
2024-03-31T18:01:14.091802958-05:00 - DEBUG - flushed 4096 bytes
2024-03-31T18:01:14.091816307-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:01:15.376993986-05:00 - TRACE - Conn::read_head
2024-03-31T18:01:15.377204114-05:00 - TRACE - received 1369 bytes
2024-03-31T18:01:15.377224874-05:00 - TRACE - parse_headers;
2024-03-31T18:01:15.377235634-05:00 - TRACE - -> parse_headers;
2024-03-31T18:01:15.377259324-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T18:01:15.377307503-05:00 - TRACE - Response.parse Complete(1202)
2024-03-31T18:01:15.377382443-05:00 - TRACE - <- parse_headers;
2024-03-31T18:01:15.377394973-05:00 - TRACE - -- parse_headers;
2024-03-31T18:01:15.377406182-05:00 - DEBUG - parsed 24 headers
2024-03-31T18:01:15.377419602-05:00 - DEBUG - incoming body is content-length (588 bytes)
2024-03-31T18:01:15.377446262-05:00 - TRACE - decode; state=Length(588)
2024-03-31T18:01:15.377466822-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(421)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:01:15.377528211-05:00 - TRACE - decode; state=Length(421)
2024-03-31T18:01:15.377575661-05:00 - TRACE - received 421 bytes
2024-03-31T18:01:15.377590611-05:00 - DEBUG - incoming body completed
2024-03-31T18:01:15.377616121-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T18:01:15.377651740-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:01:15.377673340-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:01:15.377693350-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T18:01:15.377711040-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T18:01:15.377779059-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:01:15.377803399-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "opacity = 0.7",
      "new": "opacity = 0.8"
    }
  }
}
2024-03-31T18:01:15.377838599-05:00 - INFO - key change1
2024-03-31T18:01:15.377847899-05:00 - INFO - value "opacity = 0.7"
2024-03-31T18:01:15.377881918-05:00 - INFO - old value is "opacity = 0.7"
2024-03-31T18:01:15.377897748-05:00 - INFO - writing to file
2024-03-31T18:01:15.378006257-05:00 - INFO - New value: opacity = 0.8
2024-03-31T18:01:15.378040007-05:00 - INFO - 

2024-03-31T18:01:15.378049897-05:00 - INFO - is bool true true
2024-03-31T18:01:15.378078536-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"opacity = 0.7\",\n      \"new\": \"opacity = 0.8\"\n    }\n  }\n}","role":"assistant"}}],"created":1711926074,"id":"chatcmpl-98ySoYp6okxt7TjwGFtWLDa4h4ADI","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":41,"prompt_tokens":1078,"total_tokens":1119}}
2024-03-31T18:01:20.125755661-05:00 - INFO - booting up
2024-03-31T18:01:20.126869561-05:00 - INFO - alias was selected
2024-03-31T18:01:20.126927560-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-31T18:01:20.145786541-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T18:01:20.145852440-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T18:01:20.145888570-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T18:01:20.145993299-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T18:01:20.150890943-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-31T18:01:20.158774542-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-31T18:01:20.170338677-05:00 - TRACE - client handshake Http1
2024-03-31T18:01:20.170370967-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T18:01:20.170469526-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T18:01:20.170540635-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T18:01:20.170615334-05:00 - TRACE - encode_headers;
2024-03-31T18:01:20.170631074-05:00 - TRACE - -> encode_headers;
2024-03-31T18:01:20.170639644-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3894))
2024-03-31T18:01:20.170664264-05:00 - TRACE - <- encode_headers;
2024-03-31T18:01:20.170677044-05:00 - TRACE - -- encode_headers;
2024-03-31T18:01:20.170687024-05:00 - TRACE - sized write, len = 3894
2024-03-31T18:01:20.170695364-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3894
2024-03-31T18:01:20.170748313-05:00 - DEBUG - flushed 4096 bytes
2024-03-31T18:01:20.170762603-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:01:21.111255585-05:00 - TRACE - Conn::read_head
2024-03-31T18:01:21.111456513-05:00 - TRACE - received 1369 bytes
2024-03-31T18:01:21.111476783-05:00 - TRACE - parse_headers;
2024-03-31T18:01:21.111487253-05:00 - TRACE - -> parse_headers;
2024-03-31T18:01:21.111511472-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T18:01:21.111559352-05:00 - TRACE - Response.parse Complete(1204)
2024-03-31T18:01:21.111633521-05:00 - TRACE - <- parse_headers;
2024-03-31T18:01:21.111646061-05:00 - TRACE - -- parse_headers;
2024-03-31T18:01:21.111655841-05:00 - DEBUG - parsed 24 headers
2024-03-31T18:01:21.111665111-05:00 - DEBUG - incoming body is content-length (567 bytes)
2024-03-31T18:01:21.111691781-05:00 - TRACE - decode; state=Length(567)
2024-03-31T18:01:21.111712601-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(402)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:01:21.111741820-05:00 - TRACE - decode; state=Length(402)
2024-03-31T18:01:21.111780040-05:00 - TRACE - received 402 bytes
2024-03-31T18:01:21.111801420-05:00 - DEBUG - incoming body completed
2024-03-31T18:01:21.111828440-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T18:01:21.111864079-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:01:21.111886119-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:01:21.111906569-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T18:01:21.111924389-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T18:01:21.111990268-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:01:21.112013758-05:00 - INFO - text full json {
  "results": {
    "change1": {"old": "opacity = 0.8", "new": "opacity = 0.9"}
  }
}
2024-03-31T18:01:21.112041698-05:00 - INFO - key change1
2024-03-31T18:01:21.112050778-05:00 - INFO - value "opacity = 0.8"
2024-03-31T18:01:21.112083957-05:00 - INFO - old value is "opacity = 0.8"
2024-03-31T18:01:21.112099497-05:00 - INFO - writing to file
2024-03-31T18:01:21.112206556-05:00 - INFO - New value: opacity = 0.9
2024-03-31T18:01:21.112240216-05:00 - INFO - 

2024-03-31T18:01:21.112251336-05:00 - INFO - is bool true true
2024-03-31T18:01:21.112279716-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\"old\": \"opacity = 0.8\", \"new\": \"opacity = 0.9\"}\n  }\n}","role":"assistant"}}],"created":1711926080,"id":"chatcmpl-98ySuhJNorUwtrSqFGQdV8kNRdXoP","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_b28b39ffa8","usage":{"completion_tokens":36,"prompt_tokens":1078,"total_tokens":1114}}
2024-03-31T18:01:21.112336685-05:00 - TRACE - client tx closed
2024-03-31T18:01:21.112363585-05:00 - TRACE - State::close_read()
2024-03-31T18:01:21.112381525-05:00 - TRACE - State::close_write()
2024-03-31T18:01:21.112395185-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-31T18:01:21.112438034-05:00 - TRACE - shut down IO complete
2024-03-31T18:02:29.424654626-05:00 - INFO - booting up
2024-03-31T18:02:29.425751886-05:00 - INFO - list of all aliases [ConfigFile { alias: "alacritty.toml", realpath: "/home/osamanoo/.config/alacritty/alacritty.toml", iteration: 1, backup_location: "backup_config/alacritty.toml", ts: "2024-03-25 04:06:09.037046463 UTC" }, ConfigFile { alias: "fish_config", realpath: "/home/osamanoo/.config/fish/config.fish", iteration: 1, backup_location: "backup_config/config.fish", ts: "2024-03-28 06:43:22.516949493 UTC" }, ConfigFile { alias: "fisher_fish", realpath: "/home/osamanoo/.config/fish/completions/fisher.fish", iteration: 1, backup_location: "backup_config/fisher.fish", ts: "2024-03-28 06:56:46.983009804 UTC" }]
2024-03-31T18:02:45.041524859-05:00 - INFO - booting up
2024-03-31T18:02:45.042770898-05:00 - INFO - going to get all possible content
2024-03-31T18:02:45.061422759-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T18:02:45.061478368-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T18:02:45.061525088-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T18:02:45.061632977-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T18:02:45.080585005-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-31T18:02:45.091003320-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-31T18:02:45.102823454-05:00 - TRACE - client handshake Http1
2024-03-31T18:02:45.102856444-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T18:02:45.102962903-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T18:02:45.103034982-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T18:02:45.103109651-05:00 - TRACE - encode_headers;
2024-03-31T18:02:45.103125641-05:00 - TRACE - -> encode_headers;
2024-03-31T18:02:45.103134411-05:00 - TRACE - Client::encode method=POST, body=Some(Known(331))
2024-03-31T18:02:45.103159491-05:00 - TRACE - <- encode_headers;
2024-03-31T18:02:45.103167691-05:00 - TRACE - -- encode_headers;
2024-03-31T18:02:45.103178001-05:00 - TRACE - sized write, len = 331
2024-03-31T18:02:45.103186321-05:00 - TRACE - buffer.flatten self.len=201 buf.len=331
2024-03-31T18:02:45.103226080-05:00 - DEBUG - flushed 532 bytes
2024-03-31T18:02:45.103239710-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:02:49.074001210-05:00 - TRACE - Conn::read_head
2024-03-31T18:02:49.074228328-05:00 - TRACE - received 1369 bytes
2024-03-31T18:02:49.074248818-05:00 - TRACE - parse_headers;
2024-03-31T18:02:49.074259878-05:00 - TRACE - -> parse_headers;
2024-03-31T18:02:49.074282207-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T18:02:49.074332017-05:00 - TRACE - Response.parse Complete(1203)
2024-03-31T18:02:49.074414056-05:00 - TRACE - <- parse_headers;
2024-03-31T18:02:49.074425926-05:00 - TRACE - -- parse_headers;
2024-03-31T18:02:49.074446816-05:00 - DEBUG - parsed 24 headers
2024-03-31T18:02:49.074457236-05:00 - DEBUG - incoming body is content-length (1452 bytes)
2024-03-31T18:02:49.074491446-05:00 - TRACE - decode; state=Length(1452)
2024-03-31T18:02:49.074517325-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(1286)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:02:49.074743523-05:00 - TRACE - decode; state=Length(1286)
2024-03-31T18:02:49.074802633-05:00 - TRACE - received 1286 bytes
2024-03-31T18:02:49.074821283-05:00 - DEBUG - incoming body completed
2024-03-31T18:02:49.074851372-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T18:02:49.074891022-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:02:49.074912952-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:02:49.074934752-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T18:02:49.074956151-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T18:02:49.075026651-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:02:49.075050001-05:00 - INFO - json response {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"Here is a list of possible configuration changes that can be made in the alacritty.toml file:\n\n1. Changing the font size and font family.\n2. Customizing the color scheme for text, background, cursor, and other elements.\n3. Adjusting the window dimensions, padding, and margins.\n4. Configuring keybindings for various actions such as copy, paste, and scrolling.\n5. Enabling or disabling certain features like mouse support, ligatures, or blinking text.\n6. Setting the scrollback buffer size.\n7. Configuring the cursor style and visibility.\n8. Changing the window transparency level.\n9. Configuring the shell program that Alacritty will use.\n10. Enabling and customizing scrollback search functionality.\n11. Setting up custom startup commands or shell profiles.\n12. Configuring the behavior of the tab bar.\n13. Enabling or disabling specific terminal emulation features. \n\nThese are just some of the possible configuration changes that can be made in the alacritty.toml file.","role":"assistant"}}],"created":1711926165,"id":"chatcmpl-98yUHW19JIQ92uG9TSeaRLVCGhf9c","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":211,"prompt_tokens":54,"total_tokens":265}}
2024-03-31T18:02:49.075175639-05:00 - TRACE - client tx closed
2024-03-31T18:02:49.075201199-05:00 - TRACE - State::close_read()
2024-03-31T18:02:49.075218429-05:00 - TRACE - State::close_write()
2024-03-31T18:02:49.075238299-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-31T18:02:49.075296158-05:00 - TRACE - shut down IO complete
2024-03-31T18:03:21.011425174-05:00 - INFO - booting up
2024-03-31T18:03:21.012628943-05:00 - INFO - going to get all possible content
2024-03-31T18:03:21.031932898-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T18:03:21.031990668-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T18:03:21.032028547-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T18:03:21.032130237-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T18:03:21.036355358-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-31T18:03:21.046282339-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-31T18:03:21.057924003-05:00 - TRACE - client handshake Http1
2024-03-31T18:03:21.057957162-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T18:03:21.058061621-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T18:03:21.058131211-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T18:03:21.058213210-05:00 - TRACE - encode_headers;
2024-03-31T18:03:21.058229440-05:00 - TRACE - -> encode_headers;
2024-03-31T18:03:21.058237950-05:00 - TRACE - Client::encode method=POST, body=Some(Known(331))
2024-03-31T18:03:21.058263260-05:00 - TRACE - <- encode_headers;
2024-03-31T18:03:21.058271370-05:00 - TRACE - -- encode_headers;
2024-03-31T18:03:21.058281489-05:00 - TRACE - sized write, len = 331
2024-03-31T18:03:21.058289749-05:00 - TRACE - buffer.flatten self.len=201 buf.len=331
2024-03-31T18:03:21.058328709-05:00 - DEBUG - flushed 532 bytes
2024-03-31T18:03:21.058342169-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:03:22.462019524-05:00 - TRACE - Conn::read_head
2024-03-31T18:03:22.462236012-05:00 - TRACE - received 1369 bytes
2024-03-31T18:03:22.462257782-05:00 - TRACE - parse_headers;
2024-03-31T18:03:22.462268692-05:00 - TRACE - -> parse_headers;
2024-03-31T18:03:22.462290762-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T18:03:22.462345091-05:00 - TRACE - Response.parse Complete(1203)
2024-03-31T18:03:22.462428150-05:00 - TRACE - <- parse_headers;
2024-03-31T18:03:22.462444120-05:00 - TRACE - -- parse_headers;
2024-03-31T18:03:22.462455760-05:00 - DEBUG - parsed 24 headers
2024-03-31T18:03:22.462465270-05:00 - DEBUG - incoming body is content-length (1145 bytes)
2024-03-31T18:03:22.462495150-05:00 - TRACE - decode; state=Length(1145)
2024-03-31T18:03:22.462517770-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(979)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:03:22.462616599-05:00 - TRACE - decode; state=Length(979)
2024-03-31T18:03:22.462758227-05:00 - TRACE - received 979 bytes
2024-03-31T18:03:22.462776437-05:00 - DEBUG - incoming body completed
2024-03-31T18:03:22.462809637-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T18:03:22.462849217-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:03:22.462871646-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:03:22.462894226-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T18:03:22.462914546-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T18:03:22.463013565-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:03:22.463041295-05:00 - INFO - json response {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"1. Changing the font size\n2. Modifying the colorscheme\n3. Adjusting the cursor style\n4. Setting the background transparency\n5. Enabling or disabling visual bell\n6. Configuring keybindings\n7. Customizing the window title\n8. Increasing or decreasing the scrollback buffer size\n9. Setting the default shell\n10. Configuring the window position and size\n11. Enabling or disabling the mouse support\n12. Configuring the scrollbar appearance\n13. Changing the tab behavior\n14. Setting the padding and margins\n15. Adjusting the cursor blink rate\n16. Changing the font style (bold, italic)\n17. Configuring the display DPI\n18. Enabling or disabling certain plugins or features.","role":"assistant"}}],"created":1711926201,"id":"chatcmpl-98yUrY7ifPnVggcdmpWsf75FeRhbR","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_b28b39ffa8","usage":{"completion_tokens":154,"prompt_tokens":54,"total_tokens":208}}
2024-03-31T18:03:22.463155054-05:00 - TRACE - client tx closed
2024-03-31T18:03:22.463174244-05:00 - TRACE - State::close_read()
2024-03-31T18:03:22.463187134-05:00 - TRACE - State::close_write()
2024-03-31T18:03:22.463200293-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-31T18:03:22.463257113-05:00 - TRACE - shut down IO complete
2024-03-31T18:04:35.835592465-05:00 - INFO - booting up
2024-03-31T18:04:35.836800764-05:00 - INFO - alias was selected
2024-03-31T18:04:35.836879173-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-31T18:04:35.856259047-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T18:04:35.856315767-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T18:04:35.856347416-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T18:04:35.856454335-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T18:04:35.873305293-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-31T18:04:35.881493379-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-31T18:04:35.895455823-05:00 - TRACE - client handshake Http1
2024-03-31T18:04:35.895488862-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T18:04:35.895587961-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T18:04:35.895657641-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T18:04:35.895740020-05:00 - TRACE - encode_headers;
2024-03-31T18:04:35.895755600-05:00 - TRACE - -> encode_headers;
2024-03-31T18:04:35.895764390-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3878))
2024-03-31T18:04:35.895789490-05:00 - TRACE - <- encode_headers;
2024-03-31T18:04:35.895797800-05:00 - TRACE - -- encode_headers;
2024-03-31T18:04:35.895808289-05:00 - TRACE - sized write, len = 3878
2024-03-31T18:04:35.895816969-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3878
2024-03-31T18:04:35.895868239-05:00 - DEBUG - flushed 4080 bytes
2024-03-31T18:04:35.895882169-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:04:38.745408364-05:00 - TRACE - Conn::read_head
2024-03-31T18:04:38.745633262-05:00 - TRACE - received 1369 bytes
2024-03-31T18:04:38.745653492-05:00 - TRACE - parse_headers;
2024-03-31T18:04:38.745664542-05:00 - TRACE - -> parse_headers;
2024-03-31T18:04:38.745687292-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T18:04:38.745736672-05:00 - TRACE - Response.parse Complete(1203)
2024-03-31T18:04:38.745819681-05:00 - TRACE - <- parse_headers;
2024-03-31T18:04:38.745832381-05:00 - TRACE - -- parse_headers;
2024-03-31T18:04:38.745842351-05:00 - DEBUG - parsed 24 headers
2024-03-31T18:04:38.745851890-05:00 - DEBUG - incoming body is content-length (954 bytes)
2024-03-31T18:04:38.745883520-05:00 - TRACE - decode; state=Length(954)
2024-03-31T18:04:38.745906470-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(788)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:04:38.745940690-05:00 - TRACE - decode; state=Length(788)
2024-03-31T18:04:38.745983239-05:00 - TRACE - received 788 bytes
2024-03-31T18:04:38.745998369-05:00 - DEBUG - incoming body completed
2024-03-31T18:04:38.746034349-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T18:04:38.746071829-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:04:38.746093788-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:04:38.746114858-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T18:04:38.746133888-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T18:04:38.746202747-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:04:38.746228937-05:00 - INFO - text full json {
  "results": {
    "change1": {"old": "#[font]", "new": "![font]"},  
    "change2": {"old": "#[font.bold]", "new": "![font.bold]"},
    "change3": {"old": "#[font.bold_italic]", "new": "![font.bold_italic]"},
    "change4": {"old": "#[font.italic]", "new": "![font.italic]"},
    "change5": {"old": "#[font.normal]", "new": "![font.normal]"},
    "change6": {"old": "#[font.offset]", "new": "![font.offset]"}
  }
}
2024-03-31T18:04:38.746279947-05:00 - INFO - key change1
2024-03-31T18:04:38.746298496-05:00 - INFO - value "#[font]"
2024-03-31T18:04:38.746333196-05:00 - INFO - old value is "#[font]"
2024-03-31T18:04:38.746346386-05:00 - INFO - writing to file
2024-03-31T18:04:38.746446065-05:00 - INFO - key change2
2024-03-31T18:04:38.746475405-05:00 - INFO - value "#[font.bold]"
2024-03-31T18:04:38.746502145-05:00 - INFO - old value is "#[font.bold]"
2024-03-31T18:04:38.746515705-05:00 - INFO - writing to file
2024-03-31T18:04:38.746595554-05:00 - INFO - key change3
2024-03-31T18:04:38.746614124-05:00 - INFO - value "#[font.bold_italic]"
2024-03-31T18:04:38.746636663-05:00 - INFO - old value is "#[font.bold_italic]"
2024-03-31T18:04:38.746648033-05:00 - INFO - writing to file
2024-03-31T18:04:38.746720223-05:00 - INFO - key change4
2024-03-31T18:04:38.746738293-05:00 - INFO - value "#[font.italic]"
2024-03-31T18:04:38.746760152-05:00 - INFO - old value is "#[font.italic]"
2024-03-31T18:04:38.746771552-05:00 - INFO - writing to file
2024-03-31T18:04:38.746842242-05:00 - INFO - key change5
2024-03-31T18:04:38.746859881-05:00 - INFO - value "#[font.normal]"
2024-03-31T18:04:38.746881421-05:00 - INFO - old value is "#[font.normal]"
2024-03-31T18:04:38.746892701-05:00 - INFO - writing to file
2024-03-31T18:04:38.746962781-05:00 - INFO - key change6
2024-03-31T18:04:38.746980530-05:00 - INFO - value "#[font.offset]"
2024-03-31T18:04:38.747001990-05:00 - INFO - old value is "#[font.offset]"
2024-03-31T18:04:38.747013370-05:00 - INFO - writing to file
2024-03-31T18:04:38.747083589-05:00 - INFO - New value: ![font]
2024-03-31T18:04:38.747116319-05:00 - INFO - 

2024-03-31T18:04:38.747123499-05:00 - INFO - is bool true true
2024-03-31T18:04:38.747155909-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\"old\": \"#[font]\", \"new\": \"![font]\"},  \n    \"change2\": {\"old\": \"#[font.bold]\", \"new\": \"![font.bold]\"},\n    \"change3\": {\"old\": \"#[font.bold_italic]\", \"new\": \"![font.bold_italic]\"},\n    \"change4\": {\"old\": \"#[font.italic]\", \"new\": \"![font.italic]\"},\n    \"change5\": {\"old\": \"#[font.normal]\", \"new\": \"![font.normal]\"},\n    \"change6\": {\"old\": \"#[font.offset]\", \"new\": \"![font.offset]\"}\n  }\n}","role":"assistant"}}],"created":1711926276,"id":"chatcmpl-98yW40hT8btHXoiFHEXHcunGdF6QH","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":146,"prompt_tokens":1072,"total_tokens":1218}}
2024-03-31T18:05:02.429262503-05:00 - INFO - booting up
2024-03-31T18:05:02.430491552-05:00 - INFO - user request this alias to be reverted alacritty.toml 
2024-03-31T18:05:02.430545762-05:00 - INFO - checking if alias exist, so it could be reverted
2024-03-31T18:05:02.430556372-05:00 - INFO - content alias is alacritty.toml 
2024-03-31T18:05:02.430683521-05:00 - INFO - sucessfully reverted the original file to its backup file
2024-03-31T18:05:04.562110351-05:00 - INFO - booting up
2024-03-31T18:05:04.563278941-05:00 - INFO - alias was selected
2024-03-31T18:05:04.563331210-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-31T18:05:04.581728884-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T18:05:04.581793573-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T18:05:04.581822883-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T18:05:04.581933692-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T18:05:04.587053216-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-31T18:05:04.593369567-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-31T18:05:04.608505061-05:00 - TRACE - client handshake Http1
2024-03-31T18:05:04.608540261-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T18:05:04.608631340-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T18:05:04.608705830-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T18:05:04.608776489-05:00 - TRACE - encode_headers;
2024-03-31T18:05:04.608792599-05:00 - TRACE - -> encode_headers;
2024-03-31T18:05:04.608801489-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3878))
2024-03-31T18:05:04.608828159-05:00 - TRACE - <- encode_headers;
2024-03-31T18:05:04.608836318-05:00 - TRACE - -- encode_headers;
2024-03-31T18:05:04.608846648-05:00 - TRACE - sized write, len = 3878
2024-03-31T18:05:04.608855058-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3878
2024-03-31T18:05:04.608906048-05:00 - DEBUG - flushed 4080 bytes
2024-03-31T18:05:04.608920068-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:05:06.309510201-05:00 - TRACE - Conn::read_head
2024-03-31T18:05:06.309753279-05:00 - TRACE - received 1369 bytes
2024-03-31T18:05:06.309773319-05:00 - TRACE - parse_headers;
2024-03-31T18:05:06.309782769-05:00 - TRACE - -> parse_headers;
2024-03-31T18:05:06.309803899-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T18:05:06.309855528-05:00 - TRACE - Response.parse Complete(1203)
2024-03-31T18:05:06.309931467-05:00 - TRACE - <- parse_headers;
2024-03-31T18:05:06.309941847-05:00 - TRACE - -- parse_headers;
2024-03-31T18:05:06.309952857-05:00 - DEBUG - parsed 24 headers
2024-03-31T18:05:06.309967677-05:00 - DEBUG - incoming body is content-length (733 bytes)
2024-03-31T18:05:06.310002747-05:00 - TRACE - decode; state=Length(733)
2024-03-31T18:05:06.310027407-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(567)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:05:06.310134816-05:00 - TRACE - decode; state=Length(567)
2024-03-31T18:05:06.310241165-05:00 - TRACE - received 567 bytes
2024-03-31T18:05:06.310255184-05:00 - DEBUG - incoming body completed
2024-03-31T18:05:06.310284624-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T18:05:06.310317164-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:05:06.310334554-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:05:06.310353114-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T18:05:06.310370643-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T18:05:06.310429683-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:05:06.310448583-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "#[font.bold_italic]\n#family = \"Mononoki Nerd Font\"\n#style = \"Bold Italic\"\n",
      "new": "[font.italic]\nfamily = \"Mononoki Nerd Font\"\nstyle = \"Italic\"\n"
    }
  }
}
2024-03-31T18:05:06.310484002-05:00 - INFO - key change1
2024-03-31T18:05:06.310500132-05:00 - INFO - value "#[font.bold_italic]\n#family = \"Mononoki Nerd Font\"\n#style = \"Bold Italic\"\n"
2024-03-31T18:05:06.310529842-05:00 - INFO - old value is "#[font.bold_italic]\n#family = \"Mononoki Nerd Font\"\n#style = \"Bold Italic\"\n"
2024-03-31T18:05:06.310541472-05:00 - INFO - writing to file
2024-03-31T18:05:06.310642931-05:00 - INFO - New value: [font.italic]
family = "Mononoki Nerd Font"
style = "Italic"

2024-03-31T18:05:06.310672791-05:00 - INFO - 

2024-03-31T18:05:06.310680581-05:00 - INFO - is bool true true
2024-03-31T18:05:06.310708430-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"#[font.bold_italic]\\n#family = \\\"Mononoki Nerd Font\\\"\\n#style = \\\"Bold Italic\\\"\\n\",\n      \"new\": \"[font.italic]\\nfamily = \\\"Mononoki Nerd Font\\\"\\nstyle = \\\"Italic\\\"\\n\"\n    }\n  }\n}","role":"assistant"}}],"created":1711926304,"id":"chatcmpl-98yWWBh3rozLRQ4V0AZVg36XyonjX","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":79,"prompt_tokens":1072,"total_tokens":1151}}
2024-03-31T18:05:06.310760810-05:00 - TRACE - client tx closed
2024-03-31T18:05:06.310777780-05:00 - TRACE - pool closed, canceling idle interval
2024-03-31T18:05:06.310868439-05:00 - TRACE - State::close_read()
2024-03-31T18:05:06.310890539-05:00 - TRACE - State::close_write()
2024-03-31T18:05:06.310901089-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-31T18:05:06.310931808-05:00 - TRACE - shut down IO complete
2024-03-31T18:05:18.965842511-05:00 - INFO - booting up
2024-03-31T18:05:18.966948501-05:00 - INFO - user request this alias to be reverted alacritty.toml 
2024-03-31T18:05:18.966987261-05:00 - INFO - checking if alias exist, so it could be reverted
2024-03-31T18:05:18.966995231-05:00 - INFO - content alias is alacritty.toml 
2024-03-31T18:05:18.967095490-05:00 - INFO - sucessfully reverted the original file to its backup file
2024-03-31T18:07:54.207319236-05:00 - INFO - booting up
2024-03-31T18:07:54.208466634-05:00 - INFO - alias was selected
2024-03-31T18:07:54.208516913-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-31T18:07:54.227362155-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T18:07:54.227420864-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T18:07:54.227456204-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T18:07:54.227563303-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T18:07:54.242136121-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-31T18:07:54.250945985-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-31T18:07:54.263890511-05:00 - TRACE - client handshake Http1
2024-03-31T18:07:54.263928381-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T18:07:54.264031370-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T18:07:54.264102779-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T18:07:54.264176638-05:00 - TRACE - encode_headers;
2024-03-31T18:07:54.264192268-05:00 - TRACE - -> encode_headers;
2024-03-31T18:07:54.264201038-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3877))
2024-03-31T18:07:54.264226277-05:00 - TRACE - <- encode_headers;
2024-03-31T18:07:54.264234397-05:00 - TRACE - -- encode_headers;
2024-03-31T18:07:54.264244447-05:00 - TRACE - sized write, len = 3877
2024-03-31T18:07:54.264252807-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3877
2024-03-31T18:07:54.264305607-05:00 - DEBUG - flushed 4079 bytes
2024-03-31T18:07:54.264319346-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:07:55.866894686-05:00 - TRACE - Conn::read_head
2024-03-31T18:07:55.867094644-05:00 - TRACE - received 1369 bytes
2024-03-31T18:07:55.867114044-05:00 - TRACE - parse_headers;
2024-03-31T18:07:55.867128894-05:00 - TRACE - -> parse_headers;
2024-03-31T18:07:55.867153563-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T18:07:55.867208513-05:00 - TRACE - Response.parse Complete(1203)
2024-03-31T18:07:55.867287172-05:00 - TRACE - <- parse_headers;
2024-03-31T18:07:55.867299692-05:00 - TRACE - -- parse_headers;
2024-03-31T18:07:55.867309632-05:00 - DEBUG - parsed 24 headers
2024-03-31T18:07:55.867319242-05:00 - DEBUG - incoming body is content-length (678 bytes)
2024-03-31T18:07:55.867348561-05:00 - TRACE - decode; state=Length(678)
2024-03-31T18:07:55.867370671-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(512)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:07:55.867467140-05:00 - TRACE - decode; state=Length(512)
2024-03-31T18:07:55.867603438-05:00 - TRACE - received 512 bytes
2024-03-31T18:07:55.867621518-05:00 - DEBUG - incoming body completed
2024-03-31T18:07:55.867657328-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T18:07:55.867708037-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:07:55.867730987-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:07:55.867752517-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T18:07:55.867771667-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T18:07:55.867840406-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:07:55.867862626-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "cursor = \"0x93a1a1\"\ntext = \"0x002b36\"",
      "new": "bell = true\ncursor = \"0x93a1a1\"\ntext = \"0x002b36\""
    }
  }
}
2024-03-31T18:07:55.867910885-05:00 - INFO - key change1
2024-03-31T18:07:55.867922715-05:00 - INFO - value "cursor = \"0x93a1a1\"\ntext = \"0x002b36\""
2024-03-31T18:07:55.867962304-05:00 - INFO - old value is "cursor = \"0x93a1a1\"\ntext = \"0x002b36\""
2024-03-31T18:07:55.867981104-05:00 - INFO - writing to file
2024-03-31T18:07:55.868087183-05:00 - INFO - New value: bell = true
cursor = "0x93a1a1"
text = "0x002b36"
2024-03-31T18:07:55.868116953-05:00 - INFO - 

2024-03-31T18:07:55.868128403-05:00 - INFO - is bool true true
2024-03-31T18:07:55.868162972-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"cursor = \\\"0x93a1a1\\\"\\ntext = \\\"0x002b36\\\"\",\n      \"new\": \"bell = true\\ncursor = \\\"0x93a1a1\\\"\\ntext = \\\"0x002b36\\\"\"\n    }\n  }\n}","role":"assistant"}}],"created":1711926474,"id":"chatcmpl-98yZGUlulhLtuNKNt7jMXqPjwuDAP","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":75,"prompt_tokens":1072,"total_tokens":1147}}
2024-03-31T18:08:01.041030757-05:00 - INFO - booting up
2024-03-31T18:08:01.042248363-05:00 - INFO - alias was selected
2024-03-31T18:08:01.042302113-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-31T18:08:01.062330242-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T18:08:01.062387342-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T18:08:01.062419281-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T18:08:01.062523920-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T18:08:01.066062671-05:00 - DEBUG - connecting to 104.18.7.192:443
2024-03-31T18:08:01.073713687-05:00 - DEBUG - connected to 104.18.7.192:443
2024-03-31T18:08:01.087169788-05:00 - TRACE - client handshake Http1
2024-03-31T18:08:01.087203838-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T18:08:01.087300977-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T18:08:01.087367416-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T18:08:01.087444325-05:00 - TRACE - encode_headers;
2024-03-31T18:08:01.087460175-05:00 - TRACE - -> encode_headers;
2024-03-31T18:08:01.087468995-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3890))
2024-03-31T18:08:01.087494714-05:00 - TRACE - <- encode_headers;
2024-03-31T18:08:01.087502884-05:00 - TRACE - -- encode_headers;
2024-03-31T18:08:01.087512944-05:00 - TRACE - sized write, len = 3890
2024-03-31T18:08:01.087521304-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3890
2024-03-31T18:08:01.087573944-05:00 - DEBUG - flushed 4092 bytes
2024-03-31T18:08:01.087587733-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:08:02.010908785-05:00 - TRACE - Conn::read_head
2024-03-31T18:08:02.011170712-05:00 - TRACE - received 1369 bytes
2024-03-31T18:08:02.011192722-05:00 - TRACE - parse_headers;
2024-03-31T18:08:02.011203671-05:00 - TRACE - -> parse_headers;
2024-03-31T18:08:02.011236171-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T18:08:02.011287360-05:00 - TRACE - Response.parse Complete(1204)
2024-03-31T18:08:02.011378909-05:00 - TRACE - <- parse_headers;
2024-03-31T18:08:02.011391219-05:00 - TRACE - -- parse_headers;
2024-03-31T18:08:02.011401009-05:00 - DEBUG - parsed 24 headers
2024-03-31T18:08:02.011410449-05:00 - DEBUG - incoming body is content-length (585 bytes)
2024-03-31T18:08:02.011447159-05:00 - TRACE - decode; state=Length(585)
2024-03-31T18:08:02.011474158-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(420)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:08:02.011557317-05:00 - TRACE - decode; state=Length(420)
2024-03-31T18:08:02.011679906-05:00 - TRACE - received 420 bytes
2024-03-31T18:08:02.011698006-05:00 - DEBUG - incoming body completed
2024-03-31T18:08:02.011733765-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T18:08:02.011775175-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:08:02.011797135-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:08:02.011819934-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T18:08:02.011841294-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T18:08:02.011917743-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:08:02.011944163-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "bell = false",
      "new": "bell = true"
    }
  }
}
2024-03-31T18:08:02.011974643-05:00 - INFO - key change1
2024-03-31T18:08:02.011984263-05:00 - INFO - value "bell = false"
2024-03-31T18:08:02.012018952-05:00 - INFO - old value is "bell = false"
2024-03-31T18:08:02.012036172-05:00 - INFO - New value: bell = true
2024-03-31T18:08:02.012064812-05:00 - INFO - 

2024-03-31T18:08:02.012072292-05:00 - INFO - is bool true true
2024-03-31T18:08:02.012097451-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"bell = false\",\n      \"new\": \"bell = true\"\n    }\n  }\n}","role":"assistant"}}],"created":1711926481,"id":"chatcmpl-98yZNOWdmv7WeCWuZpxKlYEI5X2zN","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":35,"prompt_tokens":1076,"total_tokens":1111}}
2024-03-31T18:08:02.012154881-05:00 - TRACE - client tx closed
2024-03-31T18:08:02.012175451-05:00 - TRACE - State::close_read()
2024-03-31T18:08:02.012192500-05:00 - TRACE - State::close_write()
2024-03-31T18:08:02.012209370-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-31T18:08:02.012273839-05:00 - TRACE - shut down IO complete
2024-03-31T18:08:05.140510231-05:00 - INFO - booting up
2024-03-31T18:08:05.141734418-05:00 - INFO - user request this alias to be reverted alacritty.toml 
2024-03-31T18:08:05.141776937-05:00 - INFO - checking if alias exist, so it could be reverted
2024-03-31T18:08:05.141787947-05:00 - INFO - content alias is alacritty.toml 
2024-03-31T18:08:05.141895246-05:00 - INFO - sucessfully reverted the original file to its backup file
2024-03-31T18:08:07.442932490-05:00 - INFO - booting up
2024-03-31T18:08:07.444131687-05:00 - INFO - alias was selected
2024-03-31T18:08:07.444184247-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-31T18:08:07.462606494-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T18:08:07.462663573-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T18:08:07.462691643-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T18:08:07.462793402-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T18:08:07.467203264-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-31T18:08:07.473108908-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-31T18:08:07.486567341-05:00 - TRACE - client handshake Http1
2024-03-31T18:08:07.486600720-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T18:08:07.486702349-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T18:08:07.486771548-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T18:08:07.486846358-05:00 - TRACE - encode_headers;
2024-03-31T18:08:07.486861837-05:00 - TRACE - -> encode_headers;
2024-03-31T18:08:07.486870697-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3877))
2024-03-31T18:08:07.486895497-05:00 - TRACE - <- encode_headers;
2024-03-31T18:08:07.486903997-05:00 - TRACE - -- encode_headers;
2024-03-31T18:08:07.486914607-05:00 - TRACE - sized write, len = 3877
2024-03-31T18:08:07.486923037-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3877
2024-03-31T18:08:07.486974646-05:00 - DEBUG - flushed 4079 bytes
2024-03-31T18:08:07.486988636-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:08:08.564422829-05:00 - TRACE - Conn::read_head
2024-03-31T18:08:08.564629017-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:08:08.564813115-05:00 - TRACE - Conn::read_head
2024-03-31T18:08:08.564863875-05:00 - TRACE - received 1369 bytes
2024-03-31T18:08:08.564879434-05:00 - TRACE - parse_headers;
2024-03-31T18:08:08.564889514-05:00 - TRACE - -> parse_headers;
2024-03-31T18:08:08.564913444-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T18:08:08.564962314-05:00 - TRACE - Response.parse Complete(1204)
2024-03-31T18:08:08.565044413-05:00 - TRACE - <- parse_headers;
2024-03-31T18:08:08.565057353-05:00 - TRACE - -- parse_headers;
2024-03-31T18:08:08.565067452-05:00 - DEBUG - parsed 24 headers
2024-03-31T18:08:08.565076952-05:00 - DEBUG - incoming body is content-length (626 bytes)
2024-03-31T18:08:08.565112252-05:00 - TRACE - decode; state=Length(626)
2024-03-31T18:08:08.565133802-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(461)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:08:08.565196491-05:00 - TRACE - decode; state=Length(461)
2024-03-31T18:08:08.565241151-05:00 - TRACE - received 461 bytes
2024-03-31T18:08:08.565267320-05:00 - DEBUG - incoming body completed
2024-03-31T18:08:08.565301820-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T18:08:08.565338790-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:08:08.565361019-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:08:08.565382649-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T18:08:08.565402249-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T18:08:08.565470128-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:08:08.565506738-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "live_config_reload = true",
      "new": "live_config_reload = true\nbell = true"
    }
  }
}
2024-03-31T18:08:08.565560727-05:00 - INFO - key change1
2024-03-31T18:08:08.565572357-05:00 - INFO - value "live_config_reload = true"
2024-03-31T18:08:08.565609417-05:00 - INFO - old value is "live_config_reload = true"
2024-03-31T18:08:08.565630706-05:00 - INFO - writing to file
2024-03-31T18:08:08.565740165-05:00 - INFO - New value: live_config_reload = true
bell = true
2024-03-31T18:08:08.565770565-05:00 - INFO - 

2024-03-31T18:08:08.565790125-05:00 - INFO - is bool true true
2024-03-31T18:08:08.565819434-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"live_config_reload = true\",\n      \"new\": \"live_config_reload = true\\nbell = true\"\n    }\n  }\n}","role":"assistant"}}],"created":1711926487,"id":"chatcmpl-98yZTl1NRh63C2c4lU9QBXrGEtxDF","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":43,"prompt_tokens":1072,"total_tokens":1115}}
2024-03-31T18:08:14.774296162-05:00 - INFO - booting up
2024-03-31T18:08:14.775492099-05:00 - INFO - user request this alias to be reverted alacritty.toml 
2024-03-31T18:08:14.775540359-05:00 - INFO - checking if alias exist, so it could be reverted
2024-03-31T18:08:14.775550359-05:00 - INFO - content alias is alacritty.toml 
2024-03-31T18:08:14.775674567-05:00 - INFO - sucessfully reverted the original file to its backup file
2024-03-31T18:08:18.303302784-05:00 - INFO - booting up
2024-03-31T18:08:18.304557700-05:00 - INFO - alias was selected
2024-03-31T18:08:18.304619329-05:00 - INFO - the final_file_path being used is /home/osamanoo/.config/alacritty/alacritty.toml
2024-03-31T18:08:18.323882058-05:00 - TRACE - checkout waiting for idle connection: ("https", api.openai.com)
2024-03-31T18:08:18.323937268-05:00 - DEBUG - starting new connection: https://api.openai.com/
2024-03-31T18:08:18.323968277-05:00 - TRACE - Http::connect; scheme=Some("https"), host=Some("api.openai.com"), port=None
2024-03-31T18:08:18.324074626-05:00 - DEBUG - resolving host="api.openai.com"
2024-03-31T18:08:18.328234480-05:00 - DEBUG - connecting to 104.18.6.192:443
2024-03-31T18:08:18.335166355-05:00 - DEBUG - connected to 104.18.6.192:443
2024-03-31T18:08:18.349064143-05:00 - TRACE - client handshake Http1
2024-03-31T18:08:18.349096922-05:00 - TRACE - handshake complete, spawning background dispatcher task
2024-03-31T18:08:18.349200491-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Busy }
2024-03-31T18:08:18.349287670-05:00 - TRACE - checkout dropped for ("https", api.openai.com)
2024-03-31T18:08:18.349362559-05:00 - TRACE - encode_headers;
2024-03-31T18:08:18.349379019-05:00 - TRACE - -> encode_headers;
2024-03-31T18:08:18.349388079-05:00 - TRACE - Client::encode method=POST, body=Some(Known(3877))
2024-03-31T18:08:18.349413649-05:00 - TRACE - <- encode_headers;
2024-03-31T18:08:18.349422059-05:00 - TRACE - -- encode_headers;
2024-03-31T18:08:18.349432309-05:00 - TRACE - sized write, len = 3877
2024-03-31T18:08:18.349440688-05:00 - TRACE - buffer.flatten self.len=202 buf.len=3877
2024-03-31T18:08:18.349497648-05:00 - DEBUG - flushed 4079 bytes
2024-03-31T18:08:18.349513618-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:08:20.955111779-05:00 - TRACE - Conn::read_head
2024-03-31T18:08:20.955361866-05:00 - TRACE - received 1369 bytes
2024-03-31T18:08:20.955382236-05:00 - TRACE - parse_headers;
2024-03-31T18:08:20.955393296-05:00 - TRACE - -> parse_headers;
2024-03-31T18:08:20.955421476-05:00 - TRACE - Response.parse bytes=1369
2024-03-31T18:08:20.955472475-05:00 - TRACE - Response.parse Complete(1205)
2024-03-31T18:08:20.955564384-05:00 - TRACE - <- parse_headers;
2024-03-31T18:08:20.955577124-05:00 - TRACE - -- parse_headers;
2024-03-31T18:08:20.955586944-05:00 - DEBUG - parsed 24 headers
2024-03-31T18:08:20.955596454-05:00 - DEBUG - incoming body is content-length (598 bytes)
2024-03-31T18:08:20.955632793-05:00 - TRACE - decode; state=Length(598)
2024-03-31T18:08:20.955660253-05:00 - TRACE - flushed({role=client}): State { reading: Body(Length(434)), writing: KeepAlive, keep_alive: Busy }
2024-03-31T18:08:20.955731072-05:00 - TRACE - decode; state=Length(434)
2024-03-31T18:08:20.955778872-05:00 - TRACE - received 434 bytes
2024-03-31T18:08:20.955794852-05:00 - DEBUG - incoming body completed
2024-03-31T18:08:20.955824191-05:00 - TRACE - maybe_notify; read_from_io blocked
2024-03-31T18:08:20.955864041-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:08:20.955885641-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:08:20.955929530-05:00 - TRACE - put; add idle connection for ("https", api.openai.com)
2024-03-31T18:08:20.955952020-05:00 - DEBUG - pooling idle connection for ("https", api.openai.com)
2024-03-31T18:08:20.956056559-05:00 - TRACE - flushed({role=client}): State { reading: Init, writing: Init, keep_alive: Idle }
2024-03-31T18:08:20.956085129-05:00 - INFO - text full json {
  "results": {
    "change1": {
      "old": "live_config_reload = true",
      "new": "bell = true"
    }
  }
}
2024-03-31T18:08:20.956122718-05:00 - INFO - key change1
2024-03-31T18:08:20.956132068-05:00 - INFO - value "live_config_reload = true"
2024-03-31T18:08:20.956161388-05:00 - INFO - old value is "live_config_reload = true"
2024-03-31T18:08:20.956181038-05:00 - INFO - writing to file
2024-03-31T18:08:20.956287786-05:00 - INFO - New value: bell = true
2024-03-31T18:08:20.956317276-05:00 - INFO - 

2024-03-31T18:08:20.956325966-05:00 - INFO - is bool true true
2024-03-31T18:08:20.956361156-05:00 - INFO - full json {"choices":[{"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"{\n  \"results\": {\n    \"change1\": {\n      \"old\": \"live_config_reload = true\",\n      \"new\": \"bell = true\"\n    }\n  }\n}","role":"assistant"}}],"created":1711926498,"id":"chatcmpl-98yZeIIaY7Oqno3dDTZWxEdqV3voV","model":"gpt-3.5-turbo-0125","object":"chat.completion","system_fingerprint":"fp_3bc1b5746c","usage":{"completion_tokens":37,"prompt_tokens":1072,"total_tokens":1109}}
2024-03-31T18:08:20.956435465-05:00 - TRACE - client tx closed
2024-03-31T18:08:20.956468495-05:00 - TRACE - State::close_read()
2024-03-31T18:08:20.956488534-05:00 - TRACE - State::close_write()
2024-03-31T18:08:20.956511254-05:00 - TRACE - flushed({role=client}): State { reading: Closed, writing: Closed, keep_alive: Disabled }
2024-03-31T18:08:20.956561434-05:00 - TRACE - shut down IO complete
2024-03-31T18:08:23.544221545-05:00 - INFO - booting up
2024-03-31T18:08:23.545318222-05:00 - INFO - user request this alias to be reverted alacritty.toml 
2024-03-31T18:08:23.545358572-05:00 - INFO - checking if alias exist, so it could be reverted
2024-03-31T18:08:23.545366502-05:00 - INFO - content alias is alacritty.toml 
2024-03-31T18:08:23.545477711-05:00 - INFO - sucessfully reverted the original file to its backup file
